1601.00022.txt	Event Specific Multimodal Pattern Mining with Image-Caption Pairs	{"Introduction": ["\nWith recent advances in the capabilities of computer vision systems, researchers have been able to demonstrate performance at near-human level capabilities in difficult tasks such as image recognition.\nThese advances have been made possible by the pervasiveness of deep learning network architectures that utilize vast amounts of supervised image information.\nImage classification and recognition performance on supervised datasets will continue to improve, and because of this rapid improvement technologies are quickly being integrated into production level products such as Google Photos with tremendous public approval.\nNow that we can perform supervised image classification and recognition tasks with great accuracy we propose to utilize the developed technologies to attempt tasks that may not be as straightforward.\nWe propose one such problem: how can we discover semantically meaningful image patches from a large set of unstructured image and caption pairs? In addition, how do we discover meaningful patterns specific to high-level events such as \"Baltimore Protest\".\n\nRecently, singh2012unsupervised and LiLSH15CVPR proposed using image patches as a feature for classification instead of low-level visual bag of words features (BoW).\nVisual patterns have also been used to summarize image collections by zhang2014scalable.\nTraditionally, the desired visual patterns must be  representative and  discriminative to perform well on image classification or summarization.\n Representative refers to the tendency of this patch to appear in many of the images for a target class, and  discriminative refers to the fact that it should be unique and not appear frequently in images outside of the target category.\nInsuring that the patterns found are representative and discriminative leads to patterns that are useful for image classification, but does not imply that they are meaningful on their own or useful for other high level information extraction tasks.\nUnique but often meaningless local texture-based image patches are usually discovered using traditional pattern mining methods.\nIf image patterns are semantically meaningful they can be used for tasks beyond just image classification.\nFor example, the image patches can be used to automatically understand the possible actors and actions in a collection of images from a specific scenario or event.\nThis information can actually be mined from the data, eliminating the need for expensive supervised bounding box based training data for learning.\nOur goal in this paper is to discover image patches that are not only discriminative and representative, but also contain high-level semantic meaning and are not simply a particular texture or part of a larger object.\nWe establish a third important characteristic of our image patches, that our discovered patches must be  informative.\nWe define a  semantic visual pattern as the patterns that are recognizable to a human and can be explained or named by a word or collection of words.\nFig. <ref> gives an example of the difference between our generated  semantic visual patterns and the vision only approach.\n\nTo accomplish this goal we have created an image-caption pair dataset with categories that are news related events such as a  demonstration or  election, which can be visually diverse.\nThis visual diversity of news events allows us to discover higher level concepts with our image patches compared to previous pattern mining works which operate on datasets for scene classification or image recognition with lower level categories.\nFor example, in our dataset we are able to find image patches that represent concepts like air strike, press conference, or award ceremony.\nContrast this with the state of the art on the MIT Indoor scene recognition dataset LiLSH15CVPR, where the discovered patterns correspond to a chair, ceiling or other very concrete lower level concepts.\nTo generate image patch patterns that are informative we propose a new multimodal pattern mining algorithm, which will be shown to generate patterns that are more semantically meaningful than visual-only methods in Sec. <ref>.\nDue to the fact that we utilize image captions we are also able to name our discovered image patch patterns.\nTo the best of our knowledge, no other related works address the issue or evaluate whether their discovered image patch patterns are informative.\nOther researchers use the discovered patterns to build a mid-level representation of images and test the new features on image classification tasks.\nThese evaluations demonstrate that the mid-level visual patterns are certainly useful, but in our opinion these evaluations don't address the fundamental question of pattern mining, what is the pattern?\nWe argue that the current evaluation procedures are not sufficient to evaluate the quality of discovered patterns.\nIn this paper, we propose a set of new subjective methods in conjunction with the objective methods to evaluate the performance of visual pattern mining.\nThe contributions of our work are as follow:\n\n  * To the best of our knowledge, we are the first to address the creation of high-level event specific image patch patterns that are semantically meaningful.\n\n  * Demonstrate a novel multimodal pattern mining approach using image-caption pairs and show that using caption data we are able to discover image patches that are recognizable and apply meaningful names to them.\n\n  * Show that directly using the output of the last pooling layer of a CNN network we are able to obtain a 25\u00d7 reduction in computational cost than when using the state of the art image patch sampling approach provided in LiLSH15CVPR with little performance drop off in visual transaction generation for pattern mining.\n\n  * Provide an evaluation framework for determining if our discovered patterns are semantically informative.\nOur performance demonstrates that we can learn semantically meaningful patterns from large weakly supervised datasets and name them using our methods.\n\n  * We will make our collected image-caption dataset and named image patch patterns available to the public at the time of publication. We will also release the tools for mining the namable patterns so that other researchers will be able to extend the methods to other data corpora.\n\n\n\n", {}], "Related Work": ["\nLow level image features such as SIFT Lowe:2004:DIF:993451.996342 and Bag-of-word methods were widely used as a representation for image retrieval and classification.\nHowever, researchers have proven that these low level features do not have enough power to represent the semantic meaning of images.\nMid-level image feature representations are often used to achieve better performance in a variety of computer vision tasks.\nSome frameworks for using middle level feature representation, such as li2010object, TorresaniSzummerFitzgibbon10, have achieved excellent performance in object recognition and scene classification.\nImageNet deng2009imagenet, was introduced and has lead to breakthroughs in tasks such as object recognition and image classification due to the scale of well-labeled and annotated data.\nEach of the images within Image-Net are manually labeled and annotated, this is a very expensive and time-consuming task.\nThis work looks beyond a manually defined ontology, and instead focuses on mining mid-level image patches automatically from very weakly supervised data to attempt to unbound researchers from the need for costly supervised datasets.\nWe approach this problem from a multi-modal perspective (using the image and caption together), which allows us to name and discover higher level image concepts.\n\nVisual pattern mining is an important task since it is the foundation of many middle-level feature representation frameworks.\nhan2000mining and zhang2014scalable use low level features and a hashing approach to mine visual patterns from image collections.\nyuan2007spatial utilizes a spatial random partition to develop a fast image matching approach to discover visual patterns.\nAll of these methods obtain image patches from the original image collection either by random sampling or salient/objectness detection and utilize image matching or clustering to detect similar patches to create visual patterns.\nThese methods are computationally intense, because they have to examine possibly hundreds or thousands of image patches from each image.\nThese methods rely heavily on low level image features, and therefore do not often produce image patches that exhibit high level semantic meaning.\nThe generated image patterns are usual visually duplicated or near-duplicated image patches.\n\nConvolutional neural networks (CNN) have achieved great success in many research areas Simonyan14c, Alexnet.\nRecently, LiLSH15CVPR combined the image representation from a CNN and association rule mining technology to effectively mine visual patterns.\nThey first randomly sampled image patches from the original image and extracted the fully connected layer response as features for each image patch utilized in an association rule mining framework.\nThis approach is able to find consistent visual patterns, but cannot guarantee the discovered visual patterns are semantically meaningful.\nMost of the existing visual pattern mining work focuses on how to find visually consistent patterns.\nTo the best of our knowledge, our paper is the first attempt to find high level semantically meaningful visual patterns.\n\nAnother category of related works is image captioning.\nIn recent years, many researchers have focused on teaching machines to understand images and captions jointly.\nImage caption generation focuses on automatically generating a caption that directly describes the content in an image using a language model.\nMultimodal CNNs frome2013devise or RNN mao2014explain frameworks are often used to generate sentences for the images.\nAll the existing work use supervised approaches to learn a language generation model based on carefully constructed image captions created for this task.\nThe datasets used in caption generation, such as the MSCoco dataset MSCoco consist of much simpler sentences than appear in news image-caption pairs.\nWe differ from these approaches in that we do not try to generate a caption for images, but instead use them jointly to mine and name the patterns that appear throughout the images.\n\n\n\n", {}], "Multimodal Pattern Mining": ["\nIn this section we discuss our multimodal pattern mining approach.\nIn particular, how we collected a large scale dataset, generated feature based transactions from the image and captions, and how we find semantic visual patterns and name them.\n\n\n||TABLE||\n\n\n\n", {"Weakly Supervised Event Dataset Collection": ["\nWe believe that by using weakly supervised image data from target categories that are sufficiently broad we can automatically discover meaningful and recongizable image patch patterns.\nTo accomplish this task we collect a set of image caption pairs from a variety of types of news event categories.\nFig. <ref> provides an example of the differences and variability of the visual content between news images and a scene classification dataset that allow us to learn higher level image patterns.\n\nWe begin by crawling the complete Twitter feeds of four prominent news agencies, the Associated Press, Al Jazeera, Reuters, and CNN.\nEach of these agencies have a prominent twitter presence, and tweet links to their articles multiple times a day.\nWe collect the links to the articles and then download the html file from each extracted link.\nThe article links span the time frame from 2007-2015, and cover a variety of different topics.\nWe then parse the raw html file and find the image and caption pairs from the downloaded news articles.\nThrough this process we are able to collect approximately 280k image-caption pairs.\n\nOnce we have collected the dataset we want to find image-caption pairs that are related to different events covered in news.\nWe utilized the event ontology that was defined for the Knowledge Base Population (KBP) task in the National Institute for Standards and Technology Text Analysis Conference in 2014 to provide supervision to our dataset.\nWithin this task there is an event track with the stated goal to \"extract information such that the information would be suitable as input to a knowledge base.\"\nThis track goal closely models the goals of learning patterns that are recognizable and hence could be used in knowledge base population.\nMaking this ontology a perfect fit for our task.\n\nThe KBP event task utilizes the ontology defined by the Lingustic Data Consortium in 2005 ACE.\nThis event ontology contains 34 distinct event types, the events are broad actions that appear commonly throughout news documents, such as  demonstrate,  divorce, and  convict.\nProvided in the training data with these event ontologies is a list of trigger words for each of the events that are used to detect when an event appears in text data.\nAn example of some of the trigger words used for the  demonstrate event are: protest, riot, insurrection, and rally.\nWe search each of the captions for a trigger word from the event category, and if an image caption contains that trigger word we assign that image caption pair to the given event category.\nThe number of images found for some of the event categories can be seen in Table. <ref>.\n\n\n", {"Review of Pattern Mining": ["\nIn this section we will review the basic ideas and definitions necessary for pattern mining.\nAssume that we are given a set of n possible observations X = { x_1, x_2, ... x_n }, a  transaction, T, is a set of observations such that T \u2286 X.\nGiven a set of transactions S = {T_1, T_2, ... T_m} containing m transactions, our goal is to find a particular subset of X, say t^*, which can accurately predict the presence of some target element y \u2208 T_a, given that t^* \u2282 T_a and y \u2229 t^* = \u2205.\nt^* is referred to as a  frequent itemset in the pattern mining literature.\nThe relationship from t^* \u2192 y is known as an  association rule.\nThe support of t^* reflects how often t^* appears in S and is defined as,\n\n||FORMULA||\n\nOur goal is to find association rules that accurately predict the correct event category for the image-caption pairs.\nTherefore, we want to find patterns such that if t^* appears in a transaction there is a high likelihood that y, which represents an event category, appears in that transaction as well.\nWe define the  confidence as the likelihood that if t* \u2286 T then y \u2208 T, or,\n\n||FORMULA||\n\n\n", {}], "Using CNN Pool5 Features for Transaction Generation from Images": ["Certain portions of a CNN are only activated by a smaller region of interest (ROI) within the original image.\nThe last layer in which the particular neurons do not correspond to the entire image is the output of the final convolutional and pooling layer for Alexnet.\nBased on this observation, for each image we find the maximum magnitude response from a particular feature map from the pool5 layer of the CNN defined by Alexnet.\nThe pool5 layer of this network consists of 256 filters, and the response of each of the filters over a 6 \u00d7 6 mapping of the image.\nThe corresponding ROI from the original image in which all the pixels in that region contribute to the response of a particular neuron in the pool5 layer is a 196 \u00d7 196 image patch from the 227 \u00d7 227 resized image.\nThese 196 \u00d7 196 image patches come from a zero-padded representation of the image with zero-padding around the image edges of 64 pixels and a stride of 32.\nNamely, from a 227 \u00d7 227 scaled input image, a total of 6 \u00d7 6 (36) patch areas are covered from all the stride positions, resulting in a 6 \u00d7 6 feature map for each filter in the layer 5.\nUsing this approach we are able to use the current existing architecture to compute the filter responses for all patches at once without actually changing the network structure.\nThis idea allows us to extract image patch patterns in a way that is much more efficient than current state of the art methods.\n\nWe use the pre-trained CNN model from Alexnet that was trained using the ImageNet dataset to extract the pool5 features for the news event images.\nFor each image, we keep the maximum response over the 6 \u00d7 6 feature map and set other non-maximal values to zero for all 256 filters, similar to the non-maximum suppression operation in the literature. Such an operation is used to find the patch triggering the highest response in each filter and avoid redundant patches in the surrounding neighborhood in the image that may also generate high responses. The above process results in a 256 dimensional feature vector representation for each image patch.\nIt has been shown in LiLSH15CVPR that most of the information is carried by the top k magnitude dimensions of the feature vector, and the magnitude response of these dimensions is not particularly important.\nThus, we follow their suggestion and set the top k magnitude dimensions of the CNN vector to 1 and the others to 0, creating a binary representation of which filters are activated for each image patch.\nWe use these sparse binarized features to build  transactions for each image patch as discussed in Sec. <ref>, where the non-zero dimensions are the items in our transaction.\n\n\n||FIGURE||\n\nBy utilizing the architecture of the CNN directly we are able to efficiently extract image features that come from specific ROI that are suitable for pattern mining.\nThe current state of the art pattern mining technique proposed by LiLSH15CVPR requires a sampling of the image patches within each image and then operating the entire CNN over this sampled and resized image patch.\nThis procedure is very costly, because the CNN must be used to extract features from a number of sampled images that can be orders of magnitude larger than the dataset size.\nFor example, for the MIT indoor dataset the authors of LiLSH15CVPR sample 128 \u00d7 128 size image patches with a stride of 32 from images that have been resized such that their smallest dimension is of size 256.\nThus, the number of image samples that are taken for each image is greater than or equal to (256 - 128/32 + 1)^2 = 25.\nThe full CNN must operate on all of these sampled images.\nIn contrast our method works directly on the image themselves without any sampling.\nWe are able to extract representations for 36 image patches from an image while only having the CNN operate on the image once.\nBy leveraging the structure of the CNN during test or deployment our method is  at least 25 times less computationally intensive than the current state of the art.\nWe demonstrate in Sec. <ref> that we are able to achieve this computational gain with very little cost to predictive performance.\nAn example of the localization power of pool5 features for pattern mining can be seen in Fig. <ref>.\n\n\n", {}], "Generating Transactions from Captions": ["\nWe have discussed how we generated transactions by binarizing and thresholding the CNN features that are extracted from the images.\nWe need an analogous algorithm for generating transactions from image captions.\n\nWe begin by cleaning each of the image captions by removing stopwords and other ancillary words that should not appear (html tags or urls).\nWe then tokenize each of the captions and find all of the words that appear in at least 10 captions in our dataset.\nOnce we find these words we use the skip-gram model proposed in DBLP:journals/corr/MikolovSCCD13 that was trained on a corpus of Google News articles to map each word to a 300 dimensional embedded space.\nThe skip-gram model works well in our context because words with similar uses end up being embedded close to each other in the feature space.\nWords such as \"religious clergy\", \"priest\", and \"pastor\" all end up close in euclidean distance after embedding and far away from words that are not similar.\nWe cluster the words using K-means clustering to generate 1000 word clusters.\n\nTo generate transactions for each caption we map each word back to its corresponding cluster, then include that cluster index in the transaction set.\nWe remove patterns that contain cluster indices that are associated with commonly used words by having a high confidence score threshold as defined in Eq. <ref>.\nThe cluster indices that frequently appear in captions from a particular event category but rarely for other categories are found through our association rule mining framework.\n\nWe require our discovered patterns to contain items from both the visual and text transactions.\nBy requiring words with similar meaning to appear in all captions of a visual pattern we are able to discard patterns that may be visually similar but semantically incoherent.\nThe skip-gram based algorithm is able to handle differences in word choice and structure between captions to effectively encode meaning into our multimodal transactions.\n\n\n", {}]}], "Mining the Patterns": ["\nWe add the event category of each image as an additional item in the generated transaction for each of the image caption pairs.\nInspired by LiLSH15CVPR, we use the popular apriori algorithm Agrawal:1994:FAM:645920.672836 to find patterns within the transactions that predict which event category the image belongs to.\nWe only find the association rules which have a confidence higher than 0.8, and calculate the support threshold that insure that the at least 30 image patches exhibit a found pattern.\nFinally, we also remove any rules which only contain items generated from the image or caption transactions, insuring that we only retain truly multimodal patterns.\nTherefore, our pattern requirements can be described mathematically as,\n\n\n&c(t^* \u2192 y) \u2265 c_min\n\n&s(t^*) \u2265 s_min\n\n&t^* \u2229I\u2260\u2205\n\n&t^* \u2229C\u2260\u2205.\n\n\nWhere as defined in Eq <ref> and Eq <ref>, y is the event category of the image-caption pair, c_min is the minimum confidence threshold, s_min is our minimum support threshold, I represents the items generated from the image transaction pipeline, and C are those generated from the caption pipeline. At the end, each multimodal pattern t* contains a set of visual items (fired filter responses in pool5 in CNN model) and a set of text patterns (clusters in the skip-gram embedded space).\n\n\n", {}], "Naming the patterns": ["\nIf we can name the generated patterns we can use them for higher level information extraction tasks.\nWe leverage the fact that we have captions associated with each of the images to generate names for each pattern.\n\nWe begin the process of name generation by removing the words that are not generally useful for naming but appear often in captions.\nThe words that are removed include standard English language stop words (or, I, etc.), the name of each month, day, and directional words such as \"left\" and \"right\".\nAfter cleaning the caption words we then encode both unigram and bigrams into a vector using tf-idf encoding.\nWe ignore any unigram or bigram that does not appear at least 10 times across our caption dataset.\n\nOnce these words are removed we then sum the tf-idf vector representations of each word in all of the captions associated with a particular pattern.\nWe then take the argument max over the summed tf-idf representations to obtain the name for this pattern.\nThis procedure is explained mathematically in the following way:\nLet p be a found multimodal itemset (pattern), and T_k is the multimodal transaction for the k'th generated transaction in our dataset.\nWe define the set P as all the indices of transactions that contain p, or P = {i|p \u2286 T_i, \u2200 i}.\nIn Eq. <ref>, V is our vocabulary, W_k is the set of words from the k'th caption, I_p(w) is an indicator function on whether w corresponds to a cluster in the itemset of p, and w_kj is the j'th word in the k'th caption,\n\n\n||FORMULA||\n\n\n||FIGURE||\n\nOnce the names are found, we remove any name that appears in more than 10% of the captions of a particular event.\nThis is important because for particular events like \"injure\", words such as  injure and  wounded appear across many captions, and may lead to poor naming.\nSome examples of discovered patterns and the names that we have assigned to them can be seen in Fig <ref>.\nOur full pattern naming algorithm and pipeline can be seen in Fig <ref>.\n\n\n", {}]}], "Evaluation": ["\n", {"Using visual patterns for classification": ["\nThe most popular evaluation for pattern mining is to produce a middle level feature representation for the images, and then use any supervised classification approach to perform a task.\nThe authors of LiLSH15CVPR take this approach.\nInstead of producing a mid-level visual feature, we integrate our discovered patterns into a CNN and perform the image classification task using a modified CNN network.\n\nWe modify a CNN Alexnet by adding a fixed fully connected layer on top of the last max pooling layer, pool5.\nThe max pooling layer produces an 256 (N) dimensional output for each image patch.\nThe fixed fully connected layer has M*N parameters, where M is the number of patterns we mined in the training dataset using the visual transactions and the apriori algorithm.\nEach dimension in the M-dimensional parameter vector corresponds to one of our mined patterns.\nThe fixed fully connected layer for our model is a sparse matrix with 1 on the dimension of the filters present in a pattern, and 0 elsewhere.\nThis fixed fully connected layer produces an M dimensional output which corresponds to whether a pattern is present in the image patch or not.\nOn top of this pattern detection layer, we add another fully connected layer, followed by a ReLU layer and softmax layer to perform image classification.\nWe share the same convolutional layers as Alexnet, therefore we use a pre-trained model to update the parameters of all the convolution layers.\nDuring the training process, we freeze all the parameters of convolution layers and the first fully connected layer.\nOnly the second fully connected layer and the softmax layer are modified during training.\nWe compare our results with the current state-of-the-art methods in table <ref> on the commonly used MIT Indoor dataset 10.1109/CVPRW.2009.5206537.\nThis dataset is widely used to evaluate performance in pattern mining tasks.\nWe can see that our model outperforms all but LiLSH15CVPR.\nHowever, our model is at least 25 times more efficient in transaction generation than their model with comparable performance.\n\n\n||TABLE||\n\n\n\n", {}], "Multimodal Pattern Discovery": ["\nGiven our large news event image-caption dataset it is interesting to analyze how many multimodal patterns that we can find per event.\nThe number of patterns that were found for each event can be seen in Table <ref>.\nWe can see that some events that are sufficiently visually and semantically varied will have many patterns discovered.\nOther events that contain images that are all very similar such as  convict, which oftentimes take place in a court room and generally only show people's faces have little or no found patterns.\n\nWe notice that although events with more images associated with them generally have more discovered patterns, this is not a concrete relationship.\nFor example, the  meet event has almost 10 times as many discovered patterns as the  attack category even though  attack has more images associated with it.\nWhen analyzing the data we see that many different summits, press conferences, and other types of meetings that are covered within the news media exhibit strong visual consistency.\nMany images of word leaders shaking hands, talking behind podiums, and similar scenarios exist that we are able to mine as patterns that define this event.\nContrast this with the  attack event which contains highly varied visual action content and we are able to find less visually consistent patterns.\n\nThis makes sense intuitively, because an attack can happen in a variety of ways, such as an air strike, shooting, bomb, riot, fire, etc.\nThe  Meet event, on the other hand, almost always entails images of two or more people simply standing around and talking, and the background, setting, and people are often consistent across multiple images from one real scenario.\nThis allows us to create patterns from a variety of images from the same scenario, which exhibit strong visual and semantic consistency.\nIn this way we are able to find both very broad patterns in events such as  attack, or more specific one time only patterns to describe things like the  republican convention or  world health summit in the  meet event.\nBoth types of patterns are interesting and useful for information extraction.\n\nWe note that our methodology is useful in a variety of scenarios where weak labels can be easily obtained.\nFor example, our multimodal algorithm could also be applied to images from movies and their transcripts with genre level weak supervision.\n\n\n||TABLE||\n\n\n\n||FIGURE||\n\n\n", {}], "Subjective Analysis of Multimodal Patterns": ["\nThe type of evaluation in <ref> is useful in determining if the patterns are discriminative and representative, but this does not actually determine if the patterns are informative.\nWe believe that the pattern mining algorithm that we have developed can be used for more than image classification performance.\nTo prove this we have developed an evaluation to determine the percentage of our patterns that are semantically meaningful as well as if we are able to accurately name the patterns that we have found.\n\n\n", {"Semantic Relevance of Patterns": ["\nTo accomplish this task we built an Amazon Mechanical Turk (AMT) interface, which we used to gather annotations.\nTo generate the patterns for inspection we randomly sampled patterns from the event categories of  Attack,  Demonstrate,  Transport,  Injure, and  Elect, and  Meet for a total of 99 randomly sampled patterns.\nThese samples were taken from our multimodal pattern mining algorithm and a visual-only version of the algorithm, which is the same as the multimodal algorithm without using any text transactions.\nWe then randomly sample up to 16 of the image patches from a given pattern to represent the pattern and show them to the annotator.\nThe annotator is asked to decide whether the group of images exhibit similar and consistent semantic content.\nIf the annotator agrees that the images exhibit consistent semantics then we ask them to supply up to 3 tags that could be applied to this group of images.\nWe choose to have the annotators provide three tags (which can be synonyms), so that we have three words from each annotator to compare to each other given that different annotators may choose different tags with similar semantic meaning.\nWe analyze the inter-annotator tag agreement to determine if the patterns are semantically relevant.\nOur assumption is that if the image patches exhibit strong semantic consistency then the annotators will supply the same tags for the group of images.\nWe obtained at least 4 annotations per pattern.\n\n\n||TABLE||\n\n\nWe can see the results from this experiment in Table <ref>.\nOur multimodal model discovers patterns that are labelled by the annotators to be more semantically consistent than the visual-only patterns.\nThe annotators answered that a pattern generated by our multimodal method was semantically consistent 82% of the time compared to 72.6% for visual-only.\nWe believe that because humans have a strong capability for reasoning they may \"find\" semantic meaning in a group of images that could be spurious.\nFor this reason we also checked the agreement between the annotator provided tags.\nIf a pattern received tags from at least 3 annotators (annotators could say the patterns were not semantically consistent and choose not to provide tags), and  at least half of the annotators provided the same tag for a pattern we decided that this pattern had \"tag agreement\".\nIn this metric our multimodal approach greatly outperformed the visual-only approach with the percentage of patterns exhibiting tag agreement for multimodal at  49.5% and visual-only at  39.2%.\nOur method outperforms the visual-only pipeline in all of our subjective tests, in particular we demonstrate that the agreement across a group of people of our image patches increases by  26.2% over the state of the art visual model.\nThe multimodal pipeline outperforms the visual-only model because the visual transaction pipeline can discover similar \"looking\" image patches across our image dataset, but these group of patches that \"look\" similar do not necessarily represent a similar semantic concept.\nHowever, by adding our caption transaction generation to this pipeline we are able to discover image patches with consistent semantics.\nFor example, in the visual model pipeline we have patches taken from images of forest fires and explosions, because each pattern exhibits the similar visual components of smoke.\nOur multimodal pipeline separates these images in a meaningful way, because a forest fire and explosion are semantically different concepts that may have little overlap in their text captions.\nThe multimodal information is particularly important in datasets like the one we have created because there is a high level of semantic and visual variance across the data.\n\n\n", {}], "Evaluation of automatic pattern naming": ["\nWe test how well we are able to name the patterns.\nTo accomplish this task we utilize AMT to analyze whether our applied names are reasonable or not.\nTo generate the test data we utilized all of the patterns that were tested in Sec. <ref>.\nTo evaluate our naming algorithm on an image patch level we asked an annotator to decide if the name that we supplied to the pattern described the semantic content.\nFrom the 100 patterns we obtained 776 images.\nWe used 3 annotators per image-patch and name pair and took the majority vote to determine if that name aptly described the content in the image.\nThe annotation revealed that we had correctly applied names to 434 images and incorrectly to 353 images, for a total naming accuracy of  54.5%.\nWe remind the reader that this accuracy comes using  no supervised data.\nOur model is also able to learn patterns that are not contained in current supervised image datasets.\nUsing our naming algorithm we applied 246 unique names to our patterns (some names apply to many patterns), but only 100 (40.6%) of these names are part of the current ImageNet ontology.\nExamples of our mined semantic visual patterns and their applied names are shown in Fig. <ref>.\n\n\n", {}]}]}], "Conclusions": ["\nWe have developed the first dataset and algorithm for mining visual patterns from high level news event image caption pairs.\nOur novel multimodal model is able to discover patterns that are more informative than the state of the art vision only approach, and accurately name the patterns.\nThis work represents the first approach in using multimodal pattern mining to learn high-level semantically meaningful image patterns.\nThe combination of our ability to find meaningful patterns and name them allow for many applications in high level information extraction tasks, such as knowledge base population using multimodal documents and automatic event ontology creation.\nieeeegbib\n\n\n", {}]}	In this paper we describe a novel framework and algorithms for discovering image patch patterns from a large corpus of weakly supervised image-caption pairs generated from news events. Current pattern mining techniques attempt to find patterns that are representative and discriminative, we stipulate that our discovered patterns must also be recognizable by humans and preferably with meaningful names. We propose a new multimodal pattern mining approach that leverages the descriptive captions often accompanying news images to learn  semantically meaningful image patch patterns. The mutltimodal patterns are then named using words mined from the associated image captions for each pattern. A novel evaluation framework is provided that demonstrates our patterns are 26.2% more semantically meaningful than those discovered by the state of the art vision only pipeline, and that we can provide tags for the discovered images patches with 54.5% accuracy with no direct supervision. Our methods also discover named patterns beyond those covered by the existing image datasets like ImageNet. To the best of our knowledge this is the first algorithm developed to automatically mine image patch patterns that have strong semantic meaning specific to high-level news events, and then evaluate these patterns based on that criteria.
1601.00025.txt	Write a Classifier:  Predicting Visual Classifiers from Unstructured Text Descriptions	{"Introduction": ["\nOne of the main challenges for scaling up object recognition systems is the lack of annotated images for real-world categories. \nTypically there are few images available for training classifiers for most of these categories. This is reflected in the number of images per category available for training in most object categorization datasets, which, as pointed out in Salakhutdinov11, shows a Zipf distribution. \nThe problem of lack of training images becomes even more severe when we target recognition problems within a general category, , fine-grained categorization, for example building classifiers for different bird species or flower types  (there are estimated over 10000 living bird species, similar for flowers). \nResearchers try to exploit shared knowledge between categories to target such scalability issue.\nThis motivated many researchers who looked into approaches that learn visual classifiers from few examples,  deng2010does,fe2003bayesian,BartU05.   This even motivated a more recent work on zero-shot learning of visual categories where there are no training images available for test categories (unseen classes),  Lampert09. Such approaches exploit the similarity (visual or semantic) between seen classes and unseen ones, or describe unseen classes in terms of a learned vocabulary of semantic visual attributes.\n\n\n  \n||FIGURE||\n\nIn contrast to the lack of reasonable size training sets for a large number of real world categories, there are abundant of textual descriptions of these categories. This comes in the form of dictionary entries, encyclopedia articles, and various online resources. For example, it is possible to find several good descriptions of a \"bobolink\" in encyclopedias of birds, while there are only a few images available for that bird online.\n\n The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. In other words, we aim at zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry; see figure <ref>. We explicitly address the question of how to automatically decide which information to transfer between classes without the need of human intervention.  In contrast to most related work, we go beyond the simple use of tags and image captions, and apply standard Natural Language Processing techniques to typical text to learn visual classifiers.\n\n\nFine-grained categorization (also known as subordinate categorization) refers to classification of highly similar objects. This similarity can be due to natural intrinsic characteristics of subordinates of one category of objects (e.g. different breeds of dogs) or artificial subcategories of an object class (different types of airplanes). Diverse applications of fine-grained categories range from classification of natural species wah2011caltech,Flower08,wang2009learning,liu2012dog to retrieval of different types of commercial products maji2013fine. \n\n||FIGURE||In this problem, When we learn from an expert about different species of birds, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarity and differences between species, hierarchal relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about the different species of birds; For example, Fig. <ref> shows an example narrative about the  Bobolink. Typically, the narrative tells you about the bird's taxonomy, highlights discriminative features about that bird and discusses similarities and differences between species, as well as within-species variations (male vs. female). The narrative might eventually show very few example images, which are often selected wisely to illustrate certain visual aspects that might be hard to explain in the narrative.  This learning strategy using textual narrative and images makes the learning effective without a huge number of images  that a typical visual learning algorithm would need to learn the class boundaries.\n\n\n\n\n\nHowever, a narrative about a specific species does not contain only \"visually\" relevant information, but also gives abundant information about the species's habitat, diet, mating habits that is not relevant for visual identification. In a sense, this information might be textual clutter for that task. The same problem takes place in images. While one image can be very effective in highlighting an important feature for learning, many images  might have a lot of visual clutter that makes their uses in learning not effective. Thus, a picture can be worth a thousand words, but not always, and an abundant number of pictures might not be the most effective way for learning. Similarly, one text paragraph can be worth a thousand pictures for learning a concept, but not always, and large amounts of text might not necessarily be effective.\n\n\n\n\n\n\n\nContributions. The contribution of the paper is on exploring this new problem, which to the best of our knowledge, is firstly explored in the computer vision community in this work. We learn from an image corpus and a textual corpus, however not in the form of image-caption pairs, instead the only alignment between the corpora is at the level of the category. In particular, we address the problem of formulating a visual classifier prediction function \u03a6 (\u00b7), which predicts a  classifier of unseen visual class given its text description; see figure <ref>.  While a part of this work was  published in Hoseini13, we extend the work here to study more formulations to solve the problem in Sec. <ref> (B,E). In addition, we also propose a kernel method to explicitly  predict a kernel classifier  in the form defined in the representer theorem rth01.  The kernelized prediction has an advantage that it opens the door for using any kind of side information about classes, as long as kernels can be used on the side information representation.  The side information can be in the form of textual, parse trees, grammar, visual representations, concepts in the ontologies (adopted in NLP domain), or any form. We focus here on unstructured text descriptions.; see figure <ref> The image features also do not need to be in a vectorized format. The kernelized classifiers also facilitates combining different types of features through a multi-kernel learning (MKL) paradigm, where the fusion of different features can be effectively achieved.\n\nWe propose and investigate two baseline formulations based on regression and domain adaptation. Then we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to solve the problem.\n\nBeyond the introduction and the related work sections, the paper is structured as follows: Sec <ref> and  <ref> details the problem definition and relation to regression and knowledge transfer models. Sec <ref> shows different formulations of \u03a6(\u00b7) that we studied to predict a linear visual classifier; see figure <ref>. Section <ref> developed extends our notion where  \u03a6(\u00b7) predicts kernel classifier in the form defined by the representer theorem rth01. Sec <ref> presents our proposed distributional semantic kernel  between unstructured text description, which is applicable to  our kernel formulation and could be useful for other applications. Sec <ref> presents our experiments  on Flower Dataset Flower08 (102 classes) and Caltech-UCSD dataset CU20010 (200 classes) for both the linear and the kernel classifier prediction.\n\n\n\n", {}], "Related Work": ["NewRelatedWork\n\n\n\n\n", {}], "Problem Definition": ["\nFig <ref> illustrates the learning setting. \nThe information in our problem comes from two different domains: the visual domain  and the textual domain, denoted by V and T, respectively. Similar to traditional visual learning problems, we are given training data in the form V={(x_i , l_i)}_N, where x_i is an image and l_i \u2208{1... N_sc} is its class label. We denote the number of classes available at training as N_sc, where sc indicates \"seen classes\". As typically done in visual classification setting, we can learn N_sc binary one-vs-all classifiers, one for each of these classes.\n\nOur goal is to be able to predict a classifier for a new category based only on the learned classes and a textual description(s) of that category. In order to achieve that, the learning process has to also include textual description of the seen classes (as shown in Fig  <ref> ). Depending on the domain we might find a few, a couple, or as little as one textual description to each class. We denote the textual training data for class j by {t_i\u2208T}^j. \nIn this paper we assume we are dealing with the extreme case of having only one textual description available per class, which makes the problem even more challenging. For simplicity, the text description of class j is denoted by t_j. However, the formulation we propose in this paper directly applies to the case of multiple textual descriptions per class.\n\n\n\nIn this paper, we cover predicting  visual classifier \u03a6(t_*) from an unseen text description t_*  in  linear form or RKHS kernalized form, defined as follows\n\n\n\n", {"Linear Classifier": ["\nLet us consider a typical binary linear classifier in the feature space in the form\n\n||FORMULA|| \nwhere x (bold) is the visual feature vector of an image x (not bold) amended with 1   and c_j \u2208R^d_v is the linear classifier parameters for class j. Given a test image, its class is determined by\n\n||FORMULA||\n  \n Similar to the visual domain, the raw textual descriptions have to go through a feature extraction process, which will be described in Sec <ref>. Let us denote the linear extracted textual feature by \nT={t_j \u2208R^d_t}_j=1... N_sc, where t_j is the features of text description t_j (not bold).  Given a textual description t_*  of a new unseen category U  with linear feature vector representation t_*, the problem can now be defined as predicting a one-vs-all linear classifier parameters \u03a6(t_*) = c(t_*) \u2208R^d_v,  such that it can be directly used to classify any test image x as \n\n      c(t_*)^T\u00b7x >  0 & if x belongs to U\n\n      c(t_*)^T\u00b7x <  0 &  otherwise\n\n\n", {}], "Kernel Classifier": ["\nFor kernel classifiers, we assume that each of the domains is equipped with a kernel function corresponding to a reproducing kernel Hilbert space (RKHS). Let us denote the kernel for V by k(\u00b7,\u00b7), and the kernel for T by g(\u00b7,\u00b7). Since, we are studying explicit kernel-classifier prediction from privileged information, we first present an overview on multi-class classification on kernel space.  One\n\nAccording to the generalized representer theorem rth01,  a minimizer of a regularized empirical risk function over an RKHS could be represented as a linear combination of kernels, evaluated on the training set. Adopting the representer theorem on classification risk function, we define a kernel-classifier of a visual class j as follows\n\n\n\n||FORMULA||\nwhere x\u2208V is the test image, x_i is the i^th image in the training data V,  k(x)= [k(x, x_1), ..., k(x, x_N), 1]^T,  \u03b2_j = [\u03b2_j^1 ...\u03b2_j^N, b]^T . Having learned f_j(x^*) for each class j (for example using SVM classifier), the class label of the test image x can be predicted by  Eq. <ref>, similar to the linear case. Eq. <ref> also shows how \u03b2_j is related to c_j in the linear classifier, where k(x, x')= \u03c6(x)^T\u00b7\u03c6(x') and \u03c6(\u00b7) is a feature map  that does not have to be defined given k(\u00b7, \u00b7) on V. Hence, our goal in the kernel classifier prediction  is to predict \u03b2(t_*) instead of c(t_*) since it is sufficient to define  f_t_*(x) for a text description t_* of an unseen class given k(x)\n\nUnder our setting, iIt is clear that f_j(x) could be learned for  all classes with training data j \u22081...N_sc, since there are examples for the seen classes; we denote the kernel-classifier parameters of the seen classes as B_sc =  {\u03b2_j }_N_sc, \u2200 j. However, it is not obvious how to predict f_t_*(x) for an unseen class given its text description t_*. Similar to the linear classifier prediction, our main notion is to use the text description t_*, associated with unseen class, and the training data to directly predict the unseen kernel-classifier parameters. In other words, the kernel classifier parameters of the unseen class is a function of  its text description t_* , the image training data V and the text training data {t_j}, j\u2208 1 ... N_sc; \n||FORMULA||\n  f_t_*(x) could be used to classify new points that\t belong to an  unseen class as follows: 1) one-vs-all setting  f_t_*(x)  \u2277  0  if x^*   belongs to unseen category z^*, \u03b2(t_*),)^T\u00b7k(x^*)< 0  otherwise; or 2) in a Multi-class prediction as in Eq <ref>. \u03a6(t_*) in this case is \u03b2(t_*). In contrast to linear classifier prediction, there is no need to explicitly represent an image x or a text description t by features, which are denoted by the bold symbols in the previous section. Rather, only the  k(\u00b7,\u00b7)  and g(\u00b7,\u00b7) needs to be defined which is general.\n\n\n\n\n", {}]}], "Relation to Regression and Knowledge Transfer Models": ["\nWe introduce two possible frameworks for this problem and discuss potential limitations for them. In this background section,  we focus on predicting linear classifiers for simplicity, which motivates the the evaluated linear classifier formulations that follow in Sec <ref>.\n\n\n", {"Regression Models": ["\nA straightforward way to solve this problem is to pose it as a regression problem where the goal is to use the textual data and the learned classifiers, {(t_j,c_j) }_j=1... N_sc to learn a regression function from the textual feature domain to the visual classifier domain, , a function c(\u00b7) : R^d_t\u2192R^d_v .  The question is which regression model would be suitable for this problem? and would posing the problem this way give reasonable results?\n\nA typical regression model, such as ridge regression ridgeReg70 or Gaussian Process (GP) Regression Rasmussen:2005, learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, ,  a set of functions c^i(\u00b7) : R^d_t\u2192R . Clearly this will not capture the correlation between the visual classifier dimensions. Instead, a structured prediction regressor would be more suitable since it would learn the correlation between the input and output domain. However, even a structured prediction model, will only learn the correlation between the textual and visual domain through the information available in the input-output pairs  (t_j,c_j).  Here the visual domain information is encapsulated in the pre-learned classifiers and prediction does not have access to the original data in the visual domain. Instead we need to directly learn the correlation between the visual and textual domain and use that for prediction.\n\nAnother fundamental problem  that a regressor would face, is the sparsity of the data; the data points are the textual description-classifier pairs, and typically the number of classes can be very small compared to the dimension of the classifier space ( N_sc\u226a d_v). In a setting like that, any regression model is bound to suffer from an under fitting problem. This can be best explained in terms of GP regression, where the predictive variance increases in the regions of the input space where there are no data points. This will result in  poor prediction of classifiers at these regions.\n\n\n", {}], "Knowledge Transfer Models": ["\nAn alternative formulation is to pose the problem as domain adaptation from the textual to the visual domain. In the computer vision context, domain adaptation work has focused on transferring categories learned from a source domain,  with a given distribution of images, to a target domain with different distribution, , images or videos from different sources yang07,saenko10,da11,duan12. \nWhat we need is an approach that learns the correlation between the textual domain features and the visual domain features, and uses that correlation to predict new visual classifier given textual features.\n\nIn particular, in da11 an approach for learning cross domain transformation was introduced. In that work a regularized asymmetric transformation between points in two domains were learned. The approach was applied to transfer learned categories between different data distributions, both in the visual domain. A particular attractive characteristic of da11, over other domain adaptation models, is that the source and target domains do not have to share the same feature spaces or the same dimensionality.\n\nWhile a totally different setting is studied in  da11, it inspired us to formulate the zero-shot learning problem as a domain transfer problem. This can be achieved by learning a linear transfer function W between T and V.  The transformation matrix W can be learned  by optimizing, with a suitable regularizer, over constraints of the form t^TWx\u2265 l  if t\u2208T  and x\u2208V  belong to the same class, and t^TWx\u2264 u   otherwise. Here l and u are model parameters. This transfer function acts as a compatibility function between the textual features and visual features, which gives high values if they are from the same class and a low value if they are from different classes.\n\nIt is not hard to see that this transfer function can act as a classifier. Given a textual feature t^* and a test image, represented by x, a classification decision can be obtained by t_*^TWx\u2277 b where b is a decision boundary which can be set to (l+u)/2. Hence, our desired predicted classifier in Eq <ref> can be obtained as c(t_*) = t_*^TW  (note that the features vectors are amended with ones).  However, since learning  W was done over seen classes only, it is not clear how the predicted classifier c(t_*) will behave for unseen classes. There is no guarantee that such a classifier will put all the seen data on one side and the new unseen class on the other side of that hyperplane.\n\n\n\n\n\n\n\n\n\n", {}]}], "Formulations for Predicting a linear   classifier form of": ["\u03a6 (t_*)\nThe proposed formulations in this section aims at predicting a linear hyperplane parameter c of a one-vs-all classifier for a new unseen class given a textual description, encoded as a feature vector t_* and the knowledge learned at the training phase from seen classes[The notations follow from Subsection <ref>]. We start by  defining the learning components that were used by the formulations described in this section:\n\n\n  Classifiers:        \n\na set of linear one-vs-all classifiers {c_j} are learned, one for each seen class.\n\n  Probabilistic Regressor:             \n\nGiven {(t_j,c_j)} a regressor is learned that can be used to give a prior estimate for p_reg(c | t) (Details in Sec <ref>).\n\n  Domain Transfer:            \n\nGiven T and V a domain transfer function, encoded in the matrix W is learned, which captures the correlation between the textual and visual domains (Details in Sec <ref>).\n \n  \n  \n  \nEach of the following subsections show a different approach to predict a linear classifier from t_* as \u03a6(t_*) = c(t_*); see Sec <ref>. The final approach (E) combines  regression, domain transfer, and additional constraints, which achieves the best performance. We compare between these alternative formulations in our experiments. Hyper-parameter selection is detailed in the supplementary materials for all the approaches. \n||FIGURE||\n\n\n", {"Probabilistic Regressor": ["\nThere are different regressors that can be used, however we need a regressor that provide a probabilistic estimate p_reg(c | (t)). For the reasons explained in Sec <ref>, we also need a structure prediction approach that is able to predict all the dimensions of the classifiers together. For these reasons, we use the Twin Gaussian Process (TGP) Bo:2010. \nTGP encodes the relations between both the inputs and structured outputs using Gaussian Process priors. This is achieved by minimizing the Kullback-Leibler divergence between the marginal GP of the outputs (i.e. classifiers in our case) and observations (i.e. textual features). The estimated regressor output (c\u0303(t_*)) in TGP is given by the solution of the following non-linear optimization problem Bo:2010[notice we are using c\u0303 to denote the output of the regressor, while using \u0109 to denote the output of the final optimization problem in Eq <ref>].\n\n||FORMULA||\nwhere u = (K_T + \u03bb_t  I)^-1 k_t(t_*), \u03b7  = K_T(t_*,t_*) -k(t_*)^Tu ,  K_T(t_l,t_m)  and K_C(c_l,c_m) are Gaussian kernel for input feature t and output vector c.  k_c(c) = [K_C(c,c_1), ..., K_C( c,c_N_sc)]^T. k_t(t_*) = [K_T(t_*,t_1), ..., K_T(t_*,t_N_sc)]^T.  \u03bb_t and \u03bb_c are regularization parameters to avoid overfitting. This optimization problem can be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection Bo:2010. In this case the classifier dimension are predicted jointly. In this case p_reg(c|t_*) is defined as a normal distribution.\n\n||FORMULA||\nThe reason that \u03a3_c = I is that TGP does not provide predictive variance, unlike Gaussian Process Regression. However, it has the advantage of handling the dependency between the dimensions of the classifiers c given the textual features t. \n \n\n", {}], "Constrained Probabilistic Regressor": ["\nWe also investigated formulations that use regression to predict an initial hyperplane  c\u0303(t_*) as described in section  <ref>, which is then optimized to put all seen data in one side, \n||FORMULA||\n\nwhere \u03c8(\u00b7,\u00b7) is a similarity function between hyperplanes,  a dot product used in this work,  \u03b1 is its constant weight, and C is the weight to the soft constraints of existing images as negative examples (inspired by linear SVM formulation), or other functions incorporating the predictive variance. We call this class of methods  constrained GPR/TGP, since c\u0303(t_*) is initially predicted through GPR or TGP.\n\n\n\n", {}], "Domain Transfer (DT)": ["\nTo learn the domain transfer function W we adapted the approach in da11 as follows. Let T be the textual feature data matrix and X be the visual feature data matrix where each feature vector is amended with a 1. Notice that amending the feature vectors with a 1 is essential in our formulation since we need t^TW to act as a classifier. We need to solve the following optimization problem \n\n||FORMULA||where c_i's are loss functions over the constraints and r(\u00b7) is a matrix regularizer.  It was shown in da11, under condition on the regularizer, that the optimal W  is in the form of \n  W^* = TK_T^-1/2L^*  K_X^-1/2X^T, where  K_T  = TT^T,  K_X  = XX^T.   L^* is computed by minimizing the following minimization problem\n\n||FORMULA||\nwhere c_p(K_T^1/2LK_X^1/2  ) = (max(0, (l-e_i K_T^1/2LK_X^1/2 e_j) ))^2 for same class pairs of index i,j, or  =(max(0, (e_i K_T^1/2LK_X^1/2 e_j -u) ))^ 2 otherwise, where e_k is a one-hot vector of zeros except a one at the k^th element, and u>l (note any appropriate l, u could work. In our case, we used l =2, u=-2 ). We used a Frobenius norm regularizer.  This energy is minimized using a second order BFGS quasi-Newton optimizer. Once L is computed W^* is computed using the transformation above. Finally \u03a6 (t_*)  = c(t_*) = t_*^TW, simplifying  W^* as W.\n\n\n\n\n\n", {}], "Constrained-DT": ["\nWe also investigated constrained-DT formulations that learns a transfer matrix W and  enforce t_j^TW to be close to the classifiers learned on seen data, {c_j } ,\n||FORMULA||\nA classifier can be then obtained by \u03a6 (t_*)  =c(t_*)= t_*^TW.\n\n\n\n||FORMULA||\nThe first term is a regularizer over the classifier c.  The second term enforces that the predicted classifier has high correlation with t_*^TW. The third term favors a classifier that aligns with the prediction of the regressor c\u0303(t_*). The constraints c^Tx_i\u2265\u03b6_i   enforce that all  seen data instances are at the negative side of the predicted hyperplane with some missclassification allowed through the  slack variables  \u03b6_i. The constraint  t_*^TWc\u2265 l enforces that the correlation between the predicted classifier and t_*^TW is no less than l,  a minimum correlation between the text and visual features. \nGiven W, and the form of the probability estimate  p_reg(c|t_*), the optimization reduces to a quadratic program on c with linear constraints.\n\n\n\n", {}], "Constrained  Regression and Domain Transfer for classifier prediction": ["\n Fig <ref> illustrates our final  framework which combines regression (formulation A (using TGP)) and domain transfer (formulation C) with additional constraints. This formulation combines the three learning components described in the beginning of this section. \nEach of these components contains partial knowledge about the problem. The question is how to combine such knowledge to predict a new classifier given a textual description. The new classifier has to be consistent with the seen classes. \nThe new classifier has to put all the seen instances at one side of the hyperplane, and has to be consistent with the learned domain transfer function. This leads to the following constrained  optimization problem \n\n||FORMULA||\nThe first term is a regularizer over the classifier c.  The second term enforces that the predicted classifier has high correlation with t_*^TW; W is learnt by Eq <ref>. The third term favors a classifier that has high probability given the prediction of the regressor. The constraints  -c^Tx_i\u2265\u03b6_i   enforce all the seen data instances to be at the negative side of the predicted classifier hyperplane with some missclassification allowed through the  slack variables  \u03b6_i. The constraint  t_*^TWc\u2265 l enforces that the correlation between the predicted classifier and t_*^TW is no less than l, this is to enforce a minimum correlation between the text and visual features.\n\n\nSolving for \u0109 as a quadratic program: \nAccording to  the definition of p_reg(c|t_*) for  TGP,   p(c|t_*) is a quadratic term in c in the form  \n\n||FORMULA||\nWe reduce - p(c|t_*) to -2 c^Tc\u0303(t_*)), since 1) c\u0303(t_*)^Tc\u0303(t_*) is a constant ( does not affect the optimization), 2) c^Tc is already included as regularizer in equation  <ref>.  In our setting, the dot product  is a better similarity measure between two hyperplanes. Hence,  -2 c^Tc\u0303(t_*) is minimized.\nGiven - p(c|t_*) from the TGP and  W, Eq <ref> reduces to a quadratic program on c with linear constraints. We tried different quadratic solvers, however the IBM CPLEX solver [http://www-01.ibm.com/software/integration/optimization/cplex-optimizer] gives the best performance in speed and optimization for our problem.\n\n\n\n\n\n\n\n\n\n", {}]}], "Formulations for Predicting a kernel   classifier form of": ["\u03a6 (t_*) \nPrediction of \u03a6 (t_*)  = \u03b2(t_*) (Sec. <ref>), is decomposed into training (domain transfer) and prediction phases, detailed as follows\n\n", {"Kernelized Domain Transfer": ["\nDuring training, we firstly learn B_sc = {\u03b2_j }, j=1\u2192 N_sc as SVM-kernel classifiers based on  the training data and defined by k(\u00b7, \u00b7) visual kernel, see Sec <ref>. Then, we learn a kernel domain transfer function to transfer the text description information t_*\u2208T to kernel-classifier parameters \u03b2\u2208R^N+1 in V domain. We call this domain transfer function \u03b2_DA(t_*), which has the form as \u03a8^Tg(t_*), where g(t_*)  = [g(t_*, t_1) ... g(t_*, t_N_sc)]^T, g(t, t')  is a kernel function that measures the similarity between t and t' on  domain E; \u03a8 is an N_sc\u00d7N+1 matrix, which transforms t to  kernel classifier parameters for the class that t_* represents.\n\n\n\nInspired by the domain transfer proposed by da11, wWe aim to learn  \u03a8 from V and {t_j}, j=1 ... N_sc, such that g(t)^T\u03a8k(x) > l if t and  x  correspond to the same class, g(t)^T\u03a8k(x) < u  otherwise. Here l  controls similarity lower-bound if t and x correspond to  same class, and u controls similarity upper-bound if t and x belong to different classes. In our setting, the term   \u03a8^Tg(t_j) should act as a classifier parameter for class j in the training data. Therefore,  we introduce  penalization constraints to our minimization function  if  \u03a8^T g(t_j) is distant from \u03b2_j \u2208B_sc, where t_i corresponds to the class that \u03b2_i classifies.Hence,  in order to learn T , we solve the following objective function  Inspired by domain adaptation [A totally different problem/setting but the optimization methods inspired our solution] optimization methods (da11),  we model our solution using the following objective functionInspired by domain adaptation optimization methods (da11) [A totally different problem/setting but the optimization methods inspired our solution],  in order to learn T,, we model the kernel domain transfer function as follows by the following objective function\n||FORMULA||\n where, \nG   is an N_sc\u00d7 N_sc  symmetric matrix, such that both the i^th    row and the i^th  column are equal to g(t_i), i=1: N_sc; K    is an N+1 \u00d7 N  matrix, such that the i^th  column is equal to k(x_i), x_i, i=1:N.\nc_k's are loss functions over the constraints defined as\n  c_k(G \u03a8 K)) = (max(0, (l-1_i^TG \u03a8 K1_j) ))^2  for same class pairs of index i  and j,  or  =r\u00b7(max(0, (1_i^TG \u03a8 K1_j -u) ))^ 2  otherwise, where 1_i  is an N_sc\u00d7 1  vector with all zeros except at index i, 1_j  is an N \u00d7 1  vector with all zeros except at index j. This leads to that    c_k(G \u03a8 K)) = (max(0, (l-g(t_i)^T \u03a8 k(x_j) ))^2  for same class pairs of index i  and j, or  =r\u00b7(max(0, (g(t_i)^T \u03a8 k(x_j) -u) ))^ 2 otherwise, where u>l (note any appropriate l, u could work in our case we used l =2, u=-2 ), r = nd/ns  such that nd  and ns  are the number of pairs (i,j)  of different classes and similar pairs respectively. r(\u00b7)  is a matrix regularizer; Finally, we used a Frobenius norm regularizer for r(\u03a8).\n\n\nThe objective function in Eq  <ref>  controls the involvement of the constraints c_k  by the term multiplied by \u03bb_1, which controls its importance; we call it C_l,u(\u03a8). While, the trained classifiers penalty is captured by the term multiplied by \u03bb_2; we call it C_\u03b2(\u03a8). One important observation on  C_\u03b2(\u03a8), is that it reaches zero when \u03a8 = G^-1B^T, where B  = [\u03b2_1 ...\u03b2_N_sc], since it could be rewritten as C_\u03b2(\u03a8) = B^T - G \u03a8_F^2. Our intuition is that for the model to have good generalization, the effect of C_\u03b2(T)  should be  minimal (\u03bb_2 \u2192 0), since this case indicates successful modeling of the transfer from E  domain to the kernel-classifier parameters in X  domain. \nIn contrast to the linear-classifier restricted approach proposed by Elhoseiny et al    Hoseini13, our domain transfer model can transfer any type of classifier of an arbitrary kernel from T to V. Furthermore, the classifier penalty term was not studied in Hoseini13, which is captured here by C_\u03b2(T).\n\nWe minimize L(\u03a8)  is by gradient-based optimization using a second order BFGS quasi-Newton optimizer. Our  gradient derivation of L(\u03a8)  leads to the following form\n\n||FORMULA||\nwhere v_ij = - 2 \u00b7 max(0, (l-g(t_i)^T \u03a8 k(x_j) )  if i  and j  correspond to the same class, 2 \u00b7 max(0, (g(t_i)^T \u03a8 k(x_j) -u )  otherwise. Another approach that can be used to minimize L(\u03a8)  is through alternating projection using Bregman algorithm bregman97, where \u03a8  is updated by a single constraint every iteration.\n\n\n\n\n", {}], "Kernel Classifier Prediction": ["\nWe study two ways to infer the final kernel-classifier prediction. (1) Direct Kernel Domain Transfer Prediction, denoted by \"DT-kernel\", (2) One-class SVM adjusted DT Prediction, denoted by \"SVM-DT kernel\". Hyper-parameter selection is attached in the supplementary materials. The source code is available here \n<https://sites.google.com/site/mhelhoseiny/computer-vision-projects/write_kernel_classifier>.\n\nDirect Domain Transfer (DT) Prediction: By construction a classifier of an unseen class can be directly computed from our trained domain transfer model as follows\n\n||FORMULA||One-class-SVM adjusted DT (SVM-DT) Prediction: \nIn order to increase separability against seen classes, we adopted the inverse of the idea of the one class kernel-svm, whose main idea is to build a confidence function that takes only positive examples of the  class. Our setting is the opposite scenario; seen examples are negative examples of the unseen class.\nIn order introduce our proposed adjustment method, we  start by presenting the one-class SVM objective function. The  Lagrangian dual  of the one-class SVM oneclasssvm07 can be written as\n\n||FORMULA||\nwhere K^'   is an N \u00d7 N  matrix, K^' (i,j) = k(x_i, x_j), \u2200x_i,x_j \u2208S_x ( in the training data), a  is an N \u00d7 1  vector, a_i = k(x_i, x_i), C  is a hyper-parameter . It is straightforward to see that, if \u03b2 is aimed to be a negative decision function instead, the objective function becomes in the form\n\n||FORMULA||\nWhile \u03b2^*_-  = - \u03b2^*_+, the objective function in Eq <ref> of the one-negative class SVM inspires us with the idea to adjust the kernel-classifier parameters to increase separability of the unseen kernel-classifier against the points of the seen classes, which leads to the following objective function \n\n||FORMULA||\nwhere  \u03b2\u0302_DT  is the first N  elements in \u03b2\u0303_DT(t^*) \u2208R^N+1, 1  is an N \u00d7 1  vector of ones. The objective function, in Eq <ref>,  pushes the classifier of the unseen class to be highly correlated with the domain transfer prediction of the kernel classifier, while putting  the points of the seen classes as negative examples. It is not hard to see that Eq <ref> is a quadratic program in \u03b2, which could be solved using any quadratic solver.In contrast to our formulation, the approaches presented in NIPS13DeViSE,NIPS13CMT,Hoseini13 assumes that X\u2208 R^d_b and  E\u2208 R^d_E  (  vectorized). It is worth to mention that,  linear classifier prediction in Eq <ref> (best Linear formulation in our results)  predicts  classifiers by solving an optimization problem of size  N+d_v+1   variables, d_v+1  linear-classifier parameters, which is the same as the length of the visual feature vector, and N  slack variables; a similar limitation can be found in NIPS13DeViSE,NIPS13CMT where the architecture depends on the number on visual features.  In contrast, the kernelized objective function (Eq <ref>) solves a  quadratic program of only N  variables, and  predicts a kernel-classifier instead with fewer parameters. Using very high-dimensional features  will not affect the optimization complexity.  \nTherefore, it is clear that the kernel formulation is expected to have better generalization properties. In addition, the kernel-approach does not assume that any of V  and  T  is a vector space.\n\n\n\n\n\n \n\n", {}]}], "Distributional Semantic (DS) Kernel for text  descriptions": ["\nWe propose a distributional semantic kernel g(\u00b7, \u00b7) = g_DS(\u00b7, \u00b7)  to define the similarity between two text descriptions in T domainof visual classes in our setting. While this kernel is applicable to  kernel classifier predictors  presented in Sec <ref>, it could be used for other applications. We start by  distributional semantic models by mikolov2013distributed,mikolov2013efficient to represent the semantic manifold M_s, and a function vec(\u00b7) that maps a word to a K\u00d7 1 vector in M_s. The main assumption behind this class of distributional semantic model  is that similar words share similar context. Mathematically speaking, these models  learn a vector for each word w_n, such  that p(w_n|(w_n-L, w_n-L+1, ...,  w_n+L-1,w_n+L) is maximized over the training corpus, where 2\u00d7 L is the context window size. Hence similarity between vec(w_i) and vec(w_j) is high if they co-occurred a lot in context of size 2\u00d7 L in the training text-corpus. We normalize all the word vectors to length 1 under L2 norm, i.e.,  vec(\u00b7) ^2=1.\n\nLet us assume a  text description D that we represent by a set of triplets D = {(w_l,f_l, vec(w_l)), l=1... M}, where w_l is a word that occurs in D with frequency f_l and its corresponding word vector is vec(w_l) in M_s. We drop the stop words from D. We define  F = [f_1, ..., f_M]^T and P = [vec(w_1), ..., vec(w_M)]^T, where F is an M\u00d71  vector of term frequencies and P is an M \u00d7 K matrix of the corresponding term vectors.\n\nGiven two text descriptions D_i and D_j which contains M_i and M_j terms respectively. We compute P_i (M_i \u00d7 1) and V_i (M_i \u00d7 K) for  D_i  and P_j (M_j \u00d7 1) and V_j (M_j \u00d7 K) for  D_j. Finally  g_DS(D_i, D_j) is defined as \n\n||FORMULA||\nOne advantage of this similarity measure is that it captures semantically related terms. It is not hard to see that the standard Term Frequency (TF) similarity could be thought as a special case of this kernel where vec(w_l)^T vec(w_m)=1 if w_l=w_m, 0 otherwise, i.e., different terms are orthogonal. However, in our case the word vectors are learnt through a distributional semantic model which makes semantically related terms have higher dot product (vec(w_l)^T vec(w_m)).\n\n\n\n\n\n\n", {}], "Experiments": ["\n\n", {"Datasets and Features": [" Datasets:\nWe used  the CU200 Birds CU20010 (200 classes - 6033 images) and the Oxford Flower-102 Flower08 (102 classes -  8189 images) image dataset to test our methods, since they are among the largest and widely used fine-grained datasets.  We created  textual descriptions for each class in both datasets. The CUB200 Birds image dataset was created based on birds that have a corresponding Wikipedia article, so we have developed a tool to automatically extract Wikipedia articles given the class name. The tool succeeded to automatically generate 178 articles, and the remaining 22 articles was extracted manually from Wikipedia. These mismatches happens only when article title is a different synonym of the same bird class. On the other hand, Flower image dataset was not created using the same criteria as the Bird dataset, so classes of the Flower dataset classes does not necessarily have corresponding Wikipedia article. The tool managed to generate only 16 classes from Wikipedia out of 102, the remaining 86 articles was generated manually for each class from Wikipedia, Plant Database  [http://plants.usda.gov/java/], Plant Encyclopedia [http://www.theplantencyclopedia.org/wiki/Main_Page], and BBC articles [http://www.bbc.co.uk/science/0/]. The collected textual description for FLower and Birds dataset are available here  <https://sites.google.com/site/mhelhoseiny/1202-Elhoseiny-sup.zip> .\n\n Textual Feature Extraction:\nThe textual features were extracted in two phases, which are typical in document retrieval literature. The first phase is an indexing phase that generates textual features with tf-idf (Term Frequency-Inverse Document Frequency) configuration (Term frequency as local weighting while inverse document frequency as a global weighting). The tf-idf is a measure of how important is a word to a text corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to control for the fact that some words are generally more common than others. We used the normalized frequency of a term in the given textual description salton1988term. The inverse document frequency is a measure of whether the term is common; in this work we used the standard logarithmic idf salton1988term.   \nThe second phase is a dimensionality reduction step, in which  Clustered Latent Semantic Indexing (CLSI) algorithm clsi05 is used. CLSI is a low-rank approximation approach for dimensionality reduction, used for document retrieval. In the Flower Dataset, tf-idf features \u2208R^8875 and after CLSI the final textual features \u2208R^102. In the Birds Dataset, tf-idf features is in R^7086 and after CLSI the final textual features is in R^200.\n\n\n||TABLE||Textual features specifications\n\n\n\n\n  Visual features Extraction:\nWe used the Classeme features classemes as the visual feature for our experiments since they provide an intermediate semantic representation of the input image. Classeme features are output of a set of classifiers corresponding to a set of C category labels, which are drawn from an appropriate term list defined in classemes, and not related to our textual features. For each category c \u2208 1 ... C  , a set of training images is gathered by issuing a query on the category label to an image search engine.\nAfter a set of coarse feature descriptors (Pyramid HOG, GIST, ) is extracted, a subset of feature dimensions was selected classemes, and  a one-versus-all classifier \u03c6_c is trained for each category. The classifier output is real-valued, and is such that \u03c6_c(x) > \u03c6_c(y) implies that x is more similar to class c than y is. Given an image x,  the feature vector (descriptor) used to represent it is the classeme vector   [\u03c6_1 (x),  ..., \u03c6_d_v (x)], d_v=2569.\n\nFor Kernel classifier prediction, we evaluated  these features and also additional representations  for text descriptions (by the proposed distributional semantic kernel using word embedding) and  images ( (a) CNN features and (b)  combined kernel over different features learnt by MKL (multiple kernel learning)), discussed later in Subsection <ref>.\n\n\n||FIGURE||\n\n\n\n||TABLE||0.7\n||TABLE||\n\n\n\n", {}], "Experimental Results for Linear Classifier Prediction": ["\n\n\n Evaluation Methodology: Similar to zero-shot learning literature, we evaluated the performance of an unseen classifier in a one-vs-all setting where the test images of unseen classes are considered to be the positives and the test images from the seen classes are considered to be the negatives. We computed the ROC curve and report the area under that curve (AUC) as a comparative measure of different approaches. In zero-shot learning setting the test data from the seen class are typically very large compared to those from unseen classes. This makes other measures, such as accuracy, useless since high accuracy can be obtained even if all the unseen class test data are wrongly classified; hence we used ROC curves, which are independent of this problem. Five-fold cross validation over the classes were performed, where in each fold 4/5 of the classes are considered as \"seen classes\" and are used for training and 1/5th of the classes were considered as \"unseen classes\" where their classifiers are predicted and tested. Within each of these class-folds, the data of the seen classes are further split into training and test sets. The hyper-parameters for the  approach were selected through another five-fold cross validation within the class-folds  (i.e. the 80% training classes are further split into 5 folds to select the hyper-parameters). We made the seen-unseen folds used in our experiments available here <https://sites.google.com/site/mhelhoseiny/computer-vision-projects/Write_a_Classifier>.\n\n Baselines:\nSince our work is the first to predict classifiers based on pure textual description, there are no other reported results to compare against. However, we designed three state-of-the-art baselines to compare against, which are designed to be inline with our argument in Sec <ref>. Namely we used: 1) A Gaussian Process Regressor (GPR) Rasmussen:2005, 2) Twin Gaussian Process (TGP) Bo:2010 as a structured regression method, 3)  Domain Transfer (DT) da11. The TGP and DT baselines are of particular importance since our formulation utilizes them, so we need to test if the formulation is making any improvement over them. It has to be noted that we also evaluate TGP and DT as alternative formulations that we are proposing for the problem, none of them was used in the same context before. \n||TABLE||\n\n\n\n\n\n\n Results:\nTable <ref> shows the average AUCs for the final linear approach in comparison to the three baselines on both datasets. GPR performed poorly in all classes in both data sets, which was expected since it is not a structure prediction approach.  The DT formulation outperformed TGP in the flower dataset but slightly underperformed on the Bird dataset. The proposed approach outperformed all the baselines on both datasets, with significant difference on the flower dataset. It is also clear that the TGP performance was improved on the Bird dataset since it has more classes (more points are used for prediction). Fig  <ref> shows the ROC curves for our approach on best predicted unseen classes from the Birds dataset on the Left  and Flower dataset on the middle. Fig  <ref> shows the AUC for all the classes on Flower dataset. \n||FIGURE||\n||FIGURE||\n||FIGURE||\n||FIGURE||\n\n\n\n||TABLE||\n\n||FIGURE||\nFig  <ref>, on the right, shows the improvement over (A) GPR,\n||FIGURE|| A(TGP), and (C) DT for each class, where the improvement is calculated as (our AUC- baseline AUC)/ baseline AUC %. \nTable <ref> shows the percentage of the classes which our approach makes a prediction improvement for each of  the three baselines. The last row show the improvement over both the TGP and DT baseline \n\n. Table <ref> shows the five classes in Flower dataset where our approach made the best average improvement.\n. (TGP+DT).  \n\n The point of that table is to show that in these cases both TGP and DT did poorly while our formulation that is based on both of them did significantly better. This shows that our formulation does not simply combine the best of the two approaches but can significantly improve the prediction performance.\n\n\n||TABLE||\n\n\n\n\n\n\n\n\nTo evaluate the effect of the constraints in the objective function, we removed the constraints - (c^Tx_i ) \u2265\u03b6_i which try to enforces all the seen examples to be on the negative side of the predicted classifier hyperplane and evaluated the approach. The result on the flower dataset (using one fold) was reduced to average AUC=0.59 compared to AUC=0.65 with the constraints. Similarly, we evaluated the effect of the constraint  t_*^TWc> l. The result was reduced to average AUC=0.58 compared to AUC=0.65 with the constraint.  This illustrates the importance of this constraint in the formulation.\n\n Constrained Baselines:We computed the ROC curves and report the area under that curve (AUC) as a comparative measure[In zero-shot learning setting the test data from the seen class are typically very large compared to those from unseen classes. This makes other measures, such as accuracy, useless since high accuracy can be obtained even if all the unseen class test data are wrongly classified; hence we used ROC curves, which are independent of this problem.] Five-fold cross validation over the classes were performed, within each of these class-folds, the data of the seen classes are further split into training and test sets.\nTable <ref> (bottom three lines) also shows the average AUCs for the constrained baselines formulations, namely Constrained GPR, TGP and DT; see section <ref>. Even though the visual features and textual features were independently extracted, by learning correlation between them, we can predict classifiers for new categories. As shown previously, GPR performed poorly, while, as expected, TGP performed better. Adding constraints to GPR/TGP improved their performance. Combining regression and DT gave significantly better results for classes where both approaches individually perform poorly, as can be seen in Table <ref>-right. We performed an additional experiment, where  W is firstly computed using Constrained Domain Transfer (CDT). Then, the unseen classifier is predicted using equation <ref> with \u03b3=0, which performs worse. This indicates that adding constraints to align to seen classifiers hurts the learnt domain transfer function on unseen classes. In conclusion, the final formulation that combines TGP and DT with additional constraints performs the best in both Birds and Flower datasets, where the effect of TGP is very limited since it was trained on sparse points.\n\n\n\n\n\n", {}], "Experimental Results for Kernel Classifier Prediction": ["\n\n\n\nNow that we have described our zero-shot learning setting and the suggested approaches to directly predict kernel-classifier parameters for unseen classes, we present several experiments to validate our model.\nIn this section, we presented a set of experiments, conducted to evaluate our proposed model for zero-shot learning of visual classifiers. The quantitative comparisons show our superior performance to the state of the art on two challenging datasets of fine-grained object categories.\n", {"Additional Evaluation Metrics": ["\n\nIn addition to the the AUC, we already studied in the previous section. We report two additional metrics while evaluating and comparing the kernel classifier prediction to the linear classifier prediction, detailed as follows.\n\n|N_sc|  to |N_sc+1|  Recall:\nUnder this metric, we aim to check  how   the learned classifiers of the seen classes confuse the predicted classifiers, when they are involved in a multi-class classification problem of N_sc + 1  classes. The first N_sc  classifiers are those of the seen classes, while (N_sc+1)^st classifier is a predicted classifier for an unseen class. We use Eq <ref> to predict label l^*  with the maximum confidence of an image x^*, such that l^* \u2208L_sc\u222a l_us,  l_us  is the label of the ground truth unseen class, and L_sc is the set of seen class labels. We compute the recall under this setting. This metric is computed for each predicted unseen classifier and the average is reported.\n\nMulticlass Accuracy of Unseen classes (MAU): Under this setting, we aim to evaluate the performance of  the unseen  classifiers against each others. Firstly, the classifiers of all unseen categories are predicted. Then, we use Eq <ref> to predict the label with the maximum confidence of a test image x, such that its label l_us^* \u2208L_us, where L_us is the set of all unseen class labels that only have text descriptions.\n\n\n\n\n", {}], "Comparisons to Linear Classifier Prediction": ["\n We compare the kernel methods to the linear prediction discussed earlier,  which predicts a linear classifier from textual descriptions  ( T  space in our framework). The aspects of the comparison includes  1) whether the predicted kernelized classifier outperforms the predicted linear classifier  2) whether this behavior is consistent on multiple datasets. We performed the comparison on both Birds and Flower dataset.  For these experiments, in our setting, domain V  is the visual domain and domain T  is the textual domain, , the goal is to predict classifiers from pure textual description. We used the same features on the visual domain  and the textual domains detailed in subsection <ref>. That is,  classeme features classemes for images, extracted from images of the Bird and the Flower datasets and tf-idf salton1988term features for text articles followed by a CLSI clsi05 dimensionality reduction phase. We denote our kernel Domain Transfer prediction  and one class SVM adjust DT prediction by  \"DT-kernel\" and \"SVM-DT-kernel\" respectively. We compared against  linear classifier prediction (Linear Formulation (E) approach, denoted by just Linear Classifier).  (which uses a quadratic program to optimize the classifier parameters)We also compared against the  linear direct domain transfer (Linear Formulation (C), denoted by DT-linear).  In our kernel approaches, we used Gaussian rbf-kernel as a similarity measure in T  and V  spaces (k(d,d') = exp(-\u03bb ||d-d'||)).\n\n \n\n||TABLE||\n\n0.943\n||TABLE||0.93\n||TABLE||\n   \n Kernel: MAU on a seen-unseen split-Birds Dataset (MKL)0.93\n||TABLE||\n\nKernel: MAU on a seen-unseen split-Birds Dataset (CNN image features, text description)0.93\n||TABLE||\n\n\n\n\n\nRecall metric :  The recall of the SVM-DT kernel approach is 44.05% for Birds and 40.34% for Flower, while it is 36.56% for Birds and  31.33% for Flower by best Linear Classifier prediction (E). This indicates that the  predicted classifier is less confused by the classifiers of the seen compared with  Linear Classifier prediction; see table  <ref> (top part)\n\n\nMAU metric: It is worth to mention that the multiclass accuracy for the trained seen classifiers is 51.3% and 15.4%, using the classeme features,  on Flower dataset and Birds dataset[Birds dataset is known to be a challenging dataset for fine-grained, even when applied in a regular multiclass setting as it is clear from the 15.4% performance on seen classes], respectively. Table  <ref> (middle part) shows the average MAU metric over three seen/unseen splits for Flower dataset and one split on Birds dataset, respectively. \nFurthermore, the relative improvements of our  SVM-DT-kernel approach is reported against the baselines. On Flower dataset,  it is interesting to see that our approach achieved 9.1% MAU, 182% the random guess performance, , which is 17.7% of the multi-class accuracy of the seen classes (i.e. 51.3%),by predicting the unseen classifiers using just textual features as privileged information (i.e. T domain). It is important to mention that we achieved also 13.4%, 268% the random guess performance, in one of the splits (the 9.1% is the average over 3 seen/unseen splits). Similarity on Birds dataset, we achieved 3.4% MAU from text features, 132%  the random guess performance (further improved up to 224% in next experiments)., which is 22.7% of the multi-class accuracy of the seen classes on the same dataset (15.4%)\n\n\nAUC metric:  Fig <ref> <ref> (top part) shows the ROC curves for our approach on the best predicted unseen classes from the Flower dataset. Fig <ref> <ref> (bottom part) shows the AUC for all the classes on Flower dataset (over three different splits). Table <ref> (bottom part)  shows the average AUC on the two datasets, compared to the baselines. More results and figures Corresponding figures for Birds dataset  for our kernel approach are attached in the supplementary materials.\n\nLooking at table <ref>, we can notice that the proposed approach performs marginally similar to the baselines from AUC perspective. However, there is a clear improvement  in MAU  and Recall metrics. These results show the advantage of predicting classifiers in kernel space. Furthermore, the table shows that our SVM-DT-kernel approach outperforms our DT-kernel model. This indicates the advantage of the class separation, which is adjusted by the SVM-DT-kernel model. In all these experiments, we used a setting of our SVM-DT-kernel model, where C_\u03b2(T )  is ignored (i.e. \u03bb_2 = 0); see Sec  <ref>). In order to study whether C_\u03b2( T )  is effective in unseen class prediction, we performed an extra experiment on Birds dataset, where \u03bb_2>0  (e.g. \u03bb_2 =1). We found that MAU of our DT approach has slightly decreased (i.e from 2.95&  to 2.91% ). Under the same setting, we also found that  C_\u03b2( T )   slightly reduced the performance of SVM-DT from 9.1% to 8.98%.MAU. This reflect our intuition argued in the approach sectionSec  <ref>. Hence, we suggest to assign \u03bb_2  to 0 for our purpose. More details on the hyper-parameter selection are attached in the supplementary materials. \n\n||FIGURE||\n\n\n\n\n\n\n\n\n\n||FIGURE||\n\n\n\n\n||FIGURE||\n||TABLE||\n\n\n\n||TABLE||\n\n\n\n\n||TABLE||\n\n\n\n||TABLE||\n\n\n\n||FIGURE||\n\n\n\n\n||FIGURE||\n\n\n||TABLE||\n\n\n\n\n||TABLE||\n\n\n\n\n\n\n\n||TABLE||\n\n||TABLE||\n\n\n\n", {}], "Multiple Kernel Learning (MKL) Experiment": ["\n\nThis experiment shows the added value of  proposing a kernelized zero-shot learning approach. We conducted an experiment where the final kernel on the visual domain is produced by Multiple Kernel Learning MKKLAlgs11. For the visual domain, we extracted kernel descriptors for Birds dataset. Kernel descriptors provide a principled way to turn any pixel attribute to patch-level features, and are able to generate rich features from various recognition cues. We specifically used four types of kernels introduced by bo_nips10 as follows: Gradient Match Kernels that captures image variation based on predefined kernels on image gradients. Color Match Kernel that describes patch appearance using two kernels on top of RGB and normalized RGB for regular images and intensity for grey images. These kernels capture image variation and visual apperances. For modeling the local shape, Local Binary Pattern kernels have been applied. We computed these kernel descriptors on local image patches with fixed size 16 x 16 sampled densely over a grid with step size 8 in a spatial pyramid setting with four layers. The dense features are vectorized using codebooks of size 1000. This process ended up with a 120,000 dimensional feature for each image (30,000 for each type). Having extracted the four types of descriptors, we compute an rbf kernel matrix for each type separately. We learn the bandwidth parameters for each rbf kernel by cross validation on the seen classes. Then, we generate a new kernel k_mkl(d, d') = \u2211_i=1^4 w_i k_i(d, d'), such that w_i is a weight assigned to each kernel. We learn these weights by applying Bucak's Multiple Kernel Learning algorithm nips10_Bucak. Then, we applied our approach where the MKL-kernel is used in the visual domain and rbf kernel in the text domain similar to the previous experiments.\n\n\n\n To compare the kernel prediction approach to the linear prediction approach (formulation (E)) under this setting,  we concatenated  all kernel descriptors to end up with  120,000 dimensional feature vector in the visual domain. As highlighted in  the kernel approach section, the linear prediction approach solves a quadratic program of N+d_v+1  variables for each unseen class.   Due to the large dimensionality of data  (d_v = 120,000), this is not tractable. To make this setting applicable, we reduced the dimensionality of the feature vector into 4000 using PCA to make it feasible to compute the performance. This highlights the benefit of our approach since our quadratic program does not depend on the dimensionality of the data. Table  <ref> shows MAU for the kernel prediction approaches under this setting against  linear prediction. The results show the benefits of having a kernel prediction for zero shot learning where kernel methods to construct an arbitrary kernel that  improves the performance.\n\n\n", {}]}], "Multiple Representation Experiment and  Distributional Semantic(DS) Kernel": ["\n\nThe aim of this experiment is to show that the kernel approach perform on different representations of text T and visual domains V. In this experiment, we extracted Convolutional Neureal Network(CNN) image features for the Visual domain. We used caffe jia2014caffe implementation of imagenetnips12. Then, we extracted the sixth activation feature of the CNN (FC6) since we found it works the best on the standard classification setting. We found this consistent with the results of donahue2014decaf over different CNN layers. While using  TFIDF feature of text description and CNN features for images, we achieved 2.65% for the linear version and 4.2% for the rbf kernel on both text and images. We further improved the performance to 5.35% by using our proposed Distributional Semantic (DS) kernel in the text domain and rbf kernel for images. In this DS experiment, we used the  distributional semantic model by mikolov2013distributed trained on  GoogleNews corpus (100 billion words)  resulting in a vocabulary of size 3 million words, and word vectors of K=300 dimensions. This experiment shows both the value of having a kernel version and also the value of the proposed kernel in our setting. We also applied the zero shot learning approach in norouzi2014zero which performs worse in our settings; see Table <ref>.\n\n\n\n\n", {"Attributes Experiment": ["\n\nWe emphasis that our  goal from this experiment is not attribute prediction. However, it was interesting for us to see the behavior of our method where T space is defined from attributes instead of text. In contrast to attribute-based models, which fully utilize attribute information to build attribute classifiers, we do not learn attribute classifiers. In this experiment, our method  uses only the first moment of information of the attributes (i.e. the average attribute vector). We decided to compare to an attribute-based approach from this perspective. In particular, we applied the  (DAP) attribute-based model lampertPAMI13,Lampert09, widely adopted in many applications (e.g.,liu2013video,rohrbach11cvpr), to the Birds dataset.\n\n\n\n\nFor this experiment, we compute the average attributes provided with images for each category, without learning any binary classifiers.  We computed this as  T  domain representation for the Birds dataset only, since the Flower dataset does not have attributes. Attribute annotation of Birds dataset is provided in terms of: \"Visibility\" and \"Certainty\". The first term is 1 if the attribute is visible and 0 otherwise. The second term indicates how certain the annotator was about his decision with three levels of \"Certain\", \"Guessing\" and \"probable\". We assigned probability of a visible attribute when the annotator is sure about his decision to 1 and when he is sure of not seeing the attribute to 0. All other combination of decisions fall in the range of (0,1). As this attribute annotation is provided for each images, we averaged these scores across all the samples of a class to get a single attribute descriptor for each class, to be consistent with our learning setting. For visual domain, we used classeme features in this experiment (as table <ref> experiment )(as in section  <ref> ).\n\n\n\n\n\n||TABLE||\n\n    1.0\n||TABLE||\n\n \n\n\nAn interesting result is that our approach achieved 5.6%  MAU (224% the random guess performance); see Table  <ref>. In contrast, we get 4.8% multiclass accuracy using  DAP approach lampertPAMI13. In this setting, we also measured the N_sc to  N_sc+1 average recall. We found the recall measure is 76.7% for our SVM-DT-kernel, while it is 68.1% on  the DAP approach, which reflects better true positive rate (positive class is the unseen one). We find these results interesting, since we achieved it without learning any attribute classifiers, as in lampertPAMI13. When comparing the results  of our approach using attributes (Table <ref>) vs. textual description (Table <ref>)[We are refering to the experiment that uses classeme as visual features to have a consistent comparison to here] as the T space used for prediction, it is clear that the attribute features gives better prediction. This support our hypothesis that the more meaningful the T  domain, the better the performance on V  domain. This indicates that if a better textual representation is used, a better performance can be achieved. Attributes are a good semantic representation of a class but it is difficult to  define attributes for an arbitrary class and further measure the confidence of each one. In contrast, it is much easier to find an unstructured  text description for visual class.\n\n\n\n\n\n\n\n\n\n", {}]}]}], "Conclusion": ["\nWe explored the problem of predicting visual classifiers from textual description of classes with no training images.  We investigated and  experimented with different formulations for the problem within the fine-grained categorization context.  We first proposed  a novel formulation that captures information between the visual and textual domains by involving knowledge transfer from textual features to visual features, which indirectly leads to predicting a linear visual classifier described by the text. In the future, we are planning to propose a kernel version to tackle the problem instead of using linear classifiers. Furthermore,  We also proposed a new zero-shot learning technique to predict kernel-classifiers of unseen categories using information from a privilege space. We formulated the problem as domain transfer function from text description  to the visual classification space, while supporting kernels in both domains. We proposed a one-class SVM adjustment to our domain transfer function in order to improve the prediction. We validated the performance of our model by several experiments. We also showed that   our approach using with weak-attributes. We illustrated the value of proposing a kernelized version by applying kernels generated by Multiple Kernel Learning (MKL) and achieved better results.  We  compared our approach with  state-of-the-art approaches and interesting findings have been reported. In the future, we aim to improve this model by learning the unseen classes jointly and on a larger scale. \nWe are also looking forward to studying more features for the X and E domains in a  large scale setting (number of classes > 1000). \n\n\n\n\nAcknowledgment.  This research was partially funded by NSF award IIS-1218872 and IIS-1409683.\n\n\n\n\n\nIEEEtranegbib,write_a_classifier,elgammal,NLPVision,NLPVisionProposal,smara\n\n\n", {}]}	People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers recognition accuracy, compared to traditional   approaches that depend only on the visual domain. More specifically, the main question of this work is how to utilize purely textual description of visual classes, as privileged information, with noor few training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the linear classifier parameters for new classes. We also propose a generic kernelized  models where a kernel classifier, in the form defined by the representer  theorem, is predicted. The kernelized models allow defining any two RKHS kernel functions in the visual space and text space, respectively, and could be useful for other applications. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our  setting  and could be useful for other applications. We applied all the studied models to predict visual classifiers for two fine-grained categorization datasets, and the results indicate successful predictions of our final model against  several baselines that we designed.
1601.00034.txt	Stochastic Neural Networks with Monotonic Activation Functions	{"Introduction": [" Appearing in Proceedings of the 19th International Conference\non Artificial Intelligence and Statistics (AISTATS)\n2016, Cadiz, Spain. JMLR: W&CP volume 41. Copyright\n2016 by the authors\nDeep neural networks lecun2015deep,bengio2009learning have produced some of the best results in complex pattern recognition tasks where the training data is abundant. \nHere, we are interested in deep learning for generative modeling.\nRecent years has witnessed a surge of interest in directed generative models that are \ntrained using (stochastic) back-propagation [][]kingma2013auto,rezende2014stochastic,goodfellow2014generative.\nThese models are distinct from deep energy-based models -- including deep Boltzmann machine hinton2006fast and (convolutional) deep belief network salakhutdinov2009deep,lee2009convolutional -- that rely on a bipartite graphical model called restricted Boltzmann machine (RBM) in each layer. Although, due to their use of Gaussian noise, the stochastic units that we introduce in this paper can be potentially used with stochastic back-propagation, this paper is limited to applications in RBM.\n\n\nTo this day, the choice of stochastic units in RBM has been constrained to well-known members of the exponential family; in the past RBMs have used units with Bernoulli smolensky1986information, Gaussian freund1994unsupervised,marks2001diffusion, categorical welling2004exponential, Gamma welling2002learning and Poisson gehler2006rate conditional distributions.\nThe exception to this specialization, is the Rectified Linear Unit that was introduced with a (heuristic) sampling procedure nair2010rectified.\n\n\nThis limitation of RBM to well-known exponential family members is despite the fact that welling2004exponential \nintroduced a generalization of RBMs, called Exponential Family Harmoniums (EFH), \ncovering a large subset of exponential family with bipartite structure. \nThe architecture of EFH does not suggest a procedure connecting the EFH to arbitrary non-linearities and more importantly a general sampling procedure is missing.[\nAs the concluding remarks of welling2004exponential suggest, this capability is indeed desirable:\"A future challenge is therefore to start the modelling process with the desired non-linearity and to subsequently introduce auxiliary variables to facilitate inference and learning.\"]\nWe introduce a useful subset of the EFH, which we \ncall exponential family RBMs (Exp-RBMs), \nwith an approximate sampling procedure addressing these shortcomings.\n\nThe basic idea in Exp-RBM is simple: restrict the sufficient statistics to identity function. This allows definition of each unit using only its mean stochastic activation, which is the non-linearity of the neuron.\nWith this restriction, not only we gain interpretability, but also trainability; \nwe show that it is possible to efficiently sample the activation of these stochastic neurons and train the resulting model using contrastive divergence.\nInterestingly, this restriction also closely relates the generative training of Exp-RBM to  \n discriminative training using the matching loss and its regularization by noise injection.\n\nIn the following, sec:model introduces the Exp-RBM family and\nsec:learning investigates learning of Exp-RBMs via an efficient approximate sampling procedure. \nHere, we also establish connections to discriminative training and\nproduce an interpretation of stochastic units in Exp-RBMs as an infinite collection of \nBernoulli units with different activation biases. \nsec:experiments demonstrates the effectiveness of\nthe proposed sampling procedure, when combined with contrastive divergence training, in data representation.\n\n\n", {}], "The Model": ["\n\nThe conventional RBM models the joint probability \n(, |) for visible variables  = [_1,...,_i,...,_I] with \u2208_1 \u00d7...\u00d7_I  and hidden variables  = [_1,...,_j,..._J] with \u2208_1 \u00d7...\u00d7_J as\n(, |) = (-(,) - ()).\n\nThis joint probability is a Boltzmann distribution with a particular energy function : \u00d7\u2192 and a normalization function A.\nThe distinguishing property of RBM compared to other Boltzmann distributions is the conditional independence due to its bipartite structure. \nwelling2004exponential construct Exponential Family Harmoniums (EFH), by first \nconstructing independent distribution over individual variables: considering a hidden variable _j, its sufficient statistics {t_b}_b  and\ncanonical parameters {\u03b7\u0303_j,b}_b, this independent distribution is\n(_j) = (_j) (\u2211_b\u03b7\u0303_j,b  t_b(_j) -({\u03b7\u0303_j,b}_b) )\n\nwhere : _j \u2192 is the base measure and ({\u03b7_i,a}_a)\nis the normalization constant. \nHere, for notational convenience, we are assuming functions with distinct inputs are distinct --  \nt_b(_j) is not necessarily the same function as t_b(_j'), for j' \u2260 j.\n\nThe authors then combine these independent distributions using quadratic terms that reflect the bipartite structure of the EFH to get its joint form\n(, ) \u221d &(\u2211_i,a\u03bd\u0303_i,a t_a(_i) \n\n+ &\u2211_j,b\u03b7\u0303_j,b  t_b(_j) + \u2211_i,a,j,b_i,j^a,b t_a(_i) t_b(_j) ) \nwhere the normalization function is ignored and the base measures are represented as additional sufficient statistics with fixed parameters. In this model, the conditional distributions are \n(_i |) = ( \u2211_a\u03bd_i,a t_a(_j) -({\u03bd_i,a}_a )\n(_j |) = ( \u2211_b\u03b7_j,b t_b(_j) -({\u03b7_j,b}_b )\n\nwhere the shifted parameters \u03b7_j,b = \u03b7\u0303_j,b + \u2211_i,a_i,j^a,b t_a(_i) \nand \u03bd_i,a = \u03bd\u0303_i,a + \u2211_j,b_i,j^a,b t_b(_j)\nincorporate the effect of evidence in network on the random variable of interest.\n\nIt is generally not possible to efficiently sample these conditionals (or the joint probability) for arbitrary sufficient statistics.\nMore importantly, the joint form of eq:EFH_joint and its energy function are \"obscure\". This is in the sense that   \nthe base measures {}, depend on the choice of sufficient statistics and the normalization function A(). In fact for a fixed set of sufficient statistics {t_a(_i)}_i, {t_b(_j)}_j, different compatible choices of normalization constants and base measures may produce diverse subsets of the exponential family. Exp-RBM is one such family, where sufficient statistics are identity functions. \n\n||TABLE||\n\n\n\n\n", {"Bregman Divergences and Exp-RBM": ["\nExp-RBM restricts the sufficient statistics t_a(_i) and t_b(_j) to single\nidentity functions _i, _j for all i and j. This means the RBM has a single weight matrix \u2208^I \u00d7 J. As before, each hidden unit j, receives an input \u03b7_j = \u2211_i W_i,j_i and similarly each visible unit i receives the input \u03bd_i = \u2211_j W_i,j_j.\n[Note that we ignore the \"bias parameters\" \u03bd\u0303_i and \u03b7\u0303_j, since they can be encoded using the weights for additional hidden or visible units (_j = 1, _i = 1)  that are clamped to one.]\n\n\nHere, the conditional distributions (_i |\u03bd_i) and (_j |\u03b7_j) \n have a single mean parameter, (\u03b7) \u2208M, which is equal to the mean of the conditional distribution. We could freely assign any desired continuous and monotonic non-linearity : \u2192M\u2286 to represent the mapping from canonical parameter \u03b7_j to this mean parameter: (\u03b7_j) = \u222b__j_j (_j |\u03b7_j) d_j.\nThis choice of  defines the conditionals\n(_j |\u03b7_j) &= ( -_(\u03b7_j  _j) - (_j) ) \n(_i |\u03bd_i) &= ( -_(\u03bd_i   _i) - (_i) )\nwhere  is the base measure and _ is the Bregman divergence for the function .\n\nThe Bregman divergence bregman1967relaxation,banerjee2005clustering between _j\nand \u03b7_j for a monotonically increasing transfer function (corresponding to the activation function)  is given by[\nThe conventional form of Bregman divergence is\n_(\u03b7_j  _j ) = (\u03b7_j) - ((_j)) - _j (\u03b7_j - (_j)),\nwhere  is the anti-derivative of .\nSince  is strictly convex and differentiable, it has a Legendre-Fenchel dual\n^*(_j) = _\u03b7_j\u3008_j, \u03b7_j\u3009 - (\u03b7_j).\nNow, set the derivative of the r.h.s.   \u03b7_j to zero to get _j = (\u03b7_j), or \u03b7_j = (_j), where ^*(_j) is the anti-derivative of (_i). \nUsing the duality to switch  and  in the above we can get ((_j)) = _j (_j) - ^*(_j). By replacing this in the original form of Bregman divergence we get the alternative form of eq:bregman.]_(\u03b7_j  _j) = - \u03b7_j _j + (\u03b7_j) + ^*(_j)\n\nwhere  with /\u03b7(\u03b7_j) =  (\u03b7_j) is the anti-derivative of \nand ^* is the anti-derivative of . Substituting this expression for Bregmann divergence in\neq:bregman_p, we notice both ^* and  are functions of _j. \nIn fact, these two functions are often not separated [][]mccullagh1989generalized. By separating them\nwe see that some times,  simplifies to a constant, enabling us to approximate eq:bregman_p in sec:sampling.\n\n\n\n\n  Let (\u03b7_j) = \u03b7_j be a linear neuron. Then (\u03b7_j) = 1/2\u03b7_j^2 and \n  ^*(_j) = 1/2_j^2, giving a Gaussian conditional distribution\n  (_j |\u03b7_j) = e^- 1/2( _j - \u03b7_j)^2  + (_j)  , where \n  (_j) = (\u221a(2\u03c0)) is a constant. \n\n\n\n\n\n", {}], "The Joint Form": ["\nSo far we have defined the conditional distribution of our Exp-RBM as members of, \nusing a single mean parameter (\u03b7_j) (or (\u03bd_i) for visible units) that represents the activation function of the neuron. Now we would like to find the corresponding joint form and the energy function.\nThe problem of relating the local conditionals to the joint form in graphical models goes back to the work of besag1974spatial.It is easy to check that, using the more general treatment of yang2012graphical, \nthe joint form corresponding to the conditional of eq:bregman_p is \n\n  &(, |)   =   ( ^T \u00b7\u00b7\n\n &- \u2211_i(^*(_i) + (_i)) \n - \u2211_j(^*(_j) + (_j) ) - ()  ) \nwhere () is the joint normalization constant. It is noteworthy that only the anti-derivative of , ^* appears in the joint form and  is absent.  \nFrom this, the energy function is\n \n   &(, ) = \n  - ^T \u00b7\u00b7\n\n &+ \u2211_i(^*(_i) + (_i)) + \u2211_j(^*(_j) + (_j) ).  For the sigmoid non-linearity (\u03b7_j) = 1/1 + e^-\u03b7_j, \n  we have (\u03b7_j) = (1 + e^\u03b7_j) and ^*(_j) = (1 - _j)(1 - _j) + _j (_j) is the negative entropy.\n  Since _j \u2208{0, 1} only takes extreme values, the negative entropy  ^*(_j) evaluates to zero: \n\n||FORMULA||Separately evaluating this expression for _j = 0 and _j = 1, shows that the above conditional is a well-defined distribution for (_j) = 0, and in \nfact it turns out to be the sigmoid function itself --  \n  (_j = 1 |\u03b7_j) = 1/1 + e^-\u03b7_j. \nWhen all conditionals in the RBM are of the form eq:sigmoid_p --  for a binary RBM with a sigmoid non-linearity, since {(\u03b7_j)}_j and {(\u03bd_i)}_i do not appear in the joint form eq:joint and ^*(0) = ^*(1) = 0, the joint form has the simple and the familiar form \n  \n  (, )   =   ( ^T \u00b7\u00b7  - ()  )\n  .\n      \n||FIGURE||\n\n\n", {}]}], "Learning": ["\nA consistent estimator for the parameters , given observations D = {1,...,N}, is obtained by maximizing the marginal likelihood \u220f_n(n|), where the eq:joint defines the joint probability (, ). \nThe gradient of the log-marginal-likelihood \u2207_ (\u2211_n((n|))  ) is 1/N\u2211_n_(|n, ) [\u00b7 (n)^T]   -  _(, |) [ \u00b7^T] \nwhere the first expectation is  the observed data \nin which (|) = \u220f_j(_j |) and (_j |) is given by eq:bregman_p. The second expectation is  the model of eq:joint.\n\nWhen discriminatively training a neuron (\u2211_i W_i,j_i) using input output pairs D={(n, _jn)}_n, \nin order to have a loss that is convex in the model parameters _:j, it is common\nto use a matching loss for the given transfer function  helmbold1999relative. This is simply the Bregman divergence _((\u03b7_jn)  _jn), \nwhere \u03b7_jn = \u2211_i W_i,j_in.\nMinimizing this matching loss corresponds to maximizing the log-likelihood of eq:bregman_p,\nand it should not be surprising that the gradient \u2207__:j (\u2211_n_((\u03b7_jn)  _jn)  ) of this loss  _:j = [W_1,j,...,W_M,j] \n\u2211_n(\u03b7_jn) (n)^T   - _jn(n)^T\n\nresembles that of eq:grad, where (\u03b7_jn) above substitutes _j in eq:grad.\n\n\nHowever, note that in generative training, _j is not simply equal to (\u03b7_j), but it is sampled from the exponential family distribution eq:bregman_p with the mean (\u03b7_j) -- that is _j = (\u03b7_j) + noise.\nThis extends the previous observations linking the discriminative and generative (or regularized) training -- via Gaussian noise injection -- to the noise from other members of the exponential family [][]an1996effects,vincent2008extracting,bishop1995training which in turn relates to the regularizing role of generative pretraining of neural networks erhan2010does.\n\nOur sampling scheme (next section) further suggests that\nwhen using output Gaussian noise injection for regularization of arbitrary activation functions, the variance of this noise should be scaled by the derivative of the activation function.\n\n\n\n", {"Sampling": ["\nTo learn the generative model, we need to be able to sample from the distributions that define the expectations in eq:grad.\nSampling from the joint model can also be reduced to alternating conditional sampling of visible and hidden variables ( block Gibbs sampling). Many methods, including contrastive divergence [CD;][]hinton2002training, stochastic maximum likelihood [ persistent CD][]tieleman2008training and their variations [][]tieleman2009using,breuleux2011quickly only require this alternating sampling\nin order to optimize an approximation to the gradient of eq:grad. \nHere, we are interested in sampling from  (_j |\u03b7_j) and (_i |\u03bd_i) as defined in eq:bregman_p, which is in general non-trivial. \n However some members of the exponential family have relatively efficient sampling procedures ahrens1974computer. One of these members that we use in our experiments is the Poisson distribution.\n\n  For a Poisson unit, a Poisson distribution \n  (_j |\u03bb) = \u03bb^_j/_j! e^-\u03bb\n  represents the probability of a neuron firing _j times in a unit of time, given its average rate \nis \u03bb. We can define Poisson units within Exp-RBM using _j(\u03b7_j) = e^\u03b7_j, which gives (\u03b7_j) = e^\u03b7_j and \n  ^*(_j) = _j ((_j) - 1). For (_j |\u03b7_j) to be properly normalized, since _j \u2208Z^+ is a non-negative integer,   \n  ^*(_j) + (_j) = (_j!) \u2248^*(_j) (using Sterling's approximation). This gives \n  (_j |\u03b7_j)   =   ( _j\u03b7_j - e^\u03b7_j -  (_j!) )\n  which is identical to distribution of eq:poisson_dist, for \u03bb = e^\u03b7_j. \nThis means, we can use any available sampling routine for Poisson\n  distribution to learn the parameters for an exponential family RBM where some units are Poisson.\nIn sec:experiments, we use a modified version of Knuth's method knuth1969seminumerical for Poisson sampling.\n\n\nBy making a simplifying assumption, the following Laplace approximation demonstrates how to use Gaussian noise to sample from general conditionals in Exp-RBM, for \"any\" smooth and monotonic non-linearity. \n\n  Assuming a constant base measure (_i) = c, the distribution of (_j   \u03b7_j ) is to the second order\napproximated by a Gaussian \n  ( -_(\u03b7_j  _j) - c ) \u2248( _j |(\u03b7_j), '(\u03b7_j) )\n  \n  where '(\u03b7_j) = /\u03b7_j(\u03b7_j) is the derivative of the activation \n  function.\n\n  The mode (and the mean) of the conditional eq:bregman_p\n  for \u03b7_j is (\u03b7_j). This is because the Bregman divergence _(\u03b7_j _j) achieves minimum when _j = (\u03b7_j).\n  Now, write the Taylor series approximation to the target log-probability around its mode\n  \n      & ( ( + (\u03b7_j) |\u03b7_j  )) \n\n& = (-_(\u03b7_j  + (\u03b7_j))) - c\n\n      & = \u03b7_j(\u03b7_j) - ^*((\u03b7_j)) - (\u03b7_j) \n \n      &+  (\u03b7_j - ((\u03b7_j))  + 1/2^2( -1/'(\u03b7_j)) + (^3) \n\n      &= \u03b7_j(\u03b7_j) - (\u03b7_j(\u03b7_j) - (\u03b7_j)) - (\u03b7_j) \n\n      &+ (\u03b7_j - \u03b7_j) + 1/2^2( -1/'(\u03b7_j)) + (^3) \n\n      &= -1/2^2/'(\u03b7_j)+ (^3) \n  In eq:proof1 we used the fact that / y(y) = 1/'((y))\n  and in eq:proof2, we used the conjugate duality of  and ^*.\n  Note that the final unnormalized log-probability in eq:proof3 is that of a Gaussian, with mean zero and variance '(\u03b7_j). Since our Taylor expansion was around (\u03b7_j), this\n  gives us the approximation of eq:gaussian_sampling.\n\n\n\n\n\n", {"Sampling Accuracy": ["\nTo exactly evaluate the accuracy of our sampling scheme, we need to \nevaluate the conditional distribution of eq:bregman_p.\nHowever, we are not aware of any analytical or numeric method to estimate \nthe base measure (_j). Here, we replace (_j) with (\u03b7_j),\nplaying the role of a normalization constant. We then evaluate   \n( _j |\u03b7_j) \u2248( -_(\u03b7_j  _j) - (\u03b7_j) )\n\nwhere \n(\u03b7_j) is numerically approximated for each \u03b7_j value. fig:comparison_sampling compares this density  against the Gaussian approximation  \n( _j |\u03b7_j) \u2248( (\u03b7_j), '(\u03b7_j) ).\nAs the figure shows, the densities are very similar.\n\n\n\n\n", {}]}], "Bernoulli Ensemble Interpretation": ["\nThis section gives an interpretation of Exp-RBM in terms of a Bernoulli RBM with an infinite collection of Bernoulli units. \nnair2010rectified introduce the softplus unit, (\u03b7_j) = (1 + e^\u03b7_j), as an approximation to the rectified linear unit (ReLU) (\u03b7_j) = (0, \u03b7_j). To have a probabilistic interpretation for this non-linearity, the authors represent it as an infinite series of\nBernoulli units with shifted bias:\n(1 + e^\u03b7_j) = \u2211_n = 1^\u221e\u03c3(\u03b7_j - n + .5)\n\nwhere \u03c3(x) = 1/1 + e^-x is the sigmoid function. This means that the sample y_j from a softplus unit is effectively the number of active Bernoulli units. \nThe authors then suggest using _j \u223c(0, (\u03b7_j, \u03c3(\u03b7_j)) to sample from this type of unit. In comparison, our Proposition <ref> suggests using  _j \u223c((1 + e^\u03b7_j), \u03c3(\u03b7_j)) for softplus and _j \u223c((0, \u03b7_j), step(\u03b7_j)) -- where step(\u03b7_j) is the step function -- for ReLU. Both of these are very similar to the approximation of nair2010rectified and we found them to perform similarly in practice as well.\n\nNote that these Gaussian approximations are assuming (\u03b7_j) is constant. \nHowever, by numerically approximating \u222b__j(-_(\u03b7_j _j)) _j, for (\u03b7_j) = (1 + e^\u03b7_j), \nfig:softplus_error shows that the  integrals are not the same for different values of \u03b7_j, showing that the base measure (_j) is not constant for ReLU. In spite of this, experimental results for pretraining ReLU units using Gaussian noise\nsuggests the usefulness of this type of approximation. \n||FIGURE||\n\n\nWe can extend this interpretation as a collection of (weighted) Bernoulli units to any non-linearity . For simplicity, let us assume _\u03b7\u2192 -\u221e(\u03b7) = 0 and _\u03b7\u2192 +\u221e(\u03b7) = \u221e[The following series and the sigmoid function need to be adjusted depending on these limits. For example, for the case where _j is antisymmetric and unbounded ( (\u03b7_j) \u2208{ sinh(\u03b7_j), sinh^-1(\u03b7_j), \u03b7_j  | \u03b7_j|} ), we need to change the domain of Bernoulli units from {0,1} to {-.5,+.5}. This corresponds to changing the sigmoid to hyperbolic tangent 1/2 tanh(1/2\u03b7_j). In this case, we also need to change the bounds for n in the series of eq:series to \u00b1\u221e.], and define the following series of Bernoulli units:\n\n\u2211_n=0^\u221e\u03b1\u03c3((\u03b1 n))\n,\nwhere the given parameter\n\u03b1 \nis the weight of each unit. Here, we are defining a new Bernoulli unit with a weight \u03b1 for each \u03b1 unit of change in the value of . Note that the underlying idea is similar to that of inverse transform sampling devroye1986non.\nAt the limit of \u03b1\u2192 0^+ we have\n(\u03b7_j) \u2248\u03b1\u2211_n=0^\u221e\u03c3(\u03b7_j - (\u03b1 n))\n\nthat is _j \u223c(_j |\u03b7_j) is the weighted sum of active Bernoulli units. \nfig:activations(a) shows the approximation of this series for the softplus function for decreasing values of \u03b1.\n\n||FIGURE||\n\n\n\n", {}]}], "Experiments and Discussion": ["\nWe evaluate the representation capabilities of Exp-RBM for different stochastic units in the following two sections.\nOur initial attempt was to adapt Annealed Importance Sampling [AIS;][]salakhutdinov2008quantitative to Exp-RBMs. \nHowever, estimation of the importance sampling ratio in AIS for general Exp-RBM proved challenging. We consider two alternatives: 1) for large datasets, sec:filters qualitatively evaluates the filters learned by various units and; 2) sec:quant evaluates Exp-RBMs on a smaller\ndataset where we can use indirect sampling likelihood to quantify the generative quality of the models with different activation functions.\n \nOur objective here is to demonstrate that a combination of our sampling scheme with contrastive divergence (CD) training can indeed\nproduce generative models for a diverse choice of activation function. \n\n", {"Learning Filters": ["\n||FIGURE||\n\n\n||FIGURE||\n\nIn this section,\nwe used CD\nwith a single Gibbs sampling step, 1000 hidden units, Gaussian visible units[Using Gaussian visible units also assumes that the input data is normalized to have a standard deviation of 1.], mini-batches and method of momentum, and selected the \nlearning rate from {10^-2, 10^-3, 10^-4} using reconstruction error at the final epoch.\nThe MNIST handwritten digits dataset lecun1998gradient \nis a dataset of 70,000 \"size-normalized and centered\" binary images.\nEach image is 28 \u00d7 28 pixel, and represents one of {0,1,...,9} digits. See the first row of fig:mnist for few instances from MNIST dataset.\nFor this dataset we use a momentum of .9 and train each model for 25 epochs. fig:mnist shows the filters of different stochastic units;\nsee table:units for details on different stochastic units.\nHere, the units are ordered based on the asymptotic behavior of the activation function ; see the right margin of the figure.\nThis asymptotic change in the activation function is also evident from the hidden unit activation histogram of fig:activations(b), where the activation are produced on the test set\nusing the trained model.\n\nThese two figures suggest that transfer functions with faster asymptotic growth, have a more heavy-tailed distributions of\nactivations and longer strokes for the MNIST dataset, also hinting that they may be preferable\nin learning representation [ see][]olshausen1997sparse. However, this comes at the cost of train-ability.\nIn particular, for all exponential units, due to occasionally large gradients, we have to reduce the learning rate to 10^-4 while the Sigmoid/Tanh unit remains stable for a learning rate of 10^-2. Other factors that affect the instability of training for exponential and quadratic Exp-RBMs are large momentum and small number of hidden units. Initialization of the weights could also play an important role, and sparse initialization\n  sutskever2013importance,martens2010deep and regularization schemes goodfellow2013maxout could potentially improve the training of these models. In all experiments, we used uniformly random values in [-.01,.01] for all unit types.\nIn terms of training time, different Exp-RBMs that use the Gaussian noise and/or Sigmoid/Tanh units have similar computation time on both CPU and GPU.\n\nfig:svhn(top) shows the receptive fields for the street-view house numbers (SVHN) netzer2011reading dataset. This dataset contains 600,000 images of digits in natural settings. Each image contains three RGB values for\n32 \u00d7 32 pixels. fig:svhn(bottom) shows few filters obtained from\nthe jittered-cluttered NORB dataset lecun2004learning. NORB dataset contains 291,600 stereo 2 \u00d7 (108 \u00d7 108) images of 50 toys under different lighting, angle and backgrounds. Here, we use a sub-sampled 48 \u00d7 48 variation, and report the features learned by two types of neurons. For learning from these two datasets, we increased the momentum to .95 and trained different models using up to 50 epochs.\n\n\n\n||FIGURE||\n\n\n\n", {}], "Generating Samples": ["\nThe USPS dataset hull1994database is relatively smaller dataset of 9,298, 16\u00d716\n digits. We binarized this data and used 90%, 5% and 5% of instances for training, validation and test respectively; see fig:samples (first two rows) for instances from this dataset. We used Tanh activation function for the 16 \u00d7 16 = 256 visible units of the Exp-RBMs[Tanh unit is similar to the sigmoid/Bernoulli unit, with the difference that it is (anti)symmetric _i \u2208{-.5, +.5}.]\nand 500 hidden units of different types: 1) Tanh unit; 2) ReLU; 3) ReQU and 4)Sinh unit.\n\n\n\n||FIGURE||\n\nWe then trained these models using CD with 10 Gibbs sampling steps. \nOur choice of CD rather than \nalternatives that are known to produce better generative models, such as Persistent CD [PCD;][]tieleman2008training, fast PCD [FPCD;][]tieleman2009using and  \n[rates-FPCD;][]breuleux2011quickly is due to practical reasons; these alternatives were unstable for some activation functions, while CD was always well-behaved. \nWe ran CD for 10,000 epochs with three different learning rates {.05, .01,.001} for each model. Note that here, we did not use method of momentum and mini-batches in order to to minimize the number of hyper-parameters for our quantitative comparison. We used rates-FPCD [ We used 10 Gibbs sampling steps for each sample, zero decay of fast weights -- as suggested in  breuleux2011quickly -- and three different fast rates {.01, .001,.0001}.] to generate  9298 \u00d790/100  samples from each model --  the same number as the samples in the training set. We produce these sampled datasets every 1000 epochs.\nfig:samples shows the samples generated by different models at their final epoch, for the \"best choices\" of sampling parameters and learning rate.\n\n\n||FIGURE||\n\nWe then used these samples _sample = {1,...,N=9298}, from each model to estimate the Indirect Sampling Likelihood [ISL;][]breuleux2011quickly of the validation set. For this, we built a non-parametric density estimate \n(; \u03b2) = \u2211_n=1^N\u220f_j=1^256\u03b2^(_jn = _j) (1 - \u03b2)^(_jn\u2260_j)\nand optimized the parameter \u03b2\u2208 (.5,1) to maximize the likelihood of the validation set -- that is \u03b2^* = _\u03b2 \u220f_\u2208_valid(, \u03b2). Here,  \u03b2 = .5 defines a uniform distribution over all possible binary images, while for \u03b2 = 1, only the training instances have a non-zero probability.\n\nWe then used the density estimate for \u03b2^* as well as the best rates-FPCD sampling parameter to evaluate the ISL of the test set.\nAt this point, we have an estimate of the likelihood of test data for each hidden unit type, for every 1000 iteration of CD updates. \nThe likelihood of the test data using the density estimate produced directly from the training data, gives us an upper-bound on the ISL of these models.\n\nfig:ISL presents all these quantities: for each hidden unit type, we present the results for the learning rate that achieves the highest ISL. \nThe figure shows the estimated log-likelihood  (left) as well as \u03b2^* (right) as a function of the number of epochs. \nAs the number of iterations increases, all models produce samples\nthat are more representative (and closer to the training-set likelihood). This is also consistent with \u03b2^* values getting closer to \u03b2^*_training = .93, the optimal parameter for the training set.\n\nIn general, we found stochastic units defined using ReLU and Sigmoid/Tanh to be the most numerically stable. However, for this problem, ReQU learns the best model and even by increasing the CD steps to 25 and also increasing the epochs by a factor of two we could not produce\nsimilar results using Tanh units.  \nThis shows that a non-linearities outside the circle of well-known and commonly used exponential family, can sometimes produce more powerful generative models, even using an \"approximate\" sampling procedure.\n\n\n\n", {}]}], "Conclusion": ["\nThis paper studies a subset of exponential family Harmoniums (EFH) with a single sufficient statistics for the purpose of learning generative models. \nThe resulting family of distributions, Exp-RBM, gives a freedom of choice for the activation function of individual units, paralleling the freedom in discriminative training of neural networks. \nMoreover, it is possible to efficiently train arbitrary members of this family.\nFor this, we introduced a principled and efficient approximate sampling procedure and demonstrated \nthat various Exp-RBMs can learn useful generative models and filters.\n\nplainnatrefs\n\nKwInputInputKwOutputOutputinitInitializefixFixTraining Exp-RBMs using contrastive divergencetraining data  = {n}_1 \u2264 n \u2264 N ;#CD steps; #epochs; learning rate \u03bb; activation functions {(_i)}_i, {(_j)}_jmodel parameters  #epochspositive phase (+)\n^+\u03b7_jn = \u2211_i_i,j  ^+n_i  \u2200 j,n using Gaussian apprx.\n  ^+n_j \u223c((^+\u03b7n_j), '(^+\u03b7n_j))   \u2200 j,n^+n_j \u223c(_j | ^+n)   \u2200 j,n\n\n^-n\u2190 ^+n  \u2200 n \n\nnegative phase (-)#CD steps\n^-\u03bd_in = \u2211_j_i,j  ^-n_i  \u2200 i,n using Gaussian apprx.\n  ^-n_i \u223c((^-\u03bdn_i), '(^-\u03bdn_i))   \u2200 i,n^-n_j \u223c(_j |n)   \u2200 i,n\n\n\n^-\u03b7_jn = \u2211_i_i,j ^-n_i  \u2200 j,n using Gaussian apprx.\n  ^-n_j \u223c((^-\u03b7n_j), '(^-\u03b7n_j))   \u2200 j,n^+n_j \u223c(_j | ^-n)   \u2200 j,n\n _i,j\u2190_i,j + \u03bb ((^+_i ^+_j) - (^-_i ^-_j)  ) \u2200 i,j \n\n\n\n\n\n\n\n\n\n\n", {}]}	We propose a Laplace approximation that creates a stochastic unit from any smooth monotonic activation function, using only Gaussian noise. This paper investigates the application of this stochastic approximation in training a family of Restricted Boltzmann Machines (RBM) that are closely linked to Bregman divergences. This family, that we call exponential family RBM (Exp-RBM), is a subset of the exponential family Harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron. Using contrastive divergence along with our Gaussian approximation, we show that Exp-RBM can learn useful representations using novel stochastic units.