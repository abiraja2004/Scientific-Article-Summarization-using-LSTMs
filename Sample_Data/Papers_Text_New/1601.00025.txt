e.g. et al. etc. i.e. op-tical net-works semi-conduc-tor



./figures/




e.get aletci.e


@#!T
Write a Classifier:  Predicting Visual Classifiers from Unstructured Text Descriptions
@#^T
Mohamed  Elhoseiny, Member, IEEE,
        Ahmed Elgammal, Senior Member, IEEE,
        and Babak Saleh








@#!A

People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers recognition accuracy, compared to traditional   approaches that depend only on the visual domain. More specifically, the main question of this work is how to utilize purely textual description of visual classes, as privileged information, with noor few training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the linear classifier parameters for new classes. We also propose a generic kernelized  models where a kernel classifier, in the form defined by the representer  theorem, is predicted. The kernelized models allow defining any two RKHS kernel functions in the visual space and text space, respectively, and could be useful for other applications. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our  setting  and could be useful for other applications. We applied all the studied models to predict visual classifiers for two fine-grained categorization datasets, and the results indicate successful predictions of our final model against  several baselines that we designed.




@#^A



Language and Vision, Zero Shot Learning, Unstructured Text. 




@#!S
Introduction
@#^S

One of the main challenges for scaling up object recognition systems is the lack of annotated images for real-world categories. 
Typically there are few images available for training classifiers for most of these categories. This is reflected in the number of images per category available for training in most object categorization datasets, which, as pointed out in Salakhutdinov11, shows a Zipf distribution. 
The problem of lack of training images becomes even more severe when we target recognition problems within a general category, , fine-grained categorization, for example building classifiers for different bird species or flower types  (there are estimated over 10000 living bird species, similar for flowers). 
Researchers try to exploit shared knowledge between categories to target such scalability issue.
This motivated many researchers who looked into approaches that learn visual classifiers from few examples,  deng2010does,fe2003bayesian,BartU05.   This even motivated a more recent work on zero-shot learning of visual categories where there are no training images available for test categories (unseen classes),  Lampert09. Such approaches exploit the similarity (visual or semantic) between seen classes and unseen ones, or describe unseen classes in terms of a learned vocabulary of semantic visual attributes.


  
###FIGURE###
< g r a p h i c s >Our proposed setting where machine can predict unseen class from class-level unstructured  text description
@@@FIGURE@@@


In contrast to the lack of reasonable size training sets for a large number of real world categories, there are abundant of textual descriptions of these categories. This comes in the form of dictionary entries, encyclopedia articles, and various online resources. For example, it is possible to find several good descriptions of a "bobolink" in encyclopedias of birds, while there are only a few images available for that bird online.

 The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. In other words, we aim at zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry; see figure <ref>. We explicitly address the question of how to automatically decide which information to transfer between classes without the need of human intervention.  In contrast to most related work, we go beyond the simple use of tags and image captions, and apply standard Natural Language Processing techniques to typical text to learn visual classifiers.


Fine-grained categorization (also known as subordinate categorization) refers to classification of highly similar objects. This similarity can be due to natural intrinsic characteristics of subordinates of one category of objects (e.g. different breeds of dogs) or artificial subcategories of an object class (different types of airplanes). Diverse applications of fine-grained categories range from classification of natural species wah2011caltech,Flower08,wang2009learning,liu2012dog to retrieval of different types of commercial products maji2013fine. 

###FIGURE###

5.5in
The Bobolink (Dolichonyx oryzivorus) is a small New World blackbird and the only member of genus Dolichonyx.


Description: Adults are 16-18 cm (6-8 in) long with short finch-like bills. They weigh about 1 oz. Adult males are mostly black, although they do display creamy napes, and white scapulars, lower backs and rumps. Adult females are mostly light brown, although their coloring includes black streaks on the back and flanks, and dark stripes on the head; their wings and tails are darker. The collective name for a group of bobolinks is a chain.


Distribution and movement: These birds migrate to Argentina, Bolivia and Paraguay. One bird was tracked flying 12,000 mi over the course of the year, and up to 1,100 mi in one day. They often migrate in flocks, feeding on cultivated grains and rice, which leads to them being considered a pest by farmers in some areas. Although Bobolinks migrate long distances, they have rarely been sighted in Europe-like many vagrants from the Americas, the overwhelming majority of records are from the British Isles. 
Each fall, Bobolinks gather in large numbers in South American rice fields, where they are inclined to eat grain. This has earned them the name "ricebird" in these parts. However, they are called something entirely different in Jamaica (Butterbirds) where they are collected as food, being that they are very fat as they pass through on migration.



Behavior:
Their breeding habitats are open grassy fields, especially hay fields, across North America. In high-quality habitats, males are often polygynous. Females lay 5 to 6 eggs in a cup-shaped nest, which is always situated on the ground and is usually well-hidden in dense vegetation. Both parents feed the young. 
Bobolinks forage on or near the ground, and mainly eat seeds, insects, cultivated grains and rice.
Males sing bright, bubbly songs in flight; these songs gave this species its common name.
1.4in< g r a p h i c s >
< g r a p h i c s > Top: Example Wikipedia article about the Painted Bunting, with an example image. Bottom: The proposed learning setting. For each category we are give one (or more) textual description (only a synopsis of a larger text is shown),  and a set of training images. Our goal is to be able to predict a classifier for a  category based only on the narrative (zero-shot learning). 
@@@FIGURE@@@
In this problem, When we learn from an expert about different species of birds, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarity and differences between species, hierarchal relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about the different species of birds; For example, Fig. <ref> shows an example narrative about the  Bobolink. Typically, the narrative tells you about the bird's taxonomy, highlights discriminative features about that bird and discusses similarities and differences between species, as well as within-species variations (male vs. female). The narrative might eventually show very few example images, which are often selected wisely to illustrate certain visual aspects that might be hard to explain in the narrative.  This learning strategy using textual narrative and images makes the learning effective without a huge number of images  that a typical visual learning algorithm would need to learn the class boundaries.





However, a narrative about a specific species does not contain only "visually" relevant information, but also gives abundant information about the species's habitat, diet, mating habits that is not relevant for visual identification. In a sense, this information might be textual clutter for that task. The same problem takes place in images. While one image can be very effective in highlighting an important feature for learning, many images  might have a lot of visual clutter that makes their uses in learning not effective. Thus, a picture can be worth a thousand words, but not always, and an abundant number of pictures might not be the most effective way for learning. Similarly, one text paragraph can be worth a thousand pictures for learning a concept, but not always, and large amounts of text might not necessarily be effective.







Contributions. The contribution of the paper is on exploring this new problem, which to the best of our knowledge, is firstly explored in the computer vision community in this work. We learn from an image corpus and a textual corpus, however not in the form of image-caption pairs, instead the only alignment between the corpora is at the level of the category. In particular, we address the problem of formulating a visual classifier prediction function Φ (·), which predicts a  classifier of unseen visual class given its text description; see figure <ref>.  While a part of this work was  published in Hoseini13, we extend the work here to study more formulations to solve the problem in Sec. <ref> (B,E). In addition, we also propose a kernel method to explicitly  predict a kernel classifier  in the form defined in the representer theorem rth01.  The kernelized prediction has an advantage that it opens the door for using any kind of side information about classes, as long as kernels can be used on the side information representation.  The side information can be in the form of textual, parse trees, grammar, visual representations, concepts in the ontologies (adopted in NLP domain), or any form. We focus here on unstructured text descriptions.; see figure <ref> The image features also do not need to be in a vectorized format. The kernelized classifiers also facilitates combining different types of features through a multi-kernel learning (MKL) paradigm, where the fusion of different features can be effectively achieved.

We propose and investigate two baseline formulations based on regression and domain adaptation. Then we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to solve the problem.

Beyond the introduction and the related work sections, the paper is structured as follows: Sec <ref> and  <ref> details the problem definition and relation to regression and knowledge transfer models. Sec <ref> shows different formulations of Φ(·) that we studied to predict a linear visual classifier; see figure <ref>. Section <ref> developed extends our notion where  Φ(·) predicts kernel classifier in the form defined by the representer theorem rth01. Sec <ref> presents our proposed distributional semantic kernel  between unstructured text description, which is applicable to  our kernel formulation and could be useful for other applications. Sec <ref> presents our experiments  on Flower Dataset Flower08 (102 classes) and Caltech-UCSD dataset CU20010 (200 classes) for both the linear and the kernel classifier prediction.



@#!S
Related Work
@#^S
NewRelatedWork




@#!S
Problem Definition
@#^S

Fig <ref> illustrates the learning setting. 
The information in our problem comes from two different domains: the visual domain  and the textual domain, denoted by V and T, respectively. Similar to traditional visual learning problems, we are given training data in the form V={(x_i , l_i)}_N, where x_i is an image and l_i ∈{1... N_sc} is its class label. We denote the number of classes available at training as N_sc, where sc indicates "seen classes". As typically done in visual classification setting, we can learn N_sc binary one-vs-all classifiers, one for each of these classes.

Our goal is to be able to predict a classifier for a new category based only on the learned classes and a textual description(s) of that category. In order to achieve that, the learning process has to also include textual description of the seen classes (as shown in Fig  <ref> ). Depending on the domain we might find a few, a couple, or as little as one textual description to each class. We denote the textual training data for class j by {t_i∈T}^j. 
In this paper we assume we are dealing with the extreme case of having only one textual description available per class, which makes the problem even more challenging. For simplicity, the text description of class j is denoted by t_j. However, the formulation we propose in this paper directly applies to the case of multiple textual descriptions per class.



In this paper, we cover predicting  visual classifier Φ(t_*) from an unseen text description t_*  in  linear form or RKHS kernalized form, defined as follows



@#S!S
Linear Classifier
@#S^S

Let us consider a typical binary linear classifier in the feature space in the form

###FORMULA###

    f_j(x) = c_j^T·x
@@@FORMULA@@@
 
where x (bold) is the visual feature vector of an image x (not bold) amended with 1   and c_j ∈R^d_v is the linear classifier parameters for class j. Given a test image, its class is determined by

###FORMULA###

l^* = _j f_j(x)

@@@FORMULA@@@

  
 Similar to the visual domain, the raw textual descriptions have to go through a feature extraction process, which will be described in Sec <ref>. Let us denote the linear extracted textual feature by 
T={t_j ∈R^d_t}_j=1... N_sc, where t_j is the features of text description t_j (not bold).  Given a textual description t_*  of a new unseen category U  with linear feature vector representation t_*, the problem can now be defined as predicting a one-vs-all linear classifier parameters Φ(t_*) = c(t_*) ∈R^d_v,  such that it can be directly used to classify any test image x as 

      c(t_*)^T·x >  0 & if x belongs to U

      c(t_*)^T·x <  0 &  otherwise


@#S!S
Kernel Classifier
@#S^S

For kernel classifiers, we assume that each of the domains is equipped with a kernel function corresponding to a reproducing kernel Hilbert space (RKHS). Let us denote the kernel for V by k(·,·), and the kernel for T by g(·,·). Since, we are studying explicit kernel-classifier prediction from privileged information, we first present an overview on multi-class classification on kernel space.  One

According to the generalized representer theorem rth01,  a minimizer of a regularized empirical risk function over an RKHS could be represented as a linear combination of kernels, evaluated on the training set. Adopting the representer theorem on classification risk function, we define a kernel-classifier of a visual class j as follows



###FORMULA###

f_j(x)=&   ∑_i=1^Nβ_j^i k(x, x_i) + b  = ∑_i=1^Nβ_j^i φ(x_i)^Tφ( x) + b 

f_j(x)=&  β_j^T·k(x) =  c_j^T· [φ(x); 1] , c_j =  [∑_i=1^Nβ_j^i φ(x_i); b]

@@@FORMULA@@@

where x∈V is the test image, x_i is the i^th image in the training data V,  k(x)= [k(x, x_1), ..., k(x, x_N), 1]^T,  β_j = [β_j^1 ...β_j^N, b]^T . Having learned f_j(x^*) for each class j (for example using SVM classifier), the class label of the test image x can be predicted by  Eq. <ref>, similar to the linear case. Eq. <ref> also shows how β_j is related to c_j in the linear classifier, where k(x, x')= φ(x)^T·φ(x') and φ(·) is a feature map  that does not have to be defined given k(·, ·) on V. Hence, our goal in the kernel classifier prediction  is to predict β(t_*) instead of c(t_*) since it is sufficient to define  f_t_*(x) for a text description t_* of an unseen class given k(x)

Under our setting, iIt is clear that f_j(x) could be learned for  all classes with training data j ∈1...N_sc, since there are examples for the seen classes; we denote the kernel-classifier parameters of the seen classes as B_sc =  {β_j }_N_sc, ∀ j. However, it is not obvious how to predict f_t_*(x) for an unseen class given its text description t_*. Similar to the linear classifier prediction, our main notion is to use the text description t_*, associated with unseen class, and the training data to directly predict the unseen kernel-classifier parameters. In other words, the kernel classifier parameters of the unseen class is a function of  its text description t_* , the image training data V and the text training data {t_j}, j∈ 1 ... N_sc; 
###FORMULA###
   f_t_*(x) = β(t_*)^T·k(x), 
@@@FORMULA@@@

  f_t_*(x) could be used to classify new points that	 belong to an  unseen class as follows: 1) one-vs-all setting  f_t_*(x)  ≷  0  if x^*   belongs to unseen category z^*, β(t_*),)^T·k(x^*)< 0  otherwise; or 2) in a Multi-class prediction as in Eq <ref>. Φ(t_*) in this case is β(t_*). In contrast to linear classifier prediction, there is no need to explicitly represent an image x or a text description t by features, which are denoted by the bold symbols in the previous section. Rather, only the  k(·,·)  and g(·,·) needs to be defined which is general.




@#!S
Relation to Regression and Knowledge Transfer Models
@#^S

We introduce two possible frameworks for this problem and discuss potential limitations for them. In this background section,  we focus on predicting linear classifiers for simplicity, which motivates the the evaluated linear classifier formulations that follow in Sec <ref>.


@#S!S
Regression Models
@#S^S

A straightforward way to solve this problem is to pose it as a regression problem where the goal is to use the textual data and the learned classifiers, {(t_j,c_j) }_j=1... N_sc to learn a regression function from the textual feature domain to the visual classifier domain, , a function c(·) : R^d_t→R^d_v .  The question is which regression model would be suitable for this problem? and would posing the problem this way give reasonable results?

A typical regression model, such as ridge regression ridgeReg70 or Gaussian Process (GP) Regression Rasmussen:2005, learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, ,  a set of functions c^i(·) : R^d_t→R . Clearly this will not capture the correlation between the visual classifier dimensions. Instead, a structured prediction regressor would be more suitable since it would learn the correlation between the input and output domain. However, even a structured prediction model, will only learn the correlation between the textual and visual domain through the information available in the input-output pairs  (t_j,c_j).  Here the visual domain information is encapsulated in the pre-learned classifiers and prediction does not have access to the original data in the visual domain. Instead we need to directly learn the correlation between the visual and textual domain and use that for prediction.

Another fundamental problem  that a regressor would face, is the sparsity of the data; the data points are the textual description-classifier pairs, and typically the number of classes can be very small compared to the dimension of the classifier space ( N_sc≪ d_v). In a setting like that, any regression model is bound to suffer from an under fitting problem. This can be best explained in terms of GP regression, where the predictive variance increases in the regions of the input space where there are no data points. This will result in  poor prediction of classifiers at these regions.


@#S!S
Knowledge Transfer Models
@#S^S

An alternative formulation is to pose the problem as domain adaptation from the textual to the visual domain. In the computer vision context, domain adaptation work has focused on transferring categories learned from a source domain,  with a given distribution of images, to a target domain with different distribution, , images or videos from different sources yang07,saenko10,da11,duan12. 
What we need is an approach that learns the correlation between the textual domain features and the visual domain features, and uses that correlation to predict new visual classifier given textual features.

In particular, in da11 an approach for learning cross domain transformation was introduced. In that work a regularized asymmetric transformation between points in two domains were learned. The approach was applied to transfer learned categories between different data distributions, both in the visual domain. A particular attractive characteristic of da11, over other domain adaptation models, is that the source and target domains do not have to share the same feature spaces or the same dimensionality.

While a totally different setting is studied in  da11, it inspired us to formulate the zero-shot learning problem as a domain transfer problem. This can be achieved by learning a linear transfer function W between T and V.  The transformation matrix W can be learned  by optimizing, with a suitable regularizer, over constraints of the form t^TWx≥ l  if t∈T  and x∈V  belong to the same class, and t^TWx≤ u   otherwise. Here l and u are model parameters. This transfer function acts as a compatibility function between the textual features and visual features, which gives high values if they are from the same class and a low value if they are from different classes.

It is not hard to see that this transfer function can act as a classifier. Given a textual feature t^* and a test image, represented by x, a classification decision can be obtained by t_*^TWx≷ b where b is a decision boundary which can be set to (l+u)/2. Hence, our desired predicted classifier in Eq <ref> can be obtained as c(t_*) = t_*^TW  (note that the features vectors are amended with ones).  However, since learning  W was done over seen classes only, it is not clear how the predicted classifier c(t_*) will behave for unseen classes. There is no guarantee that such a classifier will put all the seen data on one side and the new unseen class on the other side of that hyperplane.









@#!S
Formulations for Predicting a linear   classifier form of 
@#^S
Φ (t_*)
The proposed formulations in this section aims at predicting a linear hyperplane parameter c of a one-vs-all classifier for a new unseen class given a textual description, encoded as a feature vector t_* and the knowledge learned at the training phase from seen classes[The notations follow from Subsection <ref>]. We start by  defining the learning components that were used by the formulations described in this section:


  Classifiers:        

a set of linear one-vs-all classifiers {c_j} are learned, one for each seen class.

  Probabilistic Regressor:             

Given {(t_j,c_j)} a regressor is learned that can be used to give a prior estimate for p_reg(c | t) (Details in Sec <ref>).

  Domain Transfer:            

Given T and V a domain transfer function, encoded in the matrix W is learned, which captures the correlation between the textual and visual domains (Details in Sec <ref>).
 
  
  
  
Each of the following subsections show a different approach to predict a linear classifier from t_* as Φ(t_*) = c(t_*); see Sec <ref>. The final approach (E) combines  regression, domain transfer, and additional constraints, which achieves the best performance. We compare between these alternative formulations in our experiments. Hyper-parameter selection is detailed in the supplementary materials for all the approaches. 
###FIGURE###
< g r a p h i c s >Illustration of the Proposed Linear Prediction Framework (Constrained Regression and Domain Transfer) for the task Zero-shot learning from textual description (Linear Formulation (E))
@@@FIGURE@@@



@#S!S
Probabilistic Regressor
@#S^S

There are different regressors that can be used, however we need a regressor that provide a probabilistic estimate p_reg(c | (t)). For the reasons explained in Sec <ref>, we also need a structure prediction approach that is able to predict all the dimensions of the classifiers together. For these reasons, we use the Twin Gaussian Process (TGP) Bo:2010. 
TGP encodes the relations between both the inputs and structured outputs using Gaussian Process priors. This is achieved by minimizing the Kullback-Leibler divergence between the marginal GP of the outputs (i.e. classifiers in our case) and observations (i.e. textual features). The estimated regressor output (c̃(t_*)) in TGP is given by the solution of the following non-linear optimization problem Bo:2010[notice we are using c̃ to denote the output of the regressor, while using ĉ to denote the output of the final optimization problem in Eq <ref>].

###FORMULA###
Φ (t_*) = c̃(t_*) = & cargmin[  K_C(c,c)  -2 k_c(c)^Tu - η ( 
 & K_C(c,c -k_c(c)^T (K_C+ λ_c I)^-1 k_c(c) ) ]

@@@FORMULA@@@

where u = (K_T + λ_t  I)^-1 k_t(t_*), η  = K_T(t_*,t_*) -k(t_*)^Tu ,  K_T(t_l,t_m)  and K_C(c_l,c_m) are Gaussian kernel for input feature t and output vector c.  k_c(c) = [K_C(c,c_1), ..., K_C( c,c_N_sc)]^T. k_t(t_*) = [K_T(t_*,t_1), ..., K_T(t_*,t_N_sc)]^T.  λ_t and λ_c are regularization parameters to avoid overfitting. This optimization problem can be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection Bo:2010. In this case the classifier dimension are predicted jointly. In this case p_reg(c|t_*) is defined as a normal distribution.

###FORMULA###

p_reg(c|t_*) =  N (μ_c = c̃(t_*),Σ_c = I)

@@@FORMULA@@@

The reason that Σ_c = I is that TGP does not provide predictive variance, unlike Gaussian Process Regression. However, it has the advantage of handling the dependency between the dimensions of the classifiers c given the textual features t. 
 

@#S!S
Constrained Probabilistic Regressor
@#S^S

We also investigated formulations that use regression to predict an initial hyperplane  c̃(t_*) as described in section  <ref>, which is then optimized to put all seen data in one side, 
###FORMULA###
Φ (t_*) = ĉ(t_*) =   c,ζ_iargmin [  c^Tc  + α ψ( c,c̃(t_*)) + C   ∑_i=1^Nζ_i ] 
  s.t.:  -c^Tx_i≥ζ_i ,  ζ_i ≥ 0 , i=1,...,N 

@@@FORMULA@@@


where ψ(·,·) is a similarity function between hyperplanes,  a dot product used in this work,  α is its constant weight, and C is the weight to the soft constraints of existing images as negative examples (inspired by linear SVM formulation), or other functions incorporating the predictive variance. We call this class of methods  constrained GPR/TGP, since c̃(t_*) is initially predicted through GPR or TGP.



@#S!S
Domain Transfer (DT)
@#S^S

To learn the domain transfer function W we adapted the approach in da11 as follows. Let T be the textual feature data matrix and X be the visual feature data matrix where each feature vector is amended with a 1. Notice that amending the feature vectors with a 1 is essential in our formulation since we need t^TW to act as a classifier. We need to solve the following optimization problem 

###FORMULA###
_W  r(W) + λ∑_i c_i(TWX^T)
  
@@@FORMULA@@@
where c_i's are loss functions over the constraints and r(·) is a matrix regularizer.  It was shown in da11, under condition on the regularizer, that the optimal W  is in the form of 
  W^* = TK_T^-1/2L^*  K_X^-1/2X^T, where  K_T  = TT^T,  K_X  = XX^T.   L^* is computed by minimizing the following minimization problem

###FORMULA###
Lmin       [   r(L)+ λ∑_p c_p(K_T^1/2LK_X^1/2  ) ],

@@@FORMULA@@@

where c_p(K_T^1/2LK_X^1/2  ) = (max(0, (l-e_i K_T^1/2LK_X^1/2 e_j) ))^2 for same class pairs of index i,j, or  =(max(0, (e_i K_T^1/2LK_X^1/2 e_j -u) ))^ 2 otherwise, where e_k is a one-hot vector of zeros except a one at the k^th element, and u>l (note any appropriate l, u could work. In our case, we used l =2, u=-2 ). We used a Frobenius norm regularizer.  This energy is minimized using a second order BFGS quasi-Newton optimizer. Once L is computed W^* is computed using the transformation above. Finally Φ (t_*)  = c(t_*) = t_*^TW, simplifying  W^* as W.





@#S!S
Constrained-DT
@#S^S

We also investigated constrained-DT formulations that learns a transfer matrix W and  enforce t_j^TW to be close to the classifiers learned on seen data, {c_j } ,
###FORMULA###
_W  r(W) + λ_1 ∑_i c_i(TWX^T) +λ_2 ∑_k(c_j - t^T_j W)^T (c_j - t^T_j W)
@@@FORMULA@@@

A classifier can be then obtained by Φ (t_*)  =c(t_*)= t_*^TW.



###FORMULA###
Φ (t_*) = ĉ(t_*) =   c,ζ_iargmin [    c^Tc - αt_*^TWc  - γ p_reg(c|t_*)
  
 +γ∑_i=1^Nζ_i ]  ; s.t.:  -c^Tx_i≥ζ_i ,  ζ_i ≥ 0 ,  t_*^TWc≥ l 
 
@@@FORMULA@@@

The first term is a regularizer over the classifier c.  The second term enforces that the predicted classifier has high correlation with t_*^TW. The third term favors a classifier that aligns with the prediction of the regressor c̃(t_*). The constraints c^Tx_i≥ζ_i   enforce that all  seen data instances are at the negative side of the predicted hyperplane with some missclassification allowed through the  slack variables  ζ_i. The constraint  t_*^TWc≥ l enforces that the correlation between the predicted classifier and t_*^TW is no less than l,  a minimum correlation between the text and visual features. 
Given W, and the form of the probability estimate  p_reg(c|t_*), the optimization reduces to a quadratic program on c with linear constraints.



@#S!S
Constrained  Regression and Domain Transfer for classifier prediction
@#S^S

 Fig <ref> illustrates our final  framework which combines regression (formulation A (using TGP)) and domain transfer (formulation C) with additional constraints. This formulation combines the three learning components described in the beginning of this section. 
Each of these components contains partial knowledge about the problem. The question is how to combine such knowledge to predict a new classifier given a textual description. The new classifier has to be consistent with the seen classes. 
The new classifier has to put all the seen instances at one side of the hyperplane, and has to be consistent with the learned domain transfer function. This leads to the following constrained  optimization problem 

###FORMULA###
Φ (t_*) = ĉ(t_*) =  &   c,ζ_iargmin [    c^Tc - αt_*^TWc  - γ( p_reg(c|t_*))  

& + C   ∑ζ_i]

&s.t.:  -(c^Tx_i ) ≥ζ_i ,    ζ_i ≥ 0 ,   i = 1 ... N 

&	     t_*^TWc≥ l     

& α , γ, C, l  : hyperparameters

@@@FORMULA@@@

The first term is a regularizer over the classifier c.  The second term enforces that the predicted classifier has high correlation with t_*^TW; W is learnt by Eq <ref>. The third term favors a classifier that has high probability given the prediction of the regressor. The constraints  -c^Tx_i≥ζ_i   enforce all the seen data instances to be at the negative side of the predicted classifier hyperplane with some missclassification allowed through the  slack variables  ζ_i. The constraint  t_*^TWc≥ l enforces that the correlation between the predicted classifier and t_*^TW is no less than l, this is to enforce a minimum correlation between the text and visual features.


Solving for ĉ as a quadratic program: 
According to  the definition of p_reg(c|t_*) for  TGP,   p(c|t_*) is a quadratic term in c in the form  

###FORMULA###

- p(c|t_*) ∝ ( c - c̃(t_*))^T (c - c̃(t_*)) 
= c^Tc -2 c^Tc̃(t_*) +  c̃(t_*)^Tc̃(t_*) 

@@@FORMULA@@@

We reduce - p(c|t_*) to -2 c^Tc̃(t_*)), since 1) c̃(t_*)^Tc̃(t_*) is a constant ( does not affect the optimization), 2) c^Tc is already included as regularizer in equation  <ref>.  In our setting, the dot product  is a better similarity measure between two hyperplanes. Hence,  -2 c^Tc̃(t_*) is minimized.
Given - p(c|t_*) from the TGP and  W, Eq <ref> reduces to a quadratic program on c with linear constraints. We tried different quadratic solvers, however the IBM CPLEX solver [http://www-01.ibm.com/software/integration/optimization/cplex-optimizer] gives the best performance in speed and optimization for our problem.









@#!S
Formulations for Predicting a kernel   classifier form of 
@#^S
Φ (t_*) 
Prediction of Φ (t_*)  = β(t_*) (Sec. <ref>), is decomposed into training (domain transfer) and prediction phases, detailed as follows

@#S!S
Kernelized Domain Transfer
@#S^S

During training, we firstly learn B_sc = {β_j }, j=1→ N_sc as SVM-kernel classifiers based on  the training data and defined by k(·, ·) visual kernel, see Sec <ref>. Then, we learn a kernel domain transfer function to transfer the text description information t_*∈T to kernel-classifier parameters β∈R^N+1 in V domain. We call this domain transfer function β_DA(t_*), which has the form as Ψ^Tg(t_*), where g(t_*)  = [g(t_*, t_1) ... g(t_*, t_N_sc)]^T, g(t, t')  is a kernel function that measures the similarity between t and t' on  domain E; Ψ is an N_sc×N+1 matrix, which transforms t to  kernel classifier parameters for the class that t_* represents.



Inspired by the domain transfer proposed by da11, wWe aim to learn  Ψ from V and {t_j}, j=1 ... N_sc, such that g(t)^TΨk(x) > l if t and  x  correspond to the same class, g(t)^TΨk(x) < u  otherwise. Here l  controls similarity lower-bound if t and x correspond to  same class, and u controls similarity upper-bound if t and x belong to different classes. In our setting, the term   Ψ^Tg(t_j) should act as a classifier parameter for class j in the training data. Therefore,  we introduce  penalization constraints to our minimization function  if  Ψ^T g(t_j) is distant from β_j ∈B_sc, where t_i corresponds to the class that β_i classifies.Hence,  in order to learn T , we solve the following objective function  Inspired by domain adaptation [A totally different problem/setting but the optimization methods inspired our solution] optimization methods (da11),  we model our solution using the following objective functionInspired by domain adaptation optimization methods (da11) [A totally different problem/setting but the optimization methods inspired our solution],  in order to learn T,, we model the kernel domain transfer function as follows by the following objective function
###FORMULA###
Ψ^*= 
 _Ψ  L(Ψ) = [&1/2 r(Ψ) + λ_1 ∑_k c_k(G Ψ K) + 
 & λ_2 ∑_i=1^N_scβ_i - Ψ^T g(t_i)^2
@@@FORMULA@@@

 where, 
G   is an N_sc× N_sc  symmetric matrix, such that both the i^th    row and the i^th  column are equal to g(t_i), i=1: N_sc; K    is an N+1 × N  matrix, such that the i^th  column is equal to k(x_i), x_i, i=1:N.
c_k's are loss functions over the constraints defined as
  c_k(G Ψ K)) = (max(0, (l-1_i^TG Ψ K1_j) ))^2  for same class pairs of index i  and j,  or  =r·(max(0, (1_i^TG Ψ K1_j -u) ))^ 2  otherwise, where 1_i  is an N_sc× 1  vector with all zeros except at index i, 1_j  is an N × 1  vector with all zeros except at index j. This leads to that    c_k(G Ψ K)) = (max(0, (l-g(t_i)^T Ψ k(x_j) ))^2  for same class pairs of index i  and j, or  =r·(max(0, (g(t_i)^T Ψ k(x_j) -u) ))^ 2 otherwise, where u>l (note any appropriate l, u could work in our case we used l =2, u=-2 ), r = nd/ns  such that nd  and ns  are the number of pairs (i,j)  of different classes and similar pairs respectively. r(·)  is a matrix regularizer; Finally, we used a Frobenius norm regularizer for r(Ψ).


The objective function in Eq  <ref>  controls the involvement of the constraints c_k  by the term multiplied by λ_1, which controls its importance; we call it C_l,u(Ψ). While, the trained classifiers penalty is captured by the term multiplied by λ_2; we call it C_β(Ψ). One important observation on  C_β(Ψ), is that it reaches zero when Ψ = G^-1B^T, where B  = [β_1 ...β_N_sc], since it could be rewritten as C_β(Ψ) = B^T - G Ψ_F^2. Our intuition is that for the model to have good generalization, the effect of C_β(T)  should be  minimal (λ_2 → 0), since this case indicates successful modeling of the transfer from E  domain to the kernel-classifier parameters in X  domain. 
In contrast to the linear-classifier restricted approach proposed by Elhoseiny et al    Hoseini13, our domain transfer model can transfer any type of classifier of an arbitrary kernel from T to V. Furthermore, the classifier penalty term was not studied in Hoseini13, which is captured here by C_β(T).

We minimize L(Ψ)  is by gradient-based optimization using a second order BFGS quasi-Newton optimizer. Our  gradient derivation of L(Ψ)  leads to the following form

###FORMULA###
δ L(Ψ)/δ Ψ =  Ψ + λ_1 ·∑_i,jg(t_i)k(x_j)^T v_ij +
r ·λ_2 · ( G^2  Ψ - GB) 

@@@FORMULA@@@

where v_ij = - 2 · max(0, (l-g(t_i)^T Ψ k(x_j) )  if i  and j  correspond to the same class, 2 · max(0, (g(t_i)^T Ψ k(x_j) -u )  otherwise. Another approach that can be used to minimize L(Ψ)  is through alternating projection using Bregman algorithm bregman97, where Ψ  is updated by a single constraint every iteration.




@#S!S
Kernel Classifier Prediction
@#S^S

We study two ways to infer the final kernel-classifier prediction. (1) Direct Kernel Domain Transfer Prediction, denoted by "DT-kernel", (2) One-class SVM adjusted DT Prediction, denoted by "SVM-DT kernel". Hyper-parameter selection is attached in the supplementary materials. The source code is available here 
<https://sites.google.com/site/mhelhoseiny/computer-vision-projects/write_kernel_classifier>.

Direct Domain Transfer (DT) Prediction: By construction a classifier of an unseen class can be directly computed from our trained domain transfer model as follows

###FORMULA###
Φ(t_*) = β̃_DT(t_*) = Ψ^*^T g(t_*)

@@@FORMULA@@@
One-class-SVM adjusted DT (SVM-DT) Prediction: 
In order to increase separability against seen classes, we adopted the inverse of the idea of the one class kernel-svm, whose main idea is to build a confidence function that takes only positive examples of the  class. Our setting is the opposite scenario; seen examples are negative examples of the unseen class.
In order introduce our proposed adjustment method, we  start by presenting the one-class SVM objective function. The  Lagrangian dual  of the one-class SVM oneclasssvm07 can be written as

###FORMULA###
β^*_+ =  &   βargmin [    β^TK^' β - β^T a]

&s.t.: β^T 1 = 1,  0 <β_i < C; i = 1 ... N   

@@@FORMULA@@@

where K^'   is an N × N  matrix, K^' (i,j) = k(x_i, x_j), ∀x_i,x_j ∈S_x ( in the training data), a  is an N × 1  vector, a_i = k(x_i, x_i), C  is a hyper-parameter . It is straightforward to see that, if β is aimed to be a negative decision function instead, the objective function becomes in the form

###FORMULA###
β^*_- =  &   βargmin [    β^TK^' β + β^T a]

&s.t.: β^T 1 = -1, -C <β_i < 0; i = 1 ... N 

@@@FORMULA@@@

While β^*_-  = - β^*_+, the objective function in Eq <ref> of the one-negative class SVM inspires us with the idea to adjust the kernel-classifier parameters to increase separability of the unseen kernel-classifier against the points of the seen classes, which leads to the following objective function 

###FORMULA###
Φ(t_*) = β̂(t_*) =  &   βargmin [    β^TK^' β - ζβ̂_DT(t_*)^TK^'β   + β^T a]

&s.t.:   β^T 1 = -1, β̂_DT^TK^' β> l, -C <β_i < 0;∀ i 

& C, ζ , l  : hyper-parameters,

@@@FORMULA@@@

where  β̂_DT  is the first N  elements in β̃_DT(t^*) ∈R^N+1, 1  is an N × 1  vector of ones. The objective function, in Eq <ref>,  pushes the classifier of the unseen class to be highly correlated with the domain transfer prediction of the kernel classifier, while putting  the points of the seen classes as negative examples. It is not hard to see that Eq <ref> is a quadratic program in β, which could be solved using any quadratic solver.In contrast to our formulation, the approaches presented in NIPS13DeViSE,NIPS13CMT,Hoseini13 assumes that X∈ R^d_b and  E∈ R^d_E  (  vectorized). It is worth to mention that,  linear classifier prediction in Eq <ref> (best Linear formulation in our results)  predicts  classifiers by solving an optimization problem of size  N+d_v+1   variables, d_v+1  linear-classifier parameters, which is the same as the length of the visual feature vector, and N  slack variables; a similar limitation can be found in NIPS13DeViSE,NIPS13CMT where the architecture depends on the number on visual features.  In contrast, the kernelized objective function (Eq <ref>) solves a  quadratic program of only N  variables, and  predicts a kernel-classifier instead with fewer parameters. Using very high-dimensional features  will not affect the optimization complexity.  
Therefore, it is clear that the kernel formulation is expected to have better generalization properties. In addition, the kernel-approach does not assume that any of V  and  T  is a vector space.





 

@#!S
Distributional Semantic (DS) Kernel for text  descriptions
@#^S

We propose a distributional semantic kernel g(·, ·) = g_DS(·, ·)  to define the similarity between two text descriptions in T domainof visual classes in our setting. While this kernel is applicable to  kernel classifier predictors  presented in Sec <ref>, it could be used for other applications. We start by  distributional semantic models by mikolov2013distributed,mikolov2013efficient to represent the semantic manifold M_s, and a function vec(·) that maps a word to a K× 1 vector in M_s. The main assumption behind this class of distributional semantic model  is that similar words share similar context. Mathematically speaking, these models  learn a vector for each word w_n, such  that p(w_n|(w_n-L, w_n-L+1, ...,  w_n+L-1,w_n+L) is maximized over the training corpus, where 2× L is the context window size. Hence similarity between vec(w_i) and vec(w_j) is high if they co-occurred a lot in context of size 2× L in the training text-corpus. We normalize all the word vectors to length 1 under L2 norm, i.e.,  vec(·) ^2=1.

Let us assume a  text description D that we represent by a set of triplets D = {(w_l,f_l, vec(w_l)), l=1... M}, where w_l is a word that occurs in D with frequency f_l and its corresponding word vector is vec(w_l) in M_s. We drop the stop words from D. We define  F = [f_1, ..., f_M]^T and P = [vec(w_1), ..., vec(w_M)]^T, where F is an M×1  vector of term frequencies and P is an M × K matrix of the corresponding term vectors.

Given two text descriptions D_i and D_j which contains M_i and M_j terms respectively. We compute P_i (M_i × 1) and V_i (M_i × K) for  D_i  and P_j (M_j × 1) and V_j (M_j × K) for  D_j. Finally  g_DS(D_i, D_j) is defined as 

###FORMULA###

g_DS(D_i, D_j) = F_i^TP_i P_j^TF_j

@@@FORMULA@@@

One advantage of this similarity measure is that it captures semantically related terms. It is not hard to see that the standard Term Frequency (TF) similarity could be thought as a special case of this kernel where vec(w_l)^T vec(w_m)=1 if w_l=w_m, 0 otherwise, i.e., different terms are orthogonal. However, in our case the word vectors are learnt through a distributional semantic model which makes semantically related terms have higher dot product (vec(w_l)^T vec(w_m)).






@#!S
Experiments
@#^S


@#S!S
Datasets and Features
@#S^S
 Datasets:
We used  the CU200 Birds CU20010 (200 classes - 6033 images) and the Oxford Flower-102 Flower08 (102 classes -  8189 images) image dataset to test our methods, since they are among the largest and widely used fine-grained datasets.  We created  textual descriptions for each class in both datasets. The CUB200 Birds image dataset was created based on birds that have a corresponding Wikipedia article, so we have developed a tool to automatically extract Wikipedia articles given the class name. The tool succeeded to automatically generate 178 articles, and the remaining 22 articles was extracted manually from Wikipedia. These mismatches happens only when article title is a different synonym of the same bird class. On the other hand, Flower image dataset was not created using the same criteria as the Bird dataset, so classes of the Flower dataset classes does not necessarily have corresponding Wikipedia article. The tool managed to generate only 16 classes from Wikipedia out of 102, the remaining 86 articles was generated manually for each class from Wikipedia, Plant Database  [http://plants.usda.gov/java/], Plant Encyclopedia [http://www.theplantencyclopedia.org/wiki/Main_Page], and BBC articles [http://www.bbc.co.uk/science/0/]. The collected textual description for FLower and Birds dataset are available here  <https://sites.google.com/site/mhelhoseiny/1202-Elhoseiny-sup.zip> .

 Textual Feature Extraction:
The textual features were extracted in two phases, which are typical in document retrieval literature. The first phase is an indexing phase that generates textual features with tf-idf (Term Frequency-Inverse Document Frequency) configuration (Term frequency as local weighting while inverse document frequency as a global weighting). The tf-idf is a measure of how important is a word to a text corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to control for the fact that some words are generally more common than others. We used the normalized frequency of a term in the given textual description salton1988term. The inverse document frequency is a measure of whether the term is common; in this work we used the standard logarithmic idf salton1988term.   
The second phase is a dimensionality reduction step, in which  Clustered Latent Semantic Indexing (CLSI) algorithm clsi05 is used. CLSI is a low-rank approximation approach for dimensionality reduction, used for document retrieval. In the Flower Dataset, tf-idf features ∈R^8875 and after CLSI the final textual features ∈R^102. In the Birds Dataset, tf-idf features is in R^7086 and after CLSI the final textual features is in R^200.


###TABLE###
(0.8)
###TABLE###

& 0.22tf-idf Features dimensions 
  & 0.17CLSI Cluster count  & 0.21Reduced Dimensions 
0.17Birds Dataset &7086 & 200 & 200 
0.17Flower Dataset &8875 & 102 & 102 

@@@TABLE@@@
Textual features specifications
@@@TABLE@@@




  Visual features Extraction:
We used the Classeme features classemes as the visual feature for our experiments since they provide an intermediate semantic representation of the input image. Classeme features are output of a set of classifiers corresponding to a set of C category labels, which are drawn from an appropriate term list defined in classemes, and not related to our textual features. For each category c ∈ 1 ... C  , a set of training images is gathered by issuing a query on the category label to an image search engine.
After a set of coarse feature descriptors (Pyramid HOG, GIST, ) is extracted, a subset of feature dimensions was selected classemes, and  a one-versus-all classifier φ_c is trained for each category. The classifier output is real-valued, and is such that φ_c(x) > φ_c(y) implies that x is more similar to class c than y is. Given an image x,  the feature vector (descriptor) used to represent it is the classeme vector   [φ_1 (x),  ..., φ_d_v (x)], d_v=2569.

For Kernel classifier prediction, we evaluated  these features and also additional representations  for text descriptions (by the proposed distributional semantic kernel using word embedding) and  images ( (a) CNN features and (b)  combined kernel over different features learnt by MKL (multiple kernel learning)), discussed later in Subsection <ref>.


###FIGURE###
< g r a p h i c s >< g r a p h i c s >< g r a p h i c s >Linear : Left and Middle: ROC curves of best 10 predicted classes by the final formulation (E) for Bird  and Flower datasets respectively, Right: AUC improvement over the three baselines on Flower dataset (Formulations A (GPR), A (TGP), C). The improvement is sorted in an increasing order for each baseline separately (best seen in color)
@@@FIGURE@@@




###TABLE###
 Linear: Comparative Evaluation of Different Formulations on the Flower and Bird Datasets1.0
###TABLE###

  		&  Oxford Flowers   & UC-UCSD Birds   

  Approach  & Avg AUC (+/- std) & Avg AUC (+/- std)

  (A) Regression - GPR & 0.54 (+/- 0.02) & 0.52 (+/- 0.001) 
 
  (A) Structured Regression - TGP & 0.58 (+/- 0.02) & 0.61 (+/- 0.02)

  (C)  Domain Transfer(DT) &  0.62(+/- 0.03)  &  0.59 (+/- 0.01)
 
  (B) Constrained GPR & 0.62(+/- 0.005) & - 

  (B) Constrained TGP & 0.63(+/- 0.007) & - 

  (D) Constrained Domain Adaptation (CDT) on Eq <ref> & 0.64 (+/- 0.006)& -  

  (E) Regression+DT + constraints (final best linear approach) & 0.68 (+/- 0.01) &  0.62 (+/- 0.02) 

@@@TABLE@@@
0.7
###TABLE###
5cTop-5 Classes with highest combined improvement

  class      &  (A) TGP (AUC) & (C) DT (AUC) & (D) TGP+DT+C  & % Improv. 

   2   &  0.51 & 0.55 & 0.83 & 57%
  
   28 & 0.52 & 0.54 & 0.76 &  43.5%

   26 &  0.54 & 0.53 & 0.76 & 41.7%

   81 & 0.52 & 0.82 & 0.87   & 37%

   37 & 0.72 & 0.53 & 0.83   & 35.7 %

@@@TABLE@@@

@@@TABLE@@@



@#S!S
Experimental Results for Linear Classifier Prediction
@#S^S



 Evaluation Methodology: Similar to zero-shot learning literature, we evaluated the performance of an unseen classifier in a one-vs-all setting where the test images of unseen classes are considered to be the positives and the test images from the seen classes are considered to be the negatives. We computed the ROC curve and report the area under that curve (AUC) as a comparative measure of different approaches. In zero-shot learning setting the test data from the seen class are typically very large compared to those from unseen classes. This makes other measures, such as accuracy, useless since high accuracy can be obtained even if all the unseen class test data are wrongly classified; hence we used ROC curves, which are independent of this problem. Five-fold cross validation over the classes were performed, where in each fold 4/5 of the classes are considered as "seen classes" and are used for training and 1/5th of the classes were considered as "unseen classes" where their classifiers are predicted and tested. Within each of these class-folds, the data of the seen classes are further split into training and test sets. The hyper-parameters for the  approach were selected through another five-fold cross validation within the class-folds  (i.e. the 80% training classes are further split into 5 folds to select the hyper-parameters). We made the seen-unseen folds used in our experiments available here <https://sites.google.com/site/mhelhoseiny/computer-vision-projects/Write_a_Classifier>.

 Baselines:
Since our work is the first to predict classifiers based on pure textual description, there are no other reported results to compare against. However, we designed three state-of-the-art baselines to compare against, which are designed to be inline with our argument in Sec <ref>. Namely we used: 1) A Gaussian Process Regressor (GPR) Rasmussen:2005, 2) Twin Gaussian Process (TGP) Bo:2010 as a structured regression method, 3)  Domain Transfer (DT) da11. The TGP and DT baselines are of particular importance since our formulation utilizes them, so we need to test if the formulation is making any improvement over them. It has to be noted that we also evaluate TGP and DT as alternative formulations that we are proposing for the problem, none of them was used in the same context before. 
###TABLE###
 Comparative Evaluation on the Flowers and Birds1.0
###TABLE###

  		&  Flowers   & Birds   

  Approach  & Avg AUC (+/- std) & Avg AUC (+/- std)

  GPR  & 0.54 (+/- 0.02) & 0.52 (+/- 0.001) 
 
  TGP  & 0.58 (+/- 0.02) & 0.61 (+/- 0.02)

  DT  &  0.62(+/- 0.03)  &  0.59 (+/- 0.01)

  Our Approach &  0.68 (+/- 0.01) &  0.62 (+/- 0.02) 

@@@TABLE@@@

@@@TABLE@@@






 Results:
Table <ref> shows the average AUCs for the final linear approach in comparison to the three baselines on both datasets. GPR performed poorly in all classes in both data sets, which was expected since it is not a structure prediction approach.  The DT formulation outperformed TGP in the flower dataset but slightly underperformed on the Bird dataset. The proposed approach outperformed all the baselines on both datasets, with significant difference on the flower dataset. It is also clear that the TGP performance was improved on the Bird dataset since it has more classes (more points are used for prediction). Fig  <ref> shows the ROC curves for our approach on best predicted unseen classes from the Birds dataset on the Left  and Flower dataset on the middle. Fig  <ref> shows the AUC for all the classes on Flower dataset. 
###FIGURE###
< g r a p h i c s >ROC curves for best predicted classes -- Flower
@@@FIGURE@@@

###FIGURE###
< g r a p h i c s >ROC curves for best predicted classes -- Birds
@@@FIGURE@@@

###FIGURE###
< g r a p h i c s >
	(a)
< g r a p h i c s >
 (b)

@@@FIGURE@@@

###FIGURE###
0.45< g r a p h i c s >A gull0.45< g r a p h i c s >A tigerPictures of animals
@@@FIGURE@@@




###TABLE###
 Linear: Percentage of classes that the final proposed approach (formulation (E)) makes an improvement in predicting over the baselines (relative to the total number of classes in each dataset1.0
###TABLE###

  		&  Flowers (102)  & Birds (200)
 
 baseline      &  %  improvement & %  improvement

  (A) GPR  & 100 % & 98.31 %
 
  (A) TGP  & 66 % & 51.81 %

  (C) DT  &   54% &  56.5%

@@@TABLE@@@

@@@TABLE@@@

###FIGURE###
< g r a p h i c s >AUC improvement over the three baselines. The improvement are sorted in a decreasing order for each baseline separately. 
@@@FIGURE@@@

Fig  <ref>, on the right, shows the improvement over (A) GPR,
###FIGURE###
< g r a p h i c s >Linear: AUC of the predicated classifiers for all classes of the flower datasets (Formulation E)
@@@FIGURE@@@
 A(TGP), and (C) DT for each class, where the improvement is calculated as (our AUC- baseline AUC)/ baseline AUC %. 
Table <ref> shows the percentage of the classes which our approach makes a prediction improvement for each of  the three baselines. The last row show the improvement over both the TGP and DT baseline 

. Table <ref> shows the five classes in Flower dataset where our approach made the best average improvement.
. (TGP+DT).  

 The point of that table is to show that in these cases both TGP and DT did poorly while our formulation that is based on both of them did significantly better. This shows that our formulation does not simply combine the best of the two approaches but can significantly improve the prediction performance.


###TABLE###
 Linear: Top-5 classes with highest combined improvement in Flower dataset0.85
###TABLE###

  class      &  (A) TGP (AUC) & (C) DT (AUC) & (E) Our (AUC)  & % Improv. 

   2   &  0.51 & 0.55 & 0.83 & 57%
  
   28 & 0.52 & 0.54 & 0.76 &  43.5%

   26 &  0.54 & 0.53 & 0.76 & 41.7%

   81 & 0.52 & 0.82 & 0.87   & 37%

   37 & 0.72 & 0.53 & 0.83   & 35.7 %

@@@TABLE@@@

@@@TABLE@@@








To evaluate the effect of the constraints in the objective function, we removed the constraints - (c^Tx_i ) ≥ζ_i which try to enforces all the seen examples to be on the negative side of the predicted classifier hyperplane and evaluated the approach. The result on the flower dataset (using one fold) was reduced to average AUC=0.59 compared to AUC=0.65 with the constraints. Similarly, we evaluated the effect of the constraint  t_*^TWc> l. The result was reduced to average AUC=0.58 compared to AUC=0.65 with the constraint.  This illustrates the importance of this constraint in the formulation.

 Constrained Baselines:We computed the ROC curves and report the area under that curve (AUC) as a comparative measure[In zero-shot learning setting the test data from the seen class are typically very large compared to those from unseen classes. This makes other measures, such as accuracy, useless since high accuracy can be obtained even if all the unseen class test data are wrongly classified; hence we used ROC curves, which are independent of this problem.] Five-fold cross validation over the classes were performed, within each of these class-folds, the data of the seen classes are further split into training and test sets.
Table <ref> (bottom three lines) also shows the average AUCs for the constrained baselines formulations, namely Constrained GPR, TGP and DT; see section <ref>. Even though the visual features and textual features were independently extracted, by learning correlation between them, we can predict classifiers for new categories. As shown previously, GPR performed poorly, while, as expected, TGP performed better. Adding constraints to GPR/TGP improved their performance. Combining regression and DT gave significantly better results for classes where both approaches individually perform poorly, as can be seen in Table <ref>-right. We performed an additional experiment, where  W is firstly computed using Constrained Domain Transfer (CDT). Then, the unseen classifier is predicted using equation <ref> with γ=0, which performs worse. This indicates that adding constraints to align to seen classifiers hurts the learnt domain transfer function on unseen classes. In conclusion, the final formulation that combines TGP and DT with additional constraints performs the best in both Birds and Flower datasets, where the effect of TGP is very limited since it was trained on sparse points.





@#S!S
Experimental Results for Kernel Classifier Prediction
@#S^S




Now that we have described our zero-shot learning setting and the suggested approaches to directly predict kernel-classifier parameters for unseen classes, we present several experiments to validate our model.
In this section, we presented a set of experiments, conducted to evaluate our proposed model for zero-shot learning of visual classifiers. The quantitative comparisons show our superior performance to the state of the art on two challenging datasets of fine-grained object categories.
@S#S!S
Additional Evaluation Metrics
@S#S^S


In addition to the the AUC, we already studied in the previous section. We report two additional metrics while evaluating and comparing the kernel classifier prediction to the linear classifier prediction, detailed as follows.

|N_sc|  to |N_sc+1|  Recall:
Under this metric, we aim to check  how   the learned classifiers of the seen classes confuse the predicted classifiers, when they are involved in a multi-class classification problem of N_sc + 1  classes. The first N_sc  classifiers are those of the seen classes, while (N_sc+1)^st classifier is a predicted classifier for an unseen class. We use Eq <ref> to predict label l^*  with the maximum confidence of an image x^*, such that l^* ∈L_sc∪ l_us,  l_us  is the label of the ground truth unseen class, and L_sc is the set of seen class labels. We compute the recall under this setting. This metric is computed for each predicted unseen classifier and the average is reported.

Multiclass Accuracy of Unseen classes (MAU): Under this setting, we aim to evaluate the performance of  the unseen  classifiers against each others. Firstly, the classifiers of all unseen categories are predicted. Then, we use Eq <ref> to predict the label with the maximum confidence of a test image x, such that its label l_us^* ∈L_us, where L_us is the set of all unseen class labels that only have text descriptions.




@S#S!S
Comparisons to Linear Classifier Prediction
@S#S^S

 We compare the kernel methods to the linear prediction discussed earlier,  which predicts a linear classifier from textual descriptions  ( T  space in our framework). The aspects of the comparison includes  1) whether the predicted kernelized classifier outperforms the predicted linear classifier  2) whether this behavior is consistent on multiple datasets. We performed the comparison on both Birds and Flower dataset.  For these experiments, in our setting, domain V  is the visual domain and domain T  is the textual domain, , the goal is to predict classifiers from pure textual description. We used the same features on the visual domain  and the textual domains detailed in subsection <ref>. That is,  classeme features classemes for images, extracted from images of the Bird and the Flower datasets and tf-idf salton1988term features for text articles followed by a CLSI clsi05 dimensionality reduction phase. We denote our kernel Domain Transfer prediction  and one class SVM adjust DT prediction by  "DT-kernel" and "SVM-DT-kernel" respectively. We compared against  linear classifier prediction (Linear Formulation (E) approach, denoted by just Linear Classifier).  (which uses a quadratic program to optimize the classifier parameters)We also compared against the  linear direct domain transfer (Linear Formulation (C), denoted by DT-linear).  In our kernel approaches, we used Gaussian rbf-kernel as a similarity measure in T  and V  spaces (k(d,d') = exp(-λ ||d-d'||)).

 

###TABLE###
Kernel: Recall, MAU, and average AUC on three seen/unseen splits on Flower Dataset and a seen/unseen split on Birds dataset0.90
###TABLE###
 
  & Recall-Flower & improvement & Recall-Birds& improvement 
SVM-DT kernel-rbf & 40.34% (+/-  1.2) % & & 44.05 %   &  
 
Linear Classifier  & 31.33  (+/-  2.22)% & 27.8 %  & 36.56 % & 20.4 %

@@@TABLE@@@


0.943
###TABLE###
 
  & MAU-Flower & improvement & MAU-Birds& improvement 
SVM-DT kernel-rbf & 9.1 (+/-  2.77) % & & 3.4  %   &  
DT kernel-rbf & 6.64 (+/-  4.1) % & 37.93 %  & 2.95  % & 15.25 %
 
Linear Classifier  Prediction & 5.93  (+/-  1.48)% & 54.36 %  & 2.62 % & 29.77 %
 
DT-linear Hoseini13,da11 & 5.79 (+/-  2.59)% & 58.46 %  &  2.47 % & 37.65 %

@@@TABLE@@@
0.93
###TABLE###
 
 & AUC-Flower& improvement & AUC-Birds & improvement
SVM-DT kernel-rbf & 0.653 (+/-  0.009)  &   &   0.61  &   
DT kernel-rbf & 0.623 (+/-  0.01) % & 4.7 % &  0.57  & 7.02 %
 
Linear Classifier Prediction & 0.658 (+/-  0.034) & - 0.7 % & 0.62  & -1.61%
 
Domain Transfer Hoseini13,da11 &0.644 (+/-  0.008) &  1.28 % & 0.56  & 8.93%

@@@TABLE@@@

   
 Kernel: MAU on a seen-unseen split-Birds Dataset (MKL)0.93
###TABLE###
 
& MAU & improvement 
SVM-DT kernel-rbf (text)& 4.10  % &   
 
Linear Classifier Prediction & 2.74 % & 49.6 %

@@@TABLE@@@


Kernel: MAU on a seen-unseen split-Birds Dataset (CNN image features, text description)0.93
###TABLE###
 
& MAU & improvement 
SVM-DT kernel (V-rbf, T-DS kernel)& 5.35  % &   
SVM-DT kernel (V-rbf, T-rbf on TFIDF)& 4.20  % &  27.3%
 
Linear Classifier (TFIDF text) Prediction & 2.65 % & 102.0%
norouzi2014zero & 2.3% & 132.6%

@@@TABLE@@@

@@@TABLE@@@





Recall metric :  The recall of the SVM-DT kernel approach is 44.05% for Birds and 40.34% for Flower, while it is 36.56% for Birds and  31.33% for Flower by best Linear Classifier prediction (E). This indicates that the  predicted classifier is less confused by the classifiers of the seen compared with  Linear Classifier prediction; see table  <ref> (top part)


MAU metric: It is worth to mention that the multiclass accuracy for the trained seen classifiers is 51.3% and 15.4%, using the classeme features,  on Flower dataset and Birds dataset[Birds dataset is known to be a challenging dataset for fine-grained, even when applied in a regular multiclass setting as it is clear from the 15.4% performance on seen classes], respectively. Table  <ref> (middle part) shows the average MAU metric over three seen/unseen splits for Flower dataset and one split on Birds dataset, respectively. 
Furthermore, the relative improvements of our  SVM-DT-kernel approach is reported against the baselines. On Flower dataset,  it is interesting to see that our approach achieved 9.1% MAU, 182% the random guess performance, , which is 17.7% of the multi-class accuracy of the seen classes (i.e. 51.3%),by predicting the unseen classifiers using just textual features as privileged information (i.e. T domain). It is important to mention that we achieved also 13.4%, 268% the random guess performance, in one of the splits (the 9.1% is the average over 3 seen/unseen splits). Similarity on Birds dataset, we achieved 3.4% MAU from text features, 132%  the random guess performance (further improved up to 224% in next experiments)., which is 22.7% of the multi-class accuracy of the seen classes on the same dataset (15.4%)


AUC metric:  Fig <ref> <ref> (top part) shows the ROC curves for our approach on the best predicted unseen classes from the Flower dataset. Fig <ref> <ref> (bottom part) shows the AUC for all the classes on Flower dataset (over three different splits). Table <ref> (bottom part)  shows the average AUC on the two datasets, compared to the baselines. More results and figures Corresponding figures for Birds dataset  for our kernel approach are attached in the supplementary materials.

Looking at table <ref>, we can notice that the proposed approach performs marginally similar to the baselines from AUC perspective. However, there is a clear improvement  in MAU  and Recall metrics. These results show the advantage of predicting classifiers in kernel space. Furthermore, the table shows that our SVM-DT-kernel approach outperforms our DT-kernel model. This indicates the advantage of the class separation, which is adjusted by the SVM-DT-kernel model. In all these experiments, we used a setting of our SVM-DT-kernel model, where C_β(T )  is ignored (i.e. λ_2 = 0); see Sec  <ref>). In order to study whether C_β( T )  is effective in unseen class prediction, we performed an extra experiment on Birds dataset, where λ_2>0  (e.g. λ_2 =1). We found that MAU of our DT approach has slightly decreased (i.e from 2.95&  to 2.91% ). Under the same setting, we also found that  C_β( T )   slightly reduced the performance of SVM-DT from 9.1% to 8.98%.MAU. This reflect our intuition argued in the approach sectionSec  <ref>. Hence, we suggest to assign λ_2  to 0 for our purpose. More details on the hyper-parameter selection are attached in the supplementary materials. 

###FIGURE###
< g r a p h i c s >< g r a p h i c s >Kernel: AUC of the 62 unseen classifiers the flower data-sets over three different splits (bottom part) and their Top 10 ROC-curves (top part)
@@@FIGURE@@@










###FIGURE###
< g r a p h i c s >ROC-curves for the top 10 classifiers in Flower dataset
@@@FIGURE@@@





###FIGURE###
< g r a p h i c s >Kernel: AUC of the 62 unseen classifiers the flower data-sets over three different splits
@@@FIGURE@@@

###TABLE###
Multi-class Accuracy of the Unseen classes (MAU)  on three seen/unseen splits - Flower Dataset and a seen/unseen split on Birds dataset0.6
###TABLE###
 
  & MAU-Flower & improvement & MAU-Birds& improvement 
SVM-DT kernel-rbf & 9.1 (+/-  2.77) % & & 3.4  %   &  
DT kernel-rbf & 6.64 (+/-  4.1) % & 37.93 %  & 2.95  % & 15.25 %
 
Linear Classifier  Prediction & 5.93  (+/-  1.48)% & 54.36 %  & 2.62 % & 29.77 %
 
DT-linear Hoseini13,da11 & 5.79 (+/-  2.59)% & 58.46 %  &  2.47 % & 37.65 %

@@@TABLE@@@

@@@TABLE@@@



###TABLE###
Average AUC of the Unseen classes on three  seen/unseen  splits on Flower dataset and a seen/unseen split on Birds dataset 0.6
###TABLE###
 
 & AUC-Flower & improvement & AUC-Birds & improvement
SVM-DT kernel-rbf & 0.653 (+/-  0.009)  &   &   0.61  &   
DT kernel-rbf & 0.623 (+/-  0.01) % & 4.7 % &  0.57  & 7.02 %
 
Linear Classifier Prediction & 0.658 (+/-  0.034) & - 0.7 % & 0.62  & -1.61%
 
DT-linear Hoseini13,da11 &0.644 (+/-  0.008) &  1.28 % & 0.56  & 8.93%

@@@TABLE@@@

@@@TABLE@@@




###TABLE###
Multi-class Accuracy of the Unseen classes (MAU)  on a seen-unseen split-Birds Dataset (MKL)
###TABLE###
 
& MAU & improvement 
SVM-DT kernel-rbf (text) - mkl (visual) & 4.10  % &   
 
Linear Classifier Prediction & 2.74 % & 49.6 %

@@@TABLE@@@

@@@TABLE@@@



###TABLE###
Multi-class Accuracy of the Unseen classes (MAU)  on a seen-unseen split-Birds Dataset (Attributes)
###TABLE###
 
 & MAU & improvement 
SVM-DT kernel-rbf & 5.6  % &   
DT kernel-rbf & 4.03 % &  32.7 %
 
Lampert DAP  Lampert09 & 4.8  % & 16.6 %

@@@TABLE@@@

@@@TABLE@@@



###FIGURE###
0.25tableKernel: MAU on a seen-unseen split-Birds Dataset (MKL)0.6
###TABLE###
 
& MAU & improvement 
SVM-DT kernel-rbf (text)& 4.10  % &   
 
Linear Classifier Prediction & 2.74 % & 49.6 %

@@@TABLE@@@
 0.25tableKernel: MAU on a seen-unseen split-Birds Dataset (Attributes)0.6
###TABLE###
 
 & MAU & improvement 
SVM-DT kernel-rbf & 5.6  % &   
DT kernel-rbf & 4.03 % &  32.7 %
 
Lampert DAPLampert09 & 4.8  % & 16.6 %

@@@TABLE@@@
 0.45< g r a p h i c s >Kernel: AUC of the 62 unseen classifiers the flower data-sets over three different splits
@@@FIGURE@@@





###FIGURE###
0.25tableKernel: MAU on a seen-unseen split-Birds Dataset (MKL)0.55
###TABLE###
 
& MAU & improvement 
SVM-DT kernel-rbf (text)& 4.10  % &   
 
Linear Classifier Prediction & 2.74 % & 49.6 %

@@@TABLE@@@
tableKernel: MAU on a seen-unseen split-Birds Dataset (Attributes)0.6
###TABLE###
 
 & MAU & improvement 
SVM-DT kernel-rbf & 5.6  % &   
DT kernel-rbf & 4.03 % &  32.7 %
 
Lampert DAPLampert09 & 4.8  % & 16.6 %

@@@TABLE@@@
0.6
###TABLE###
 
 & Recall & improvement 
SVM-DT kernel-rbf & 76.7  % &   
 
Lampert DAP  Lampert09 & 68.1  % & 12.6 %

@@@TABLE@@@
0.01 0.27< g r a p h i c s >ROC-curves for the top 10 classifiers in Flower dataset 0.25< g r a p h i c s >Kernel: AUC of the 62 unseen classifiers the flower data-sets over three different splits
@@@FIGURE@@@



###TABLE###
Average AUC of the Unseen classes on three three seen/unseen  splits - Flower Dataset
###TABLE###
 
 & AUC & improvement 
SVM-DT kernel-rbf & 0.653 (+/-  0.009)  &   
DT kernel-rbf & 0.623 (+/-  0.01) % & 4.7 %
 
Linear Classifier Prediction & 0.658 (+/-  0.034) & - 0.7 %
 
Domain Transfer Hoseini13,da11 &0.644 (+/-  0.008) &  1.28 %

@@@TABLE@@@

@@@TABLE@@@




###TABLE###
Average AUC of the Unseen classes on a seen/unseen  split - Birds Dataset 
###TABLE###
 
 & AUC & improvement 
 
    SVM-DT kernel-rbf & 0.61  &  
 
    DT-kernel-rbf & 0.57  & 7.02 %
 
    Linear Classifier Prediction & 0.62  & -1.61%
 
    Domain Transfer Hoseini13,da11  & 0.56  & 8.93%

@@@TABLE@@@

@@@TABLE@@@







###TABLE###
Multi-class Accuracy of the Unseen classes (MAU)  on a seen-unseen split-Birds Dataset
###TABLE###
 
& accuracy & improvement 
SVM-DT kernel-rbf & 3.4  % &   
DT kernel-rbf & 2.95  % & 15.25 %
 
Linear Classifier Prediction& 2.62 % & 29.77 %
 
Domain Transfer Hoseini13,da11 & 2.47 % & 37.65 %

@@@TABLE@@@

@@@TABLE@@@

###TABLE###
Multi-class Accuracy of the Unseen classes (MAU)  on three seen/unseen splits - Flower Dataset
###TABLE###
 
  & accuracy & improvement 
SVM-DT kernel-rbf & 9.1 (+/-  2.77) % &   
DT kernel-rbf & 6.64 (+/-  4.1) % & 37.93 %
 
Linear Classifier Prediction & 5.93  (+/-  1.48)% & 54.36 %
 
Domain Transfer Hoseini13,da11 & 5.79 (+/-  2.59)% & 58.46 %

@@@TABLE@@@

@@@TABLE@@@



@S#S!S
Multiple Kernel Learning (MKL) Experiment
@S#S^S


This experiment shows the added value of  proposing a kernelized zero-shot learning approach. We conducted an experiment where the final kernel on the visual domain is produced by Multiple Kernel Learning MKKLAlgs11. For the visual domain, we extracted kernel descriptors for Birds dataset. Kernel descriptors provide a principled way to turn any pixel attribute to patch-level features, and are able to generate rich features from various recognition cues. We specifically used four types of kernels introduced by bo_nips10 as follows: Gradient Match Kernels that captures image variation based on predefined kernels on image gradients. Color Match Kernel that describes patch appearance using two kernels on top of RGB and normalized RGB for regular images and intensity for grey images. These kernels capture image variation and visual apperances. For modeling the local shape, Local Binary Pattern kernels have been applied. We computed these kernel descriptors on local image patches with fixed size 16 x 16 sampled densely over a grid with step size 8 in a spatial pyramid setting with four layers. The dense features are vectorized using codebooks of size 1000. This process ended up with a 120,000 dimensional feature for each image (30,000 for each type). Having extracted the four types of descriptors, we compute an rbf kernel matrix for each type separately. We learn the bandwidth parameters for each rbf kernel by cross validation on the seen classes. Then, we generate a new kernel k_mkl(d, d') = ∑_i=1^4 w_i k_i(d, d'), such that w_i is a weight assigned to each kernel. We learn these weights by applying Bucak's Multiple Kernel Learning algorithm nips10_Bucak. Then, we applied our approach where the MKL-kernel is used in the visual domain and rbf kernel in the text domain similar to the previous experiments.



 To compare the kernel prediction approach to the linear prediction approach (formulation (E)) under this setting,  we concatenated  all kernel descriptors to end up with  120,000 dimensional feature vector in the visual domain. As highlighted in  the kernel approach section, the linear prediction approach solves a quadratic program of N+d_v+1  variables for each unseen class.   Due to the large dimensionality of data  (d_v = 120,000), this is not tractable. To make this setting applicable, we reduced the dimensionality of the feature vector into 4000 using PCA to make it feasible to compute the performance. This highlights the benefit of our approach since our quadratic program does not depend on the dimensionality of the data. Table  <ref> shows MAU for the kernel prediction approaches under this setting against  linear prediction. The results show the benefits of having a kernel prediction for zero shot learning where kernel methods to construct an arbitrary kernel that  improves the performance.


@#S!S
Multiple Representation Experiment and  Distributional Semantic(DS) Kernel
@#S^S


The aim of this experiment is to show that the kernel approach perform on different representations of text T and visual domains V. In this experiment, we extracted Convolutional Neureal Network(CNN) image features for the Visual domain. We used caffe jia2014caffe implementation of imagenetnips12. Then, we extracted the sixth activation feature of the CNN (FC6) since we found it works the best on the standard classification setting. We found this consistent with the results of donahue2014decaf over different CNN layers. While using  TFIDF feature of text description and CNN features for images, we achieved 2.65% for the linear version and 4.2% for the rbf kernel on both text and images. We further improved the performance to 5.35% by using our proposed Distributional Semantic (DS) kernel in the text domain and rbf kernel for images. In this DS experiment, we used the  distributional semantic model by mikolov2013distributed trained on  GoogleNews corpus (100 billion words)  resulting in a vocabulary of size 3 million words, and word vectors of K=300 dimensions. This experiment shows both the value of having a kernel version and also the value of the proposed kernel in our setting. We also applied the zero shot learning approach in norouzi2014zero which performs worse in our settings; see Table <ref>.




@S#S!S
Attributes Experiment
@S#S^S


We emphasis that our  goal from this experiment is not attribute prediction. However, it was interesting for us to see the behavior of our method where T space is defined from attributes instead of text. In contrast to attribute-based models, which fully utilize attribute information to build attribute classifiers, we do not learn attribute classifiers. In this experiment, our method  uses only the first moment of information of the attributes (i.e. the average attribute vector). We decided to compare to an attribute-based approach from this perspective. In particular, we applied the  (DAP) attribute-based model lampertPAMI13,Lampert09, widely adopted in many applications (e.g.,liu2013video,rohrbach11cvpr), to the Birds dataset.




For this experiment, we compute the average attributes provided with images for each category, without learning any binary classifiers.  We computed this as  T  domain representation for the Birds dataset only, since the Flower dataset does not have attributes. Attribute annotation of Birds dataset is provided in terms of: "Visibility" and "Certainty". The first term is 1 if the attribute is visible and 0 otherwise. The second term indicates how certain the annotator was about his decision with three levels of "Certain", "Guessing" and "probable". We assigned probability of a visible attribute when the annotator is sure about his decision to 1 and when he is sure of not seeing the attribute to 0. All other combination of decisions fall in the range of (0,1). As this attribute annotation is provided for each images, we averaged these scores across all the samples of a class to get a single attribute descriptor for each class, to be consistent with our learning setting. For visual domain, we used classeme features in this experiment (as table <ref> experiment )(as in section  <ref> ).





###TABLE###
Kernel: Recall and MAU on a seen-unseen split-Birds Dataset (Attributes)1.0
###TABLE###
 
 & Recall & improvement 
SVM-DT kernel-rbf & 76.7  % &   
 
Lampert DAP  Lampert09 & 68.1  % & 12.6 %

@@@TABLE@@@


    1.0
###TABLE###
 
 & MAU & improvement 
SVM-DT kernel-rbf & 5.6  % &   
DT kernel-rbf & 4.03 % &  32.7 %
 
Lampert DAP Lampert09 & 4.8  % & 16.6 %

@@@TABLE@@@


 
@@@TABLE@@@


An interesting result is that our approach achieved 5.6%  MAU (224% the random guess performance); see Table  <ref>. In contrast, we get 4.8% multiclass accuracy using  DAP approach lampertPAMI13. In this setting, we also measured the N_sc to  N_sc+1 average recall. We found the recall measure is 76.7% for our SVM-DT-kernel, while it is 68.1% on  the DAP approach, which reflects better true positive rate (positive class is the unseen one). We find these results interesting, since we achieved it without learning any attribute classifiers, as in lampertPAMI13. When comparing the results  of our approach using attributes (Table <ref>) vs. textual description (Table <ref>)[We are refering to the experiment that uses classeme as visual features to have a consistent comparison to here] as the T space used for prediction, it is clear that the attribute features gives better prediction. This support our hypothesis that the more meaningful the T  domain, the better the performance on V  domain. This indicates that if a better textual representation is used, a better performance can be achieved. Attributes are a good semantic representation of a class but it is difficult to  define attributes for an arbitrary class and further measure the confidence of each one. In contrast, it is much easier to find an unstructured  text description for visual class.









@#!S
Conclusion
@#^S

We explored the problem of predicting visual classifiers from textual description of classes with no training images.  We investigated and  experimented with different formulations for the problem within the fine-grained categorization context.  We first proposed  a novel formulation that captures information between the visual and textual domains by involving knowledge transfer from textual features to visual features, which indirectly leads to predicting a linear visual classifier described by the text. In the future, we are planning to propose a kernel version to tackle the problem instead of using linear classifiers. Furthermore,  We also proposed a new zero-shot learning technique to predict kernel-classifiers of unseen categories using information from a privilege space. We formulated the problem as domain transfer function from text description  to the visual classification space, while supporting kernels in both domains. We proposed a one-class SVM adjustment to our domain transfer function in order to improve the prediction. We validated the performance of our model by several experiments. We also showed that   our approach using with weak-attributes. We illustrated the value of proposing a kernelized version by applying kernels generated by Multiple Kernel Learning (MKL) and achieved better results.  We  compared our approach with  state-of-the-art approaches and interesting findings have been reported. In the future, we aim to improve this model by learning the unseen classes jointly and on a larger scale. 
We are also looking forward to studying more features for the X and E domains in a  large scale setting (number of classes > 1000). 




Acknowledgment.  This research was partially funded by NSF award IIS-1218872 and IIS-1409683.





IEEEtranegbib,write_a_classifier,elgammal,NLPVision,NLPVisionProposal,smara


