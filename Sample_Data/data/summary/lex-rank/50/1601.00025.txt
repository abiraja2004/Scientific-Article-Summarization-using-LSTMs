We propose a distributional semantic kernel g(·, ·) = g_DS(·, ·)  to define the similarity between two text descriptions in T domainof visual classes in our setting.
While this kernel is applicable to  kernel classifier predictors  presented in Sec <ref>, it could be used for other applications.
We start by  distributional semantic models by mikolov2013distributed,mikolov2013efficient to represent the semantic manifold M_s, and a function vec(·) that maps a word to a K× 1 vector in M_s.
The main assumption behind this class of distributional semantic model  is that similar words share similar context.
Mathematically speaking, these models  learn a vector for each word w_n, such  that p(w_n|(w_n-L, w_n-L+1, ...,  w_n+L-1,w_n+L) is maximized over the training corpus, where 2× L is the context window size.
Hence similarity between vec(w_i) and vec(w_j) is high if they co-occurred a lot in context of size 2× L in the training text-corpus.
We normalize all the word vectors to length 1 under L2 norm, i.e.,  vec(·) ^2=1.
Let us assume a  text description D that we represent by a set of triplets D = {(w_l,f_l, vec(w_l)), l=1... M}, where w_l is a word that occurs in D with frequency f_l and its corresponding word vector is vec(w_l) in M_s.
We drop the stop words from D. We define  F = [f_1, ..., f_M]^T and P = [vec(w_1), ..., vec(w_M)]^T, where F is an M×1  vector of term frequencies and P is an M × K matrix of the corresponding term vectors.
Given two text descriptions D_i and D_j which contains M_i and M_j terms respectively.
We compute P_i (M_i × 1) and V_i (M_i × K) for  D_i  and P_j (M_j × 1) and V_j (M_j × K) for  D_j.
Finally  g_DS(D_i, D_j) is defined as   ||FORMULA|| One advantage of this similarity measure is that it captures semantically related terms.
It is not hard to see that the standard Term Frequency (TF) similarity could be thought as a special case of this kernel where vec(w_l)^T vec(w_m)=1 if w_l=w_m, 0 otherwise, i.e., different terms are orthogonal.
However, in our case the word vectors are learnt through a distributional semantic model which makes semantically related terms have higher dot product (vec(w_l)^T vec(w_m)).
Φ (t_*)  Prediction of Φ (t_*)  = β(t_*) (Sec.
<ref>), is decomposed into training (domain transfer) and prediction phases, detailed as follows   Φ (t_*) The proposed formulations in this section aims at predicting a linear hyperplane parameter c of a one-vs-all classifier for a new unseen class given a textual description, encoded as a feature vector t_* and the knowledge learned at the training phase from seen classes[The notations follow from Subsection <ref>].
We start by  defining the learning components that were used by the formulations described in this section:     Classifiers:          a set of linear one-vs-all classifiers {c_j} are learned, one for each seen class.
Probabilistic Regressor:               Given {(t_j,c_j)} a regressor is learned that can be used to give a prior estimate for p_reg(c | t) (Details in Sec <ref>).
Domain Transfer:              Given T and V a domain transfer function, encoded in the matrix W is learned, which captures the correlation between the textual and visual domains (Details in Sec <ref>).
Each of the following subsections show a different approach to predict a linear classifier from t_* as Φ(t_*) = c(t_*); see Sec <ref>.
The final approach (E) combines  regression, domain transfer, and additional constraints, which achieves the best performance.
We compare between these alternative formulations in our experiments.
Hyper-parameter selection is detailed in the supplementary materials for all the approaches.
||FIGURE||    NewRelatedWork       Fig <ref> illustrates the learning setting.
The information in our problem comes from two different domains: the visual domain  and the textual domain, denoted by V and T, respectively.
Similar to traditional visual learning problems, we are given training data in the form V={(x_i , l_i)}_N, where x_i is an image and l_i ∈{1... N_sc} is its class label.
We denote the number of classes available at training as N_sc, where sc indicates "seen classes".
As typically done in visual classification setting, we can learn N_sc binary one-vs-all classifiers, one for each of these classes.
Our goal is to be able to predict a classifier for a new category based only on the learned classes and a textual description(s) of that category.
In order to achieve that, the learning process has to also include textual description of the seen classes (as shown in Fig  <ref> ).
Depending on the domain we might find a few, a couple, or as little as one textual description to each class.
We denote the textual training data for class j by {t_i∈T}^j.
In this paper we assume we are dealing with the extreme case of having only one textual description available per class, which makes the problem even more challenging.
For simplicity, the text description of class j is denoted by t_j.
However, the formulation we propose in this paper directly applies to the case of multiple textual descriptions per class.
In this paper, we cover predicting  visual classifier Φ(t_*) from an unseen text description t_*  in  linear form or RKHS kernalized form, defined as follows         We introduce two possible frameworks for this problem and discuss potential limitations for them.
In this background section,  we focus on predicting linear classifiers for simplicity, which motivates the the evaluated linear classifier formulations that follow in Sec <ref>.
