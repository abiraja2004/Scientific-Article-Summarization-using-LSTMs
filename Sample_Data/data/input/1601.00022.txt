 Low level image features such as SIFT Lowe:2004:DIF:993451.996342 and Bag-of-word methods were widely used as a representation for image retrieval and classification. However, researchers have proven that these low level features do not have enough power to represent the semantic meaning of images. Mid-level image feature representations are often used to achieve better performance in a variety of computer vision tasks. Some frameworks for using middle level feature representation, such as li2010object, TorresaniSzummerFitzgibbon10, have achieved excellent performance in object recognition and scene classification. ImageNet deng2009imagenet, was introduced and has lead to breakthroughs in tasks such as object recognition and image classification due to the scale of well-labeled and annotated data. Each of the images within Image-Net are manually labeled and annotated, this is a very expensive and time-consuming task. This work looks beyond a manually defined ontology, and instead focuses on mining mid-level image patches automatically from very weakly supervised data to attempt to unbound researchers from the need for costly supervised datasets. We approach this problem from a multi-modal perspective (using the image and caption together), which allows us to name and discover higher level image concepts.  Visual pattern mining is an important task since it is the foundation of many middle-level feature representation frameworks. han2000mining and zhang2014scalable use low level features and a hashing approach to mine visual patterns from image collections. yuan2007spatial utilizes a spatial random partition to develop a fast image matching approach to discover visual patterns. All of these methods obtain image patches from the original image collection either by random sampling or salient/objectness detection and utilize image matching or clustering to detect similar patches to create visual patterns. These methods are computationally intense, because they have to examine possibly hundreds or thousands of image patches from each image. These methods rely heavily on low level image features, and therefore do not often produce image patches that exhibit high level semantic meaning. The generated image patterns are usual visually duplicated or near-duplicated image patches.  Convolutional neural networks (CNN) have achieved great success in many research areas Simonyan14c, Alexnet. Recently, LiLSH15CVPR combined the image representation from a CNN and association rule mining technology to effectively mine visual patterns. They first randomly sampled image patches from the original image and extracted the fully connected layer response as features for each image patch utilized in an association rule mining framework. This approach is able to find consistent visual patterns, but cannot guarantee the discovered visual patterns are semantically meaningful. Most of the existing visual pattern mining work focuses on how to find visually consistent patterns. To the best of our knowledge, our paper is the first attempt to find high level semantically meaningful visual patterns.  Another category of related works is image captioning. In recent years, many researchers have focused on teaching machines to understand images and captions jointly. Image caption generation focuses on automatically generating a caption that directly describes the content in an image using a language model. Multimodal CNNs frome2013devise or RNN mao2014explain frameworks are often used to generate sentences for the images. All the existing work use supervised approaches to learn a language generation model based on carefully constructed image captions created for this task. The datasets used in caption generation, such as the MSCoco dataset MSCoco consist of much simpler sentences than appear in news image-caption pairs. We differ from these approaches in that we do not try to generate a caption for images, but instead use them jointly to mine and name the patterns that appear throughout the images.        In this section we discuss our multimodal pattern mining approach. In particular, how we collected a large scale dataset, generated feature based transactions from the image and captions, and how we find semantic visual patterns and name them.   ||TABLE||     
