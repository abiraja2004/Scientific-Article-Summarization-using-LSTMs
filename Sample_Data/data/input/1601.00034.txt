  The conventional RBM models the joint probability  (, |) for visible variables  = [_1,...,_i,...,_I] with ∈_1 ×...×_I  and hidden variables  = [_1,...,_j,..._J] with ∈_1 ×...×_J as (, |) = (-(,) - ()).  This joint probability is a Boltzmann distribution with a particular energy function : ×→ and a normalization function A. The distinguishing property of RBM compared to other Boltzmann distributions is the conditional independence due to its bipartite structure.  welling2004exponential construct Exponential Family Harmoniums (EFH), by first  constructing independent distribution over individual variables: considering a hidden variable _j, its sufficient statistics {t_b}_b  and canonical parameters {η̃_j,b}_b, this independent distribution is (_j) = (_j) (∑_bη̃_j,b  t_b(_j) -({η̃_j,b}_b) )  where : _j → is the base measure and ({η_i,a}_a) is the normalization constant.  Here, for notational convenience, we are assuming functions with distinct inputs are distinct --   t_b(_j) is not necessarily the same function as t_b(_j'), for j' ≠ j.  The authors then combine these independent distributions using quadratic terms that reflect the bipartite structure of the EFH to get its joint form (, ) ∝ &(∑_i,aν̃_i,a t_a(_i)   + &∑_j,bη̃_j,b  t_b(_j) + ∑_i,a,j,b_i,j^a,b t_a(_i) t_b(_j) )  where the normalization function is ignored and the base measures are represented as additional sufficient statistics with fixed parameters. In this model, the conditional distributions are  (_i |) = ( ∑_aν_i,a t_a(_j) -({ν_i,a}_a ) (_j |) = ( ∑_bη_j,b t_b(_j) -({η_j,b}_b )  where the shifted parameters η_j,b = η̃_j,b + ∑_i,a_i,j^a,b t_a(_i)  and ν_i,a = ν̃_i,a + ∑_j,b_i,j^a,b t_b(_j) incorporate the effect of evidence in network on the random variable of interest.  It is generally not possible to efficiently sample these conditionals (or the joint probability) for arbitrary sufficient statistics. More importantly, the joint form of eq:EFH_joint and its energy function are "obscure". This is in the sense that    the base measures {}, depend on the choice of sufficient statistics and the normalization function A(). In fact for a fixed set of sufficient statistics {t_a(_i)}_i, {t_b(_j)}_j, different compatible choices of normalization constants and base measures may produce diverse subsets of the exponential family. Exp-RBM is one such family, where sufficient statistics are identity functions.   ||TABLE||       A consistent estimator for the parameters , given observations D = {1,...,N}, is obtained by maximizing the marginal likelihood ∏_n(n|), where the eq:joint defines the joint probability (, ).  The gradient of the log-marginal-likelihood ∇_ (∑_n((n|))  ) is 1/N∑_n_(|n, ) [· (n)^T]   -  _(, |) [ ·^T]  where the first expectation is  the observed data  in which (|) = ∏_j(_j |) and (_j |) is given by eq:bregman_p. The second expectation is  the model of eq:joint.  When discriminatively training a neuron (∑_i W_i,j_i) using input output pairs D={(n, _jn)}_n,  in order to have a loss that is convex in the model parameters _:j, it is common to use a matching loss for the given transfer function  helmbold1999relative. This is simply the Bregman divergence _((η_jn)  _jn),  where η_jn = ∑_i W_i,j_in. Minimizing this matching loss corresponds to maximizing the log-likelihood of eq:bregman_p, and it should not be surprising that the gradient ∇__:j (∑_n_((η_jn)  _jn)  ) of this loss  _:j = [W_1,j,...,W_M,j]  ∑_n(η_jn) (n)^T   - _jn(n)^T  resembles that of eq:grad, where (η_jn) above substitutes _j in eq:grad.   However, note that in generative training, _j is not simply equal to (η_j), but it is sampled from the exponential family distribution eq:bregman_p with the mean (η_j) -- that is _j = (η_j) + noise. This extends the previous observations linking the discriminative and generative (or regularized) training -- via Gaussian noise injection -- to the noise from other members of the exponential family [][]an1996effects,vincent2008extracting,bishop1995training which in turn relates to the regularizing role of generative pretraining of neural networks erhan2010does.  Our sampling scheme (next section) further suggests that when using output Gaussian noise injection for regularization of arbitrary activation functions, the variance of this noise should be scaled by the derivative of the activation function.      We evaluate the representation capabilities of Exp-RBM for different stochastic units in the following two sections. Our initial attempt was to adapt Annealed Importance Sampling [AIS;][]salakhutdinov2008quantitative to Exp-RBMs.  However, estimation of the importance sampling ratio in AIS for general Exp-RBM proved challenging. We consider two alternatives: 1) for large datasets, sec:filters qualitatively evaluates the filters learned by various units and; 2) sec:quant evaluates Exp-RBMs on a smaller dataset where we can use indirect sampling likelihood to quantify the generative quality of the models with different activation functions.   Our objective here is to demonstrate that a combination of our sampling scheme with contrastive divergence (CD) training can indeed produce generative models for a diverse choice of activation function.    
